# RoArm-M3 Pick and Place ê°•í™”í•™ìŠµ êµ¬í˜„ ë¡œê·¸
**ë‚ ì§œ**: 2025-10-19  
**ì‘ì—… ì‹œê°„**: 10:40 - 12:30 (1ì‹œê°„ 50ë¶„)  
**ëª©í‘œ**: ê·¸ë¦¬í¼ ë¹„ìœ¨ ì¡°ì • + ê¸°ì´ˆ ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬ì¶•

---

## ğŸ“‹ ì‘ì—… ê°œìš”

### ì˜¤ëŠ˜ ëª©í‘œ
1. âœ… ê·¸ë¦¬í¼ ë¹„ìœ¨ ì¡°ì • (Isaac Sim í”¼ë“œë°± ë°˜ì˜)
2. âœ… ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬ì¶•
3. âœ… ê°•í™”í•™ìŠµ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ê³„
4. âœ… PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ êµ¬í˜„
5. ğŸŸ¡ Isaac Sim GUI ì‹œê°ì  í™•ì¸ (ë‹¤ìŒ)

---

## ğŸ”§ 1. ê·¸ë¦¬í¼ ë¹„ìœ¨ ì¡°ì •

### ë¬¸ì œ ì¸ì‹
- Isaac Simì—ì„œ í™•ì¸ ê²°ê³¼: ê·¸ë¦¬í¼ í•‘ê±°ê°€ ê³¼ë„í•˜ê²Œ í¬ê²Œ í‘œí˜„ë¨
- ì›ì¸: URDFì—ì„œ `radius=0.008m(8mm)`, `length=0.05m(50mm)` ì„¤ì •

### í•´ê²° ë°©ì•ˆ
**ìŠ¤í™ ë³€ê²½**:
| íŒŒë¼ë¯¸í„° | ê¸°ì¡´ | ìˆ˜ì • | ê°ì†Œìœ¨ |
|---------|------|------|--------|
| Radius | 8mm | 5mm | -37.5% |
| Length | 50mm | 35mm | -30% |
| Tip box | 12Ã—8Ã—6mm | 8Ã—6Ã—4mm | -50% |
| Mass | 10g | 8g | -20% |

**ì½”ë“œ ìˆ˜ì •** (`scripts/generate_multiprim_urdf.py`):
```python
"gripper_left_finger": {
    "visual": [
        {"type": "cylinder", "radius": 0.005, "length": 0.035, 
         "origin": [0, 0, 0.0175], "material": "black"},
        {"type": "box", "size": [0.008, 0.006, 0.004], 
         "origin": [0, 0, 0.037], "material": "silver"},
    ],
    # ... collision & inertial
}
```

**ê²°ê³¼**:
```bash
$ python scripts/generate_multiprim_urdf.py
âœ… URDF ìƒì„± ì™„ë£Œ: roarm_m3_multiprim.urdf
ğŸ“Š í†µê³„:
  - Visual í”„ë¦¬ë¯¸í‹°ë¸Œ: 23ê°œ (ë³€ë™ ì—†ìŒ)
  - Collision í”„ë¦¬ë¯¸í‹°ë¸Œ: 10ê°œ (ë³€ë™ ì—†ìŒ)
  - íŒŒì¼ í¬ê¸°: 10.8 KB (ë³€ë™ ì—†ìŒ)
```

---

## ğŸ¤– 2. ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬ì¶•

### 2.1 í™˜ê²½ í´ë˜ìŠ¤ ì„¤ê³„
**íŒŒì¼**: `envs/roarm_pick_place_env.py` (400+ lines)

**í•µì‹¬ ì»´í¬ë„ŒíŠ¸**:
```python
class RoArmPickPlaceEnv:
    def __init__(self, cfg):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        - Isaac Sim World ì´ˆê¸°í™”
        - URDF ë¡œë´‡ ë¡œë“œ
        - íë¸Œ & íƒ€ê²Ÿ ë§ˆì»¤ ìƒì„±
    
    def reset(self) -> np.ndarray:
        """í™˜ê²½ ë¦¬ì…‹"""
        - ë¡œë´‡ Home position
        - íë¸Œ ìœ„ì¹˜ ëœë¤í™” (Â±5cm)
        - ì´ˆê¸° observation ë°˜í™˜
    
    def step(self, action) -> (obs, reward, done, info):
        """í™˜ê²½ ìŠ¤í…"""
        - Action â†’ Joint velocities
        - Physics ì‹œë®¬ë ˆì´ì…˜
        - Reward ê³„ì‚°
        - ì¢…ë£Œ ì¡°ê±´ í™•ì¸
    
    def _calculate_reward(self, obs) -> float:
        """ë³´ìƒ í•¨ìˆ˜"""
        - Distance-based: -distance * 10.0
        - Success bonus: +100.0 (< 5cm)
    
    def _check_done(self, obs) -> bool:
        """ì¢…ë£Œ ì¡°ê±´"""
        - Success: distance < 5cm
        - Timeout: step >= max_steps
        - Failure: cube_z < -0.1m
```

### 2.2 Observation Space (15 dim)
| ìš”ì†Œ | ì°¨ì› | ì„¤ëª… |
|------|------|------|
| Joint positions | 6 | `joint_1 ~ joint_6` (revolute) |
| Gripper state | 2 | `left_finger, right_finger` (prismatic) |
| End-effector position | 3 | `x, y, z` (FKë¡œ ê³„ì‚°) |
| Object position | 3 | `x, y, z` (íë¸Œ) |
| Distance to target | 1 | `â€–cube - targetâ€–` |

**ì„¤ê³„ ê·¼ê±°**:
- Joint positions: ë¡œë´‡ ìì„¸ ì¸ì‹
- Gripper state: ë¬¼ì²´ íŒŒì§€ ìƒíƒœ
- EE position: ì‘ì—… ê³µê°„ ìœ„ì¹˜
- Object position: ë¬¼ì²´ ìƒíƒœ
- Distance: ëª©í‘œ ë‹¬ì„±ë„

### 2.3 Action Space (8 dim)
| ìš”ì†Œ | ì°¨ì› | ë²”ìœ„ | ìŠ¤ì¼€ì¼ |
|------|------|------|--------|
| Joint velocities | 6 | [-1, 1] | Ã— [2.0, 1.5, 2.0, 2.5, 3.0, 2.0] |
| Gripper velocities | 2 | [-1, 1] | Ã— [0.05, 0.05] |

**ì •ê·œí™”**: ëª¨ë“  ì•¡ì…˜ì€ [-1, 1] ë²”ìœ„ë¡œ ì •ê·œí™”  
**ìŠ¤ì¼€ì¼ë§**: ì‹¤ì œ joint limitì— ë§ê²Œ ë³€í™˜

### 2.4 Reward Function
```python
def _calculate_reward(self, obs: np.ndarray) -> float:
    distance = obs[14]  # Object-target distance
    
    # Distance-based penalty (ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ í° íŒ¨ë„í‹°)
    distance_reward = -distance * 10.0
    
    # Success bonus (íƒ€ê²Ÿ ë„ë‹¬ ì‹œ)
    success_bonus = 0.0
    if distance < 0.05:  # 5cm ì´ë‚´
        success_bonus = 100.0
    
    return distance_reward + success_bonus
```

**ì„¤ê³„ ì›ì¹™**:
- **Dense reward**: ë§¤ ìŠ¤í…ë§ˆë‹¤ ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ
- **Sparse bonus**: ì„±ê³µ ì‹œ í° ë³´ìƒ
- **Scale balance**: ê±°ë¦¬ íŒ¨ë„í‹°ì™€ ì„±ê³µ ë³´ìƒì˜ ê· í˜•

**ì˜ˆì‹œ**:
| Scenario | Distance | Reward | ì„¤ëª… |
|----------|----------|--------|------|
| ì´ˆê¸° ìƒíƒœ | 0.35m | -3.5 | ë¨¼ ê±°ë¦¬ |
| ì ‘ê·¼ ì¤‘ | 0.15m | -1.5 | ê°€ê¹Œì›Œì§ |
| ê±°ì˜ ë„ë‹¬ | 0.03m | -0.3 + 100 = 99.7 | ì„±ê³µ! |

### 2.5 ì¢…ë£Œ ì¡°ê±´
1. **Success**: `distance < 0.05m` (5cm)
2. **Timeout**: `step >= 600` (10ì´ˆ Ã— 60 FPS)
3. **Failure**: `cube_z < -0.1m` (í…Œì´ë¸” ë°–ìœ¼ë¡œ ë–¨ì–´ì§)

---

## ğŸ§  3. PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸

### 3.1 GymWrapper
**íŒŒì¼**: `scripts/train_roarm_rl.py`

**ëª©ì **: Isaac Sim í™˜ê²½ì„ Gymnasium APIë¡œ ë˜í•‘

```python
class GymWrapper(gym.Env):
    def __init__(self):
        self.env = RoArmPickPlaceEnv(cfg)
        
        # Gymnasium spaces
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, 
            shape=(15,), dtype=np.float32
        )
        
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, 
            shape=(8,), dtype=np.float32
        )
    
    def reset(self, seed=None, options=None):
        obs = self.env.reset()
        return obs.astype(np.float32), {}
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        terminated = done
        truncated = False
        return obs, reward, terminated, truncated, info
```

### 3.2 PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
```python
model = PPO(
    policy="MlpPolicy",              # Multi-Layer Perceptron
    env=env,
    learning_rate=3e-4,              # Adam LR
    n_steps=2048,                    # Rollout buffer size
    batch_size=64,                   # Minibatch size
    n_epochs=10,                     # Update epochs per rollout
    gamma=0.99,                      # Discount factor
    gae_lambda=0.95,                 # GAE lambda
    clip_range=0.2,                  # PPO clipping
    ent_coef=0.01,                   # Entropy coefficient
    vf_coef=0.5,                     # Value function coefficient
    max_grad_norm=0.5,               # Gradient clipping
    tensorboard_log="logs/rl_training/tensorboard",
    device="cuda",                   # GPU ì‚¬ìš©
)
```

**ì„ íƒ ê·¼ê±°**:
- `learning_rate=3e-4`: í‘œì¤€ Adam LR (ì•ˆì •ì )
- `n_steps=2048`: ì¶©ë¶„í•œ ë°ì´í„° ìˆ˜ì§‘ (ì—í”¼ì†Œë“œë‹¹ ~3-4ê°œ)
- `clip_range=0.2`: í‘œì¤€ PPO clipping (ì•ˆì •ì„±)
- `ent_coef=0.01`: ì•½ê°„ì˜ exploration (ë„ˆë¬´ í¬ë©´ ë¶ˆì•ˆì •)

### 3.3 Callbacks
**CheckpointCallback**:
- ì£¼ê¸°: 5,000 ìŠ¤í…ë§ˆë‹¤
- ê²½ë¡œ: `logs/rl_training/checkpoints/`
- í˜•ì‹: `roarm_ppo_5000_steps.zip`

**EvalCallback**:
- ì£¼ê¸°: 2,000 ìŠ¤í…ë§ˆë‹¤
- í‰ê°€: 5 ì—í”¼ì†Œë“œ
- Best model ìë™ ì €ì¥

### 3.4 í•™ìŠµ ì‹¤í–‰
```bash
# 50K ìŠ¤í… í•™ìŠµ (ì•½ 30-60ë¶„)
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode train --timesteps 50000

# ì¶œë ¥ ì˜ˆì‹œ:
# ================================================
# ğŸš€ RoArm-M3 Pick and Place PPO Training
# ================================================
#   Total timesteps: 50,000
#   Save directory: logs/rl_training
#
# ğŸ”§ í™˜ê²½ ìƒì„± ì¤‘...
# ğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...
#   âœ… Device: cuda
#   âœ… Policy network:
#       - Observation space: (15,)
#       - Action space: (8,)
#
# ğŸ“š í•™ìŠµ ì‹œì‘...
# ================================================
```

---

## ğŸ® 4. ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸

### íŒŒì¼: `scripts/demo_roarm_env.py`
**ëª©ì **: í•™ìŠµ ì „ í™˜ê²½ ê²€ì¦

**ê¸°ëŠ¥**:
- ëœë¤ ì•¡ì…˜ìœ¼ë¡œ ë¡œë´‡ ë™ì‘
- ì£¼ê¸°ì ìœ¼ë¡œ ê·¸ë¦¬í¼ ì—´ê³  ë‹«ê¸°
- ê´€ì¸¡ê°’ ë° ë³´ìƒ ì¶œë ¥
- GUIì—ì„œ ì‹œê°ì  í™•ì¸

**ì‹¤í–‰**:
```bash
~/isaac-sim.sh -m scripts/demo_roarm_env.py --episodes 3 --steps 200
```

**ì˜ˆìƒ ì¶œë ¥**:
```
======================================================================
ğŸ® RoArm-M3 Pick and Place ë°ëª¨
======================================================================
  Episodes: 3
  Max steps per episode: 200

ğŸ’¡ ëœë¤ ì•¡ì…˜ìœ¼ë¡œ ë¡œë´‡ì„ ì›€ì§ì…ë‹ˆë‹¤.
   (í•™ìŠµ ì „ í™˜ê²½ ê²€ì¦ìš©)

======================================================================
ğŸ“º Episode 1/3
======================================================================
Initial observation:
  - Joint positions: [0. 0. 0. 0. 0. 0.]
  - Gripper state: [0. 0.]
  - End-effector position: [0.31  0.    0.335]
  - Object position: [0.298 -0.023  0.05 ]
  - Distance to target: 0.287m

  ğŸ“Š Step 50:
     - Cube position: [0.301 -0.019  0.048]
     - Distance to target: 0.283m
     - Reward: -2.83
     - Total reward: -141.52

  ========================================================
  ğŸ“Š Episode 1 ê²°ê³¼:
     - Total steps: 200
     - Total reward: -564.38
     - Final distance: 0.279m
     - ìƒíƒœ: âŒ Failed
  ========================================================
```

---

## ğŸ“Š 5. íŒŒì¼ êµ¬ì¡°

### ìƒˆë¡œ ìƒì„±ëœ íŒŒì¼
```
roarm_isaac_clean/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py                     # í™˜ê²½ íŒ¨í‚¤ì§€
â”‚   â”œâ”€â”€ roarm_pick_place_env.py (400+)  # í™˜ê²½ êµ¬í˜„
â”‚   â””â”€â”€ README.md                        # í™˜ê²½ ì‚¬ìš© ê°€ì´ë“œ
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_roarm_rl.py (350+)        # PPO í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
â”‚   â”œâ”€â”€ demo_roarm_env.py (120+)        # ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ setup_rl_env.sh                 # í™˜ê²½ ì„¤ì • ìŠ¤í¬ë¦½íŠ¸
â””â”€â”€ logs/
    â””â”€â”€ rl_training/                     # í•™ìŠµ ë¡œê·¸ (ìƒì„± ì˜ˆì •)
        â”œâ”€â”€ tensorboard/
        â”œâ”€â”€ checkpoints/
        â”œâ”€â”€ best_model/
        â””â”€â”€ eval_logs/
```

### ì½”ë“œ í†µê³„
| íŒŒì¼ | ë¼ì¸ ìˆ˜ | ì£¼ìš” ê¸°ëŠ¥ |
|------|--------|----------|
| roarm_pick_place_env.py | ~400 | í™˜ê²½ êµ¬í˜„ |
| train_roarm_rl.py | ~350 | PPO í•™ìŠµ |
| demo_roarm_env.py | ~120 | ë°ëª¨ |
| README.md | ~300 | ë¬¸ì„œ |
| **í•©ê³„** | **~1,170** | - |

---

## ğŸ§ª 6. ê²€ì¦ ê³„íš

### 6.1 í™˜ê²½ ê²€ì¦ (í˜„ì¬ ë‹¨ê³„)
```bash
# ë°ëª¨ ì‹¤í–‰
~/isaac-sim.sh -m scripts/demo_roarm_env.py

# ì²´í¬ë¦¬ìŠ¤íŠ¸:
- [ ] ë¡œë´‡ URDF ì •ìƒ ë¡œë“œ
- [ ] íë¸Œ ë° íƒ€ê²Ÿ ìƒì„±
- [ ] ëœë¤ ì•¡ì…˜ìœ¼ë¡œ ë¡œë´‡ ë™ì‘
- [ ] Observation ì •ìƒ ë°˜í™˜
- [ ] Reward ê³„ì‚° ì •ìƒ
- [ ] ì¢…ë£Œ ì¡°ê±´ ë™ì‘
- [ ] GUIì—ì„œ ì‹œê°ì  í™•ì¸
```

### 6.2 í•™ìŠµ ê²€ì¦ (ë‹¤ìŒ ë‹¨ê³„)
```bash
# ì§§ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸ (10K ìŠ¤í…, ~10ë¶„)
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode train --timesteps 10000

# ëª¨ë‹ˆí„°ë§:
# Terminal 1: í•™ìŠµ ì‹¤í–‰
# Terminal 2: TensorBoard
tensorboard --logdir logs/rl_training/tensorboard

# í™•ì¸ ì‚¬í•­:
- [ ] Reward ì¦ê°€ ì¶”ì„¸
- [ ] Policy loss ê°ì†Œ
- [ ] Episode length ë³€í™”
- [ ] GPU ì‚¬ìš©ë¥  (nvidia-smi)
```

### 6.3 í…ŒìŠ¤íŠ¸ (ìµœì¢… ë‹¨ê³„)
```bash
# Best ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode test

# ê¸°ëŒ€ ê²°ê³¼:
- Success rate: > 40% (50K ìŠ¤í… ê¸°ì¤€)
- Avg reward: > 0
- Smooth motion (í•™ìŠµëœ ì •ì±…)
```

---

## ğŸ“ˆ ê¸°ëŒ€ í•™ìŠµ ê³¡ì„ 

### ì˜ˆìƒ ì„±ëŠ¥ (50K ìŠ¤í… ê¸°ì¤€)
| Timesteps | Success Rate | Avg Reward | ë¹„ê³  |
|-----------|--------------|------------|------|
| 0 (Random) | ~0% | -100 | ì´ˆê¸° ìƒíƒœ |
| 10K | ~5% | -50 | íƒìƒ‰ ë‹¨ê³„ |
| 25K | ~20% | -20 | í•™ìŠµ ì‹œì‘ |
| 50K | ~40% | 0 | ëª©í‘œ ë‹¬ì„± |

**í•™ìŠµ ì‹œê°„ ì˜ˆìƒ**:
- 10K ìŠ¤í…: ~10-15ë¶„
- 50K ìŠ¤í…: ~30-60ë¶„
- 100K ìŠ¤í…: ~1-2ì‹œê°„

---

## ğŸ› ì˜ˆìƒ ë¬¸ì œ ë° í•´ê²°

### ë¬¸ì œ 1: CUDA out of memory
**ì¦ìƒ**: `RuntimeError: CUDA out of memory`  
**ì›ì¸**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡± (Isaac Sim + PyTorch)  
**í•´ê²°**:
```python
# train_roarm_rl.pyì—ì„œ batch_size ê°ì†Œ
batch_size=32  # ê¸°ì¡´: 64

# ë˜ëŠ” headless mode
simulation_app = SimulationApp({"headless": True})
```

### ë¬¸ì œ 2: í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ
**ì¦ìƒ**: Rewardê°€ ê³„ì† ìŒìˆ˜  
**ì›ì¸**: Reward shaping ë¬¸ì œ  
**í•´ê²°**:
```python
# í™˜ê²½ì—ì„œ distance_reward_scale ì¡°ì •
distance_reward_scale = 5.0  # ê¸°ì¡´: 10.0

# ë˜ëŠ” Success threshold ì™„í™”
success_threshold = 0.10  # ê¸°ì¡´: 0.05 (5cm â†’ 10cm)
```

### ë¬¸ì œ 3: ë¡œë´‡ì´ ë„ˆë¬´ ë¹ ë¥´ê²Œ ì›€ì§ì„
**ì¦ìƒ**: ë¶ˆì•ˆì •í•œ ì›€ì§ì„, ì¶©ëŒ  
**ì›ì¸**: ì•¡ì…˜ ìŠ¤ì¼€ì¼ë§ ê³¼ë„  
**í•´ê²°**:
```python
# í™˜ê²½ì—ì„œ max_velocities ê°ì†Œ
max_velocities = np.array([1.0, 0.75, 1.0, 1.25, 1.5, 1.0, 0.03, 0.03])
# ê¸°ì¡´: [2.0, 1.5, 2.0, 2.5, 3.0, 2.0, 0.05, 0.05]
```

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„ (ìš°ì„ ìˆœìœ„)

### ì¦‰ì‹œ (ì˜¤ëŠ˜ ì˜¤í›„)
1. **ë°ëª¨ ì‹¤í–‰** (15ë¶„)
   ```bash
   ~/isaac-sim.sh -m scripts/demo_roarm_env.py
   ```
   - GUIì—ì„œ ë¡œë´‡ ë™ì‘ í™•ì¸
   - ê·¸ë¦¬í¼ ë¹„ìœ¨ ê²€ì¦
   - Observation/Reward ê²€ì¦

2. **ì§§ì€ í•™ìŠµ í…ŒìŠ¤íŠ¸** (20ë¶„)
   ```bash
   ~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode train --timesteps 10000
   ```
   - í™˜ê²½-í•™ìŠµ íŒŒì´í”„ë¼ì¸ ê²€ì¦
   - TensorBoard í™•ì¸

### ë‹¨ê¸° (ì˜¤ëŠ˜ ì €ë…)
3. **ë³¸ê²© í•™ìŠµ** (1-2ì‹œê°„)
   ```bash
   ~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode train --timesteps 50000
   ```
   - 50K ìŠ¤í… í•™ìŠµ
   - ì£¼ê¸°ì ìœ¼ë¡œ TensorBoard ëª¨ë‹ˆí„°ë§

4. **í•™ìŠµ ê²°ê³¼ í‰ê°€** (30ë¶„)
   ```bash
   ~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode test
   ```
   - Best ëª¨ë¸ í…ŒìŠ¤íŠ¸
   - Success rate ì¸¡ì •
   - ë¹„ë””ì˜¤ ë…¹í™”

### ì¤‘ê¸° (ë‚´ì¼)
5. **í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹**
   - Learning rate ì¡°ì •
   - Reward shaping ê°œì„ 
   - 100K ìŠ¤í… ì¬í•™ìŠµ

6. **í™˜ê²½ ê°œì„ **
   - ë¬¼ì²´ ìœ„ì¹˜ ë” ëœë¤í™”
   - ë‹¤ì–‘í•œ ë¬¼ì²´ í¬ê¸°
   - ì¥ì• ë¬¼ ì¶”ê°€ (ì„ íƒ)

---

## ğŸ“š ê¸°ìˆ  ë…¸íŠ¸

### Forward Kinematics ê·¼ì‚¬
**ë¬¸ì œ**: End-effector ìœ„ì¹˜ë¥¼ ì •í™•í•˜ê²Œ ê³„ì‚°í•˜ë ¤ë©´ ë³µì¡í•œ FK í•„ìš”  
**í˜„ì¬ êµ¬í˜„**: ê°„ë‹¨í•œ 2D íˆ¬ì˜ ê·¼ì‚¬

```python
def _get_ee_position(self) -> np.ndarray:
    joint_positions = self.robot.get_joint_positions()
    
    # Zì¶• (ë†’ì´)
    z_base = 0.06
    z_link1 = 0.08
    link2_length = 0.16
    link3_length = 0.15
    
    z_reach = z_base + z_link1 + \
              link2_length * np.sin(joint_positions[1]) + \
              link3_length * np.sin(joint_positions[1] + joint_positions[2])
    
    # X-Y í‰ë©´ (ìˆ˜í‰)
    x_reach = link2_length * np.cos(joint_positions[1]) + \
              link3_length * np.cos(joint_positions[1] + joint_positions[2])
    
    x_offset = x_reach * np.cos(joint_positions[0])
    y_offset = x_reach * np.sin(joint_positions[0])
    
    return np.array([x_offset, y_offset, z_reach])
```

**ê°œì„  ë°©í–¥**:
- Isaac Simì˜ `robot.get_link_world_pose("gripper_base")` ì‚¬ìš©
- ë” ì •í™•í•œ FK (joint_4, joint_5, joint_6 ê³ ë ¤)

### Reward Shaping ì² í•™
**Dense vs Sparse**:
- **Dense**: ë§¤ ìŠ¤í…ë§ˆë‹¤ í”¼ë“œë°± (ê±°ë¦¬ ê¸°ë°˜)
  - ì¥ì : í•™ìŠµ ë¹ ë¦„
  - ë‹¨ì : Local minimum ë¹ ì§ˆ ìˆ˜ ìˆìŒ
  
- **Sparse**: ì„±ê³µ ì‹œì—ë§Œ ë³´ìƒ
  - ì¥ì : ìì—°ìŠ¤ëŸ¬ìš´ í–‰ë™ í•™ìŠµ
  - ë‹¨ì : í•™ìŠµ ë§¤ìš° ëŠë¦¼

**í˜„ì¬ ì„ íƒ**: Dense + Sparse ì¡°í•©
- Distance-based: `-distance * 10.0` (dense)
- Success bonus: `+100.0` (sparse)

---

## ğŸ“Š ì„±ê³¼ ìš”ì•½

### ì •ëŸ‰ì  ì„±ê³¼
| ì§€í‘œ | ë‹¬ì„± | ë¹„ê³  |
|------|------|------|
| í™˜ê²½ êµ¬í˜„ | 400 lines | roarm_pick_place_env.py |
| í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ | 350 lines | train_roarm_rl.py |
| ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸ | 120 lines | demo_roarm_env.py |
| ë¬¸ì„œ | 300 lines | README.md |
| **ì´ ì½”ë“œ** | **1,170 lines** | - |
| **ì‘ì—… ì‹œê°„** | **1ì‹œê°„ 50ë¶„** | 10:40-12:30 |

### ì •ì„±ì  ì„±ê³¼
- âœ… **ì™„ì „í•œ RL íŒŒì´í”„ë¼ì¸**: í™˜ê²½ â†’ í•™ìŠµ â†’ í…ŒìŠ¤íŠ¸
- âœ… **Stable-Baselines3 í†µí•©**: í‘œì¤€ RL ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
- âœ… **Isaac Sim ë„¤ì´í‹°ë¸Œ**: USDê°€ ì•„ë‹Œ URDF ì§ì ‘ ì‚¬ìš©
- âœ… **ì¬ì‚¬ìš© ê°€ëŠ¥**: ë‹¤ë¥¸ ë¡œë´‡ ì‘ì—…ì—ë„ ì ìš© ê°€ëŠ¥
- âœ… **ë¬¸ì„œí™”**: README, ì£¼ì„, íƒ€ì… íŒíŠ¸

---

## ğŸ“ êµí›ˆ

### 1. í™˜ê²½ ì„¤ê³„ì˜ ì¤‘ìš”ì„±
**ë°œê²¬**: Observationê³¼ Reward ì„¤ê³„ê°€ í•™ìŠµ ì„±ëŠ¥ì˜ 80%ë¥¼ ê²°ì •  
**êµí›ˆ**:
- Observation: ì‘ì—…ì— í•„ìš”í•œ ìµœì†Œ ì •ë³´ë§Œ
- Reward: Dense + Sparse ì¡°í•©ìœ¼ë¡œ ì‹œì‘
- Termination: ëª…í™•í•œ ì„±ê³µ/ì‹¤íŒ¨ ì¡°ê±´

### 2. í”„ë¡œí† íƒ€ì´í•‘ ìš°ì„ 
**ì ‘ê·¼**: ì™„ë²½í•œ FKë³´ë‹¤ ê°„ë‹¨í•œ ê·¼ì‚¬ë¡œ ì‹œì‘  
**ê·¼ê±°**:
- ë¹ ë¥¸ ë°˜ë³µ (iteration)
- ë¬¸ì œ íŒŒì•… ìš©ì´
- ì ì§„ì  ê°œì„  ê°€ëŠ¥

**ì ìš©**: FK ê·¼ì‚¬ â†’ í•™ìŠµ ê²€ì¦ â†’ ì •í™•í•œ FKë¡œ êµì²´

### 3. íˆ´ì²´ì¸ ì„ íƒ
**ì„ íƒ**: Stable-Baselines3 > ì§ì ‘ êµ¬í˜„  
**ì´ìœ **:
- ê²€ì¦ëœ ì•Œê³ ë¦¬ì¦˜
- TensorBoard í†µí•©
- ì²´í¬í¬ì¸íŠ¸ ê´€ë¦¬
- ì»¤ë®¤ë‹ˆí‹° ì§€ì›

---

**ì‘ì—… ì¢…ë£Œ ì‹œê°„**: 12:30  
**ë‹¤ìŒ ì„¸ì…˜**: ì˜¤ëŠ˜ ì˜¤í›„ (ë°ëª¨ ì‹¤í–‰ ë° í•™ìŠµ ì‹œì‘)  
**í˜„ì¬ ìƒíƒœ**: ğŸ‰ ê°•í™”í•™ìŠµ í™˜ê²½ êµ¬ì¶• ì™„ë£Œ!

**í•œ ì¤„ ìš”ì•½**:  
> "1ì‹œê°„ 50ë¶„ ë§Œì— ì™„ì „í•œ Pick and Place RL íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì™„ë£Œ!"
