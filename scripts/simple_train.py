#!/usr/bin/env python3
"""
RoArm-M3 ê°„ë‹¨í•œ RL í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ë‹¨ì¼ í™˜ê²½ + PPO ì•Œê³ ë¦¬ì¦˜
"""

import sys
from pathlib import Path

# stdout buffering í•´ê²°
sys.stdout = sys.stderr

from isaacsim import SimulationApp

# Isaac Sim ì´ˆê¸°í™” (headless=True for faster training)
simulation_app = SimulationApp({
    "headless": True,  # GUI ì—†ì´ í•™ìŠµ (ë¹ ë¦„)
    "width": 640,
    "height": 480,
})

print("âœ… Isaac Sim ì´ˆê¸°í™” ì™„ë£Œ\n")

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
import gymnasium as gym
from gymnasium import spaces
import time
from datetime import datetime

# í™˜ê²½ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent))
from envs.roarm_pick_place_env import RoArmPickPlaceEnv, RoArmPickPlaceEnvCfg


class GymWrapper(gym.Env):
    """Isaac Sim í™˜ê²½ì„ Gymnasium í˜•ì‹ìœ¼ë¡œ ë˜í•‘"""
    
    def __init__(self):
        super().__init__()
        
        # í™˜ê²½ ìƒì„±
        print("ğŸ”§ í™˜ê²½ ìƒì„± ì¤‘...")
        cfg = RoArmPickPlaceEnvCfg()
        cfg.episode_length_s = 10.0
        self.env = RoArmPickPlaceEnv(cfg)
        
        # Observation space
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.env.observation_space_dim,),
            dtype=np.float32
        )
        
        # Action space
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.env.action_space_dim,),
            dtype=np.float32
        )
        
        self.episode_count = 0
        self.total_steps = 0
        print("âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ\n")
    
    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
        
        obs = self.env.reset()
        self.episode_count += 1
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥
        if self.episode_count % 10 == 0:
            print(f"ğŸ“Š Episode {self.episode_count} | Total steps: {self.total_steps}")
        
        return obs.astype(np.float32), {}
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.total_steps += 1
        
        # 1000 ìŠ¤í…ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥
        if self.total_steps % 1000 == 0:
            print(f"  â±ï¸  {self.total_steps} steps completed...")
        
        terminated = done
        truncated = False
        
        return obs.astype(np.float32), reward, terminated, truncated, info
    
    def render(self):
        self.env.render()
    
    def close(self):
        self.env.close()


def train_simple_ppo(total_timesteps=50000):
    """ê°„ë‹¨í•œ PPO í•™ìŠµ"""
    
    print("=" * 70)
    print("ğŸš€ RoArm-M3 Pick and Place - Easy Mode Training")
    print("=" * 70)
    print(f"  ì•Œê³ ë¦¬ì¦˜: PPO")
    print(f"  ì´ ìŠ¤í…: {total_timesteps:,}")
    print(f"  ì‹œì‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 70)
    print()
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_dir = Path("logs/rl_training")
    log_dir.mkdir(parents=True, exist_ok=True)
    tensorboard_log = str(log_dir / "tensorboard")
    
    # í™˜ê²½ ìƒì„±
    env = GymWrapper()
    env = Monitor(env, str(log_dir / "monitor"))
    
    print("ğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...")
    # PPO ëª¨ë¸
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        verbose=1,
        tensorboard_log=tensorboard_log,
        device="cuda" if torch.cuda.is_available() else "cpu"
    )
    
    print(f"  Device: {model.device}")
    print(f"  Policy: MlpPolicy")
    print(f"  Learning rate: 3e-4")
    print("âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ\n")
    
    # Callback ì„¤ì •
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=str(log_dir / "checkpoints"),
        name_prefix="roarm_ppo",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    
    print("=" * 70)
    print("ğŸ“ í•™ìŠµ ì‹œì‘!")
    print("=" * 70)
    print()
    
    start_time = time.time()
    
    try:
        # í•™ìŠµ ì‹¤í–‰
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            progress_bar=False,  # Isaac Simê³¼ ì¶©ëŒ ë°©ì§€
        )
        
        elapsed = time.time() - start_time
        
        print()
        print("=" * 70)
        print("âœ… í•™ìŠµ ì™„ë£Œ!")
        print("=" * 70)
        print(f"  ì†Œìš” ì‹œê°„: {elapsed/60:.1f}ë¶„")
        print(f"  í‰ê·  ì†ë„: {total_timesteps/elapsed:.1f} steps/sec")
        print()
        
        # ëª¨ë¸ ì €ì¥
        model_path = log_dir / "final_model"
        model_path.mkdir(parents=True, exist_ok=True)
        final_model = model_path / "roarm_ppo_final.zip"
        model.save(final_model)
        print(f"ğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model}")
        
        return model
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ì‚¬ìš©ì ì¤‘ë‹¨")
        elapsed = time.time() - start_time
        print(f"  í•™ìŠµ ì‹œê°„: {elapsed/60:.1f}ë¶„")
        
        # ì¤‘ë‹¨ ì‹œì—ë„ ëª¨ë¸ ì €ì¥
        interrupted_path = log_dir / "interrupted_model.zip"
        model.save(interrupted_path)
        print(f"ğŸ’¾ ì¤‘ë‹¨ëœ ëª¨ë¸ ì €ì¥: {interrupted_path}")
        
        return model
    
    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise
    
    finally:
        env.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RoArm-M3 ê°„ë‹¨í•œ RL í•™ìŠµ")
    parser.add_argument("--timesteps", type=int, default=50000,
                        help="ì´ í•™ìŠµ ìŠ¤í… (ê¸°ë³¸: 50000)")
    
    args = parser.parse_args()
    
    try:
        model = train_simple_ppo(total_timesteps=args.timesteps)
        print("\nğŸ‰ ëª¨ë“  ì‘ì—… ì™„ë£Œ!")
        
    finally:
        simulation_app.close()
        print("ğŸ‘‹ Simulation ì¢…ë£Œ")
