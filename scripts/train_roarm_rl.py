#!/usr/bin/env python3
"""
RoArm-M3 Pick and Place ê°•í™”í•™ìŠµ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
Stable-Baselines3 PPO ì•Œê³ ë¦¬ì¦˜ ì‚¬ìš©
"""

import sys
from pathlib import Path

# Isaac Sim í™˜ê²½ ì„¤ì •
isaac_sim_path = Path.home() / "isaacsim"
sys.path.append(str(isaac_sim_path))

from isaacsim import SimulationApp

# Isaac Sim ì´ˆê¸°í™” (headless mode for training)
simulation_app = SimulationApp({
    "headless": False,  # GUIë¡œ í™•ì¸í•˜ë ¤ë©´ False
    "width": 1280,
    "height": 720,
})

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
import gymnasium as gym
from gymnasium import spaces

# í™˜ê²½ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent))
from envs.roarm_pick_place_env import RoArmPickPlaceEnv, RoArmPickPlaceEnvCfg


class GymWrapper(gym.Env):
    """
    Isaac Sim í™˜ê²½ì„ Gymnasium í˜•ì‹ìœ¼ë¡œ ë˜í•‘
    Stable-Baselines3ì™€ í˜¸í™˜
    """
    
    def __init__(self):
        super().__init__()
        
        # í™˜ê²½ ìƒì„±
        cfg = RoArmPickPlaceEnvCfg()
        cfg.episode_length_s = 10.0  # 10ì´ˆ ì—í”¼ì†Œë“œ
        self.env = RoArmPickPlaceEnv(cfg)
        
        # Observation space (15 dim)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.env.observation_space_dim,),
            dtype=np.float32
        )
        
        # Action space (8 dim, normalized [-1, 1])
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.env.action_space_dim,),
            dtype=np.float32
        )
    
    def reset(self, seed=None, options=None):
        """í™˜ê²½ ë¦¬ì…‹"""
        if seed is not None:
            np.random.seed(seed)
        
        obs = self.env.reset()
        return obs.astype(np.float32), {}
    
    def step(self, action):
        """í™˜ê²½ ìŠ¤í…"""
        obs, reward, done, info = self.env.step(action)
        
        # Gymnasium API: (obs, reward, terminated, truncated, info)
        terminated = done
        truncated = False
        
        return obs.astype(np.float32), reward, terminated, truncated, info
    
    def render(self):
        """ë Œë”ë§ (Isaac Simì—ì„œ ìë™)"""
        self.env.render()
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        self.env.close()


def make_env():
    """í™˜ê²½ ìƒì„± í•¨ìˆ˜"""
    env = GymWrapper()
    env = Monitor(env)
    return env


def train_ppo(
    total_timesteps: int = 50000,
    save_dir: str = "logs/rl_training",
    checkpoint_freq: int = 5000,
    eval_freq: int = 2000,
):
    """
    PPO ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµ
    
    Args:
        total_timesteps: ì´ í•™ìŠµ ìŠ¤í… ìˆ˜
        save_dir: ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬
        checkpoint_freq: ì²´í¬í¬ì¸íŠ¸ ì €ì¥ ì£¼ê¸°
        eval_freq: í‰ê°€ ì£¼ê¸°
    """
    print("=" * 60)
    print("ğŸš€ RoArm-M3 Pick and Place PPO Training")
    print("=" * 60)
    print(f"  Total timesteps: {total_timesteps:,}")
    print(f"  Save directory: {save_dir}")
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)
    
    # í™˜ê²½ ìƒì„±
    print(f"\nğŸ”§ í™˜ê²½ ìƒì„± ì¤‘...")
    env = DummyVecEnv([make_env])
    
    # í‰ê°€ìš© í™˜ê²½
    eval_env = DummyVecEnv([make_env])
    
    # PPO ëª¨ë¸ ìƒì„±
    print(f"\nğŸ§  PPO ëª¨ë¸ ìƒì„± ì¤‘...")
    model = PPO(
        policy="MlpPolicy",
        env=env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log=str(save_path / "tensorboard"),
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    
    print(f"  âœ… Device: {model.device}")
    print(f"  âœ… Policy network:")
    print(f"      - Observation space: {env.observation_space.shape}")
    print(f"      - Action space: {env.action_space.shape}")
    
    # Callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=checkpoint_freq,
        save_path=str(save_path / "checkpoints"),
        name_prefix="roarm_ppo",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=str(save_path / "best_model"),
        log_path=str(save_path / "eval_logs"),
        eval_freq=eval_freq,
        deterministic=True,
        render=False,
        n_eval_episodes=5,
    )
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸ“š í•™ìŠµ ì‹œì‘...")
    print(f"{'='*60}\n")
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=[checkpoint_callback, eval_callback],
            progress_bar=True,
        )
        
        # ìµœì¢… ëª¨ë¸ ì €ì¥
        final_model_path = save_path / "roarm_ppo_final.zip"
        model.save(str(final_model_path))
        print(f"\nâœ… í•™ìŠµ ì™„ë£Œ!")
        print(f"  ìµœì¢… ëª¨ë¸ ì €ì¥: {final_model_path}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ ì‚¬ìš©ì ì¤‘ë‹¨ - í˜„ì¬ê¹Œì§€ í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ì¤‘...")
        interrupted_model_path = save_path / "roarm_ppo_interrupted.zip"
        model.save(str(interrupted_model_path))
        print(f"  ì €ì¥ ì™„ë£Œ: {interrupted_model_path}")
    
    finally:
        env.close()
        eval_env.close()
    
    return model


def test_trained_model(model_path: str, num_episodes: int = 5):
    """
    í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸
    
    Args:
        model_path: ëª¨ë¸ íŒŒì¼ ê²½ë¡œ (.zip)
        num_episodes: í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜
    """
    print("=" * 60)
    print("ğŸ® í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print(f"  Model: {model_path}")
    print(f"  Episodes: {num_episodes}")
    
    # í™˜ê²½ ìƒì„±
    env = make_env()
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"\nğŸ”§ ëª¨ë¸ ë¡œë”© ì¤‘...")
    model = PPO.load(model_path)
    print(f"  âœ… ë¡œë“œ ì™„ë£Œ")
    
    # í…ŒìŠ¤íŠ¸
    success_count = 0
    total_rewards = []
    
    for episode in range(num_episodes):
        print(f"\n{'='*60}")
        print(f"Episode {episode + 1}/{num_episodes}")
        print(f"{'='*60}")
        
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        step = 0
        
        while not done:
            # ëª¨ë¸ë¡œ ì•¡ì…˜ ì˜ˆì¸¡
            action, _states = model.predict(obs, deterministic=True)
            
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            
            episode_reward += reward
            step += 1
            
            if step % 50 == 0:
                print(f"  Step {step}: Distance={info['distance_to_target']:.3f}m")
        
        total_rewards.append(episode_reward)
        
        # Success íŒì •
        if info['distance_to_target'] < 0.05:
            success_count += 1
            print(f"  âœ… SUCCESS!")
        else:
            print(f"  âŒ Failed")
        
        print(f"  ğŸ“Š Episode reward: {episode_reward:.2f}")
    
    # í†µê³„
    print(f"\n{'='*60}")
    print(f"ğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"  Success rate: {success_count}/{num_episodes} ({success_count/num_episodes*100:.1f}%)")
    print(f"  Average reward: {np.mean(total_rewards):.2f} Â± {np.std(total_rewards):.2f}")
    print(f"  Min/Max reward: {np.min(total_rewards):.2f} / {np.max(total_rewards):.2f}")
    
    env.close()


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="RoArm-M3 Pick and Place RL Training")
    parser.add_argument("--mode", choices=["train", "test"], default="train",
                        help="í•™ìŠµ(train) ë˜ëŠ” í…ŒìŠ¤íŠ¸(test)")
    parser.add_argument("--timesteps", type=int, default=50000,
                        help="ì´ í•™ìŠµ ìŠ¤í… ìˆ˜")
    parser.add_argument("--model", type=str, default=None,
                        help="í…ŒìŠ¤íŠ¸í•  ëª¨ë¸ ê²½ë¡œ (.zip)")
    parser.add_argument("--episodes", type=int, default=5,
                        help="í…ŒìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜")
    
    args = parser.parse_args()
    
    try:
        if args.mode == "train":
            # í•™ìŠµ
            model = train_ppo(total_timesteps=args.timesteps)
            
        elif args.mode == "test":
            # í…ŒìŠ¤íŠ¸
            if args.model is None:
                # ìµœì‹  best ëª¨ë¸ ì°¾ê¸°
                best_model_path = Path("logs/rl_training/best_model/best_model.zip")
                if best_model_path.exists():
                    args.model = str(best_model_path)
                else:
                    print("âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•˜ì„¸ìš”: --model <path>")
                    sys.exit(1)
            
            test_trained_model(args.model, num_episodes=args.episodes)
    
    finally:
        simulation_app.close()
        print("\nğŸ‘‹ Simulation ì¢…ë£Œ")
