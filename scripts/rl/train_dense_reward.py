#!/usr/bin/env python3
"""
RoArm-M3 Dense Reward RL í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸
ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜ë¡œ ë” ë‚˜ì€ í•™ìŠµ ì„±ëŠ¥
"""

import sys
from pathlib import Path

# stdout buffering í•´ê²°
sys.stdout = sys.stderr

from isaacsim import SimulationApp

# Isaac Sim ì´ˆê¸°í™” (headless=True for faster training)
simulation_app = SimulationApp({
    "headless": True,  # GUI ì—†ì´ í•™ìŠµ (ë¹ ë¦„)
    "width": 640,
    "height": 480,
})

print("âœ… Isaac Sim ì´ˆê¸°í™” ì™„ë£Œ (Dense Reward í™˜ê²½)\n")

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize
import gymnasium as gym
from gymnasium import spaces
import time
from datetime import datetime

# í™˜ê²½ ì„í¬íŠ¸
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))
print(f"ğŸ“ Project root: {project_root}")

from envs.roarm_pick_place_env import RoArmPickPlaceEnv, RoArmPickPlaceEnvCfg

# Early warning callback (ì˜µì…˜)
try:
    from scripts.rl.early_warning_callback import EarlyWarningCallback
    EARLY_WARNING_AVAILABLE = True
except ImportError:
    print("âš ï¸ EarlyWarningCallback not found, skipping")
    EarlyWarningCallback = None
    EARLY_WARNING_AVAILABLE = False

# Training progress callback
try:
    from scripts.rl.training_callbacks import TrainingProgressCallback, CurriculumCallback
    TRAINING_CALLBACKS_AVAILABLE = True
except ImportError:
    print("âš ï¸ Training callbacks not found, using basic logging")
    TrainingProgressCallback = None
    CurriculumCallback = None
    TRAINING_CALLBACKS_AVAILABLE = False


class GymWrapper(gym.Env):
    """Isaac Sim í™˜ê²½ì„ Gymnasium í˜•ì‹ìœ¼ë¡œ ë˜í•‘
    
    âœ… TimeLimit ëª…ì‹œì  ì„¤ì • (max_episode_steps)
    """
    
    def __init__(self):
        super().__init__()
        
        # í™˜ê²½ ìƒì„±
        print("ğŸ”§ í™˜ê²½ ìƒì„± ì¤‘ (Shaped-Sparse + Curriculum)...")
        cfg = RoArmPickPlaceEnvCfg()
        cfg.episode_length_s = 10.0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“š CURRICULUM LEARNING í™œì„±í™”
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        cfg.curriculum_enabled = True
        cfg.curriculum_phase = 0  # Phase 0: Easy Mode
        
        self.env = RoArmPickPlaceEnv(cfg)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # âœ… TimeLimit ëª…ì‹œì  ì„¤ì • (ì—í”¼ì†Œë“œ ê¸¸ì´ ë³´ì¥!)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Isaac Sim í™˜ê²½ì˜ max_stepsë¥¼ Gymnasium TimeLimitìœ¼ë¡œ ì „ë‹¬
        self._max_episode_steps = self.env.max_steps
        print(f"  âœ… Max episode steps: {self._max_episode_steps} (from env.max_steps)")
        
        # Observation space (28 dim now!)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.env.observation_space_dim,),
            dtype=np.float32
        )
        
        # Action space
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.env.action_space_dim,),
            dtype=np.float32
        )
        
        # ì—í”¼ì†Œë“œ ì¶”ì 
        self.episode_count = 0
        self.total_steps = 0
        self._elapsed_steps = 0  # í˜„ì¬ ì—í”¼ì†Œë“œ ìŠ¤í… ìˆ˜
        print("âœ… Shaped-Sparse + Curriculum í™˜ê²½ ìƒì„± ì™„ë£Œ\n")
    
    def reset(self, seed=None, options=None):
        if seed is not None:
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            # âœ… ì‹œë“œ ì „íŒŒ (ì¬í˜„ì„± ê°•í™”!)
            # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
            np.random.seed(seed)
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
        
        obs = self.env.reset()
        self.episode_count += 1
        self._elapsed_steps = 0  # ì—í”¼ì†Œë“œ ìŠ¤í… ì¹´ìš´í„° ë¦¬ì…‹
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥ (10 â†’ 50 ì—í”¼ì†Œë“œë§ˆë‹¤, ìŠ¤íŒ¸ ì¤„ì´ê¸°!)
        if self.episode_count % 50 == 0:
            print(f"ğŸ“Š Episode {self.episode_count} | Total steps: {self.total_steps}")
        
        return obs, {}
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.total_steps += 1
        self._elapsed_steps += 1
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # âœ… TimeLimit ëª…ì‹œì  ì²´í¬ (truncation ë¶„ë¦¬)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # í™˜ê²½ ë‚´ë¶€ done: SUCCESS, ì•ˆì „ ìœ„ë°˜ ë“± â†’ terminated
        # TimeLimit ë„ë‹¬: max_episode_steps â†’ truncated
        terminated = done and self._elapsed_steps < self._max_episode_steps
        truncated = self._elapsed_steps >= self._max_episode_steps
        
        # âœ… TimeLimit truncationì„ infoì— ëª…ì‹œ (SB3/Monitorê°€ ì¸ì‹)
        if truncated:
            info["TimeLimit.truncated"] = True
            print(f"  â±ï¸ TimeLimit reached: {self._elapsed_steps}/{self._max_episode_steps} steps")
        
        return obs, reward, terminated, truncated, info


def train(timesteps: int = 200000):
    """RL í•™ìŠµ ì‹¤í–‰ (Curriculum + Shaped-Sparse)"""
    print("=" * 60)
    print("ğŸš€ RoArm-M3 Curriculum + Shaped-Sparse RL í•™ìŠµ ì‹œì‘")
    print("=" * 60)
    print(f"  ëª©í‘œ timesteps: {timesteps:,}")
    print(f"  í•™ìŠµ ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
    log_dir = Path("logs/rl_training_curriculum")  # Curriculum ë¡œê·¸
    log_dir.mkdir(parents=True, exist_ok=True)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # âœ… SB3 ê¶Œì¥ íŒ¨í„´: DummyVecEnv([lambda: Monitor(env)])
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    def make_env():
        """í™˜ê²½ ìƒì„± íŒ©í† ë¦¬ (Monitor í¬í•¨)"""
        env = GymWrapper()
        env = Monitor(env, str(log_dir / "monitor.monitor.csv"))
        return env
    
    print("ğŸ”§ í™˜ê²½ ìƒì„± ì¤‘...")
    env = DummyVecEnv([make_env])
    
    # âœ… VecNormalize ì ìš©
    print("ğŸ”§ VecNormalize ì ìš© ì¤‘...")
    env = VecNormalize(
        env,
        norm_obs=True,          # ê´€ì°° ì •ê·œí™” âœ…
        norm_reward=True,       # ë³´ìƒ ì •ê·œí™” âœ…
        clip_obs=10.0,          # ê´€ì°° í´ë¦½
        clip_reward=10.0,       # ë¦¬í„´ í´ë¦½ âœ…
        gamma=0.99,
    )
    print(f"  âœ… Observation normalization: ON")
    print(f"  âœ… Reward normalization: ON")
    print(f"  âœ… Clip reward: Â±10.0")
    print()
    
    print(f"ğŸ“‚ ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}\n")
    
    # ì½œë°± ì„¤ì •
    callbacks = []
    
    # 1. í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… (NEW!)
    if TRAINING_CALLBACKS_AVAILABLE:
        progress_callback = TrainingProgressCallback(
            verbose=1,
            log_freq=10,  # 10 ì—í”¼ì†Œë“œë§ˆë‹¤ ì¶œë ¥
        )
        callbacks.append(progress_callback)
        print("âœ… í•™ìŠµ ì§„í–‰ ìƒí™© ë¡œê¹… í™œì„±í™” (10 ì—í”¼ì†Œë“œë§ˆë‹¤)")
    
    # 2. Curriculum ìë™ ìŠ¹ê¸‰ (NEW!)
    if TRAINING_CALLBACKS_AVAILABLE:
        # env_wrapperëŠ” DummyVecEnvì˜ ì²« ë²ˆì§¸ í™˜ê²½
        env_wrapper = env.envs[0]
        curriculum_callback = CurriculumCallback(
            env_wrapper=env_wrapper,
            success_window=100,
            phase_0_threshold=0.30,  # 30% ì„±ê³µë¥ ë¡œ Phase 1
            phase_1_threshold=0.60,  # 60% ì„±ê³µë¥ ë¡œ Phase 2
            verbose=1,
        )
        callbacks.append(curriculum_callback)
        print("âœ… Curriculum ìë™ ìŠ¹ê¸‰ í™œì„±í™”")
        print("   - Phase 0â†’1: ì„±ê³µë¥  30% (100 ì—í”¼ì†Œë“œ)")
        print("   - Phase 1â†’2: ì„±ê³µë¥  60% (100 ì—í”¼ì†Œë“œ)")
    
    # 3. ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=str(log_dir / "checkpoints"),
        name_prefix="roarm_ppo_curriculum",
        save_replay_buffer=False,
        save_vecnormalize=True,  # âœ… VecNormalize í†µê³„ ì €ì¥
    )
    callbacks.append(checkpoint_callback)
    
    # 4. ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ (ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°ë§Œ)
    if EARLY_WARNING_AVAILABLE:
        early_warning_callback = EarlyWarningCallback(
            ev_threshold=0.05,       # EV < 0.05
            ev_consecutive=5,        # 5íšŒ ì—°ì†
            vl_consecutive=5,        # VL ì¦ê°€ 5íšŒ ì—°ì†
            verbose=1,
        )
        callbacks.append(early_warning_callback)
        print("âœ… ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ í™œì„±í™”")
        print("   - EV < 0.05 ì—°ì† 5íšŒ â†’ ìë™ ì¤‘ë‹¨")
        print("   - Value Loss ì¦ê°€ ì—°ì† 5íšŒ â†’ ìë™ ì¤‘ë‹¨")
    else:
        print("âš ï¸ ì¡°ê¸° ê²½ë³´ ì‹œìŠ¤í…œ ë¹„í™œì„±í™” (ëª¨ë“ˆ ì—†ìŒ)")
    print()
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ¯ Curriculum + Shaped-Sparse í•™ìŠµ ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. Phase 0: Easy Mode (íë¸Œ 10~15cm, íƒ€ê²Ÿ 20~25cm)
    # 2. Shaped-Sparse ë³´ìƒ (ê²Œì´íŒ… + 1íšŒì„± ì´ë²¤íŠ¸)
    # 3. íƒìƒ‰ ê°•í™” (ent_coef=0.01)
    # 4. Value Clipping + target_kl ìœ ì§€
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    
    print("ğŸ¤– PPO ëª¨ë¸ ìƒì„± ì¤‘ (Curriculum + Shaped-Sparse)...")
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=3e-4,      # ê¸°ë³¸ê°’
        n_steps=2048,
        batch_size=64,
        n_epochs=10,             # ê¸°ë³¸ê°’
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        clip_range_vf=1.0,       # âœ… Value Clipping
        ent_coef=0.01,           # âœ… íƒìƒ‰ ê°•í™” (Shaped-Sparseìš©)
        vf_coef=0.5,             # ê¸°ë³¸ê°’
        max_grad_norm=0.5,
        target_kl=0.03,          # âœ… ì •ì±… ì•ˆì •ì„±
        verbose=1,
        device="cuda" if torch.cuda.is_available() else "cpu",
        tensorboard_log=str(log_dir / "tensorboard"),
    )
    print(f"  Device: {model.device}")
    print(f"  Policy: MlpPolicy")
    print(f"  ğŸ”§ Learning Rate: 3e-4")
    print(f"  ğŸ”§ n_epochs: 10")
    print(f"  ğŸ”§ vf_coef: 0.5")
    print(f"  âœ… clip_range_vf: 1.0")
    print(f"  âœ… ent_coef: 0.01 (íƒìƒ‰ ê°•í™”)")
    print(f"  âœ… target_kl: 0.03")
    print(f"  ğŸ“š Curriculum: Phase 0 (Easy Mode)")
    print(f"  ğŸ¯ Shaped-Sparse: ê·¼ì ‘(+5) ê·¸ë¦½(+10) ë¦¬í”„íŠ¸(+15) ëª©í‘œ(+20) Success(+100)")
    print()
    
    # í•™ìŠµ ì‹œì‘
    print("ğŸ“ í•™ìŠµ ì‹œì‘...\n")
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=timesteps,
            callback=callbacks,  # âœ… ì¡°ê±´ë¶€ ì½œë°± ë¦¬ìŠ¤íŠ¸
            progress_bar=False,
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ (Ctrl+C)")
    
    # í•™ìŠµ ì™„ë£Œ
    elapsed_time = time.time() - start_time
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  ì´ ì‹œê°„: {elapsed_time / 60:.1f}ë¶„")
    print(f"  ì¢…ë£Œ ì‹œê°: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # âœ… ìµœì¢… ëª¨ë¸ + VecNormalize í†µê³„ ì €ì¥
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    final_model_dir = log_dir / "final_model"
    final_model_dir.mkdir(parents=True, exist_ok=True)
    
    # 1. PPO ëª¨ë¸ ì €ì¥
    model_path = final_model_dir / "roarm_ppo_dense_final.zip"
    model.save(str(model_path))
    print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")
    
    # 2. VecNormalize í†µê³„ ì €ì¥ (ì¬í˜„ í•„ìˆ˜!)
    vecnorm_path = final_model_dir / "vecnormalize.pkl"
    env.save(str(vecnorm_path))
    print(f"ğŸ’¾ VecNormalize í†µê³„ ì €ì¥: {vecnorm_path}")
    print("   âš ï¸ í…ŒìŠ¤íŠ¸ ì‹œ ë°˜ë“œì‹œ í•¨ê»˜ ë¡œë“œí•˜ì„¸ìš”!")
    
    return model


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="RoArm-M3 Dense Reward RL Training")
    parser.add_argument(
        "--timesteps",
        type=int,
        default=100000,
        help="ì´ í•™ìŠµ timesteps (ê¸°ë³¸: 100,000)"
    )
    
    args = parser.parse_args()
    
    try:
        train(args.timesteps)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ”š Isaac Sim ì¢…ë£Œ ì¤‘...")
        simulation_app.close()


if __name__ == "__main__":
    main()
