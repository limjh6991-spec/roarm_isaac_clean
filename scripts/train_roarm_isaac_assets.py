#!/usr/bin/env python3
"""
RoArm M3 Pick and Place PPO í•™ìŠµ - Isaac Assets ë²„ì „
"""

import argparse
import sys
from pathlib import Path

# Isaac Sim í™˜ê²½ ì„¤ì •
isaac_sim_path = Path.home() / "isaacsim"
sys.path.append(str(isaac_sim_path))

# Isaac Sim ì´ˆê¸°í™” (ìµœìš°ì„ )
from isaacsim import SimulationApp

def parse_args():
    parser = argparse.ArgumentParser(description="RoArm Isaac Assets RL Training")
    parser.add_argument("--mode", type=str, default="train", choices=["train", "test", "demo"])
    parser.add_argument("--level", type=str, default="easy", choices=["easy", "medium", "hard", "mixed"])
    parser.add_argument("--timesteps", type=int, default=50000)
    parser.add_argument("--episodes", type=int, default=10)
    parser.add_argument("--render", type=str, default="true", choices=["true", "false"])
    parser.add_argument("--warehouse", type=str, default="false", choices=["true", "false"])
    parser.add_argument("--model", type=str, default=None, help="í•™ìŠµëœ ëª¨ë¸ ê²½ë¡œ (test mode)")
    return parser.parse_args()

args = parse_args()

# SimulationApp ìƒì„±
simulation_app = SimulationApp({
    "headless": args.render == "false",
    "width": 1280,
    "height": 720,
})

# ì´ì œ ë‚˜ë¨¸ì§€ imports
import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv
import gymnasium as gym
from gymnasium import spaces

# í”„ë¡œì íŠ¸ ê²½ë¡œ
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.append(str(PROJECT_ROOT))

# í™˜ê²½ Import
from envs.roarm_pickplace_isaac_assets import RoArmPickPlaceIsaacEnv


class GymWrapper(gym.Env):
    """Isaac Sim í™˜ê²½ â†’ Gymnasium API"""
    
    def __init__(self, curriculum_level="easy", use_warehouse=False):
        super().__init__()
        
        # Isaac Sim í™˜ê²½ ìƒì„±
        self.env = RoArmPickPlaceIsaacEnv(
            curriculum_level=curriculum_level,
            use_warehouse=use_warehouse,
            render=args.render == "true"
        )
        
        # Gymnasium spaces
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(self.env.observation_dim,),
            dtype=np.float32
        )
        
        self.action_space = spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(self.env.action_dim,),
            dtype=np.float32
        )
    
    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
        obs = self.env.reset()
        return obs, {}
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        truncated = False  # Gymnasium API
        return obs, reward, done, truncated, info
    
    def close(self):
        self.env.close()


def train_ppo(curriculum_level="easy", use_warehouse=False, total_timesteps=50000):
    """PPO í•™ìŠµ"""
    print(f"\nğŸ“ PPO í•™ìŠµ ì‹œì‘ (Level: {curriculum_level})")
    print(f"   Total timesteps: {total_timesteps:,}")
    print(f"   Device: {'cuda' if torch.cuda.is_available() else 'cpu'}")
    
    # í™˜ê²½ ìƒì„±
    def make_env():
        return GymWrapper(
            curriculum_level=curriculum_level,
            use_warehouse=use_warehouse
        )
    
    env = DummyVecEnv([make_env])
    
    # ë¡œê·¸ ë””ë ‰í† ë¦¬
    log_dir = PROJECT_ROOT / "logs" / "rl_training_isaac" / f"level_{curriculum_level}"
    log_dir.mkdir(parents=True, exist_ok=True)
    
    checkpoint_dir = log_dir / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    
    tensorboard_dir = log_dir / "tensorboard"
    tensorboard_dir.mkdir(exist_ok=True)
    
    best_model_dir = log_dir / "best_model"
    best_model_dir.mkdir(exist_ok=True)
    
    # PPO ëª¨ë¸
    model = PPO(
        policy="MlpPolicy",
        env=env,
        learning_rate=3e-4,
        n_steps=2048,
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        vf_coef=0.5,
        max_grad_norm=0.5,
        verbose=1,
        tensorboard_log=str(tensorboard_dir),
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    
    # Callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=str(checkpoint_dir),
        name_prefix="roarm_ppo"
    )
    
    eval_callback = EvalCallback(
        env,
        best_model_save_path=str(best_model_dir),
        log_path=str(log_dir),
        eval_freq=2000,
        deterministic=True,
        render=False
    )
    
    # í•™ìŠµ ì‹œì‘
    print(f"\nğŸ“Š TensorBoard: tensorboard --logdir {tensorboard_dir}")
    print(f"ğŸ“ Checkpoints: {checkpoint_dir}")
    print(f"ğŸ† Best model: {best_model_dir}")
    print("\nğŸš€ í•™ìŠµ ì‹œì‘...\n")
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=[checkpoint_callback, eval_callback],
        progress_bar=True
    )
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_path = log_dir / "final_model.zip"
    model.save(str(final_model_path))
    print(f"\nâœ… í•™ìŠµ ì™„ë£Œ! ìµœì¢… ëª¨ë¸: {final_model_path}")
    
    env.close()


def test_trained_model(model_path=None, num_episodes=10, curriculum_level="easy"):
    """í•™ìŠµëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
    print(f"\nğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ (Level: {curriculum_level})")
    
    # ëª¨ë¸ ê²½ë¡œ ìë™ ì°¾ê¸°
    if model_path is None:
        best_model_path = PROJECT_ROOT / "logs" / "rl_training_isaac" / f"level_{curriculum_level}" / "best_model" / "best_model.zip"
        if best_model_path.exists():
            model_path = str(best_model_path)
            print(f"   Best model ì‚¬ìš©: {model_path}")
        else:
            print(f"âŒ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {best_model_path}")
            return
    
    # í™˜ê²½ ìƒì„±
    env = GymWrapper(curriculum_level=curriculum_level, use_warehouse=False)
    
    # ëª¨ë¸ ë¡œë“œ
    model = PPO.load(model_path)
    
    # í…ŒìŠ¤íŠ¸
    success_count = 0
    total_rewards = []
    
    for ep in range(num_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0.0
        steps = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if done or truncated:
                if info.get("success", False):
                    success_count += 1
                    print(f"   Episode {ep + 1}: âœ… SUCCESS (reward={episode_reward:.2f}, steps={steps})")
                else:
                    print(f"   Episode {ep + 1}: âŒ FAIL (reward={episode_reward:.2f}, reason={info.get('reason', 'unknown')})")
                total_rewards.append(episode_reward)
                break
    
    # í†µê³„
    success_rate = (success_count / num_episodes) * 100
    avg_reward = np.mean(total_rewards)
    
    print(f"\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   Success Rate: {success_rate:.1f}% ({success_count}/{num_episodes})")
    print(f"   Avg Reward: {avg_reward:.2f}")
    print(f"   Min/Max Reward: {np.min(total_rewards):.2f} / {np.max(total_rewards):.2f}")
    
    env.close()


def demo_environment(curriculum_level="easy", use_warehouse=False, num_episodes=3):
    """í™˜ê²½ ë°ëª¨ (ëœë¤ ì•¡ì…˜)"""
    print(f"\nğŸ¬ í™˜ê²½ ë°ëª¨ (Level: {curriculum_level})")
    
    env = GymWrapper(curriculum_level=curriculum_level, use_warehouse=use_warehouse)
    
    for ep in range(num_episodes):
        print(f"\nğŸ“ Episode {ep + 1}/{num_episodes}")
        obs, _ = env.reset()
        done = False
        episode_reward = 0.0
        steps = 0
        
        while not done and steps < 200:  # ìµœëŒ€ 200 ìŠ¤í…
            # ëœë¤ ì•¡ì…˜
            action = env.action_space.sample()
            
            # ì£¼ê¸°ì ìœ¼ë¡œ ê·¸ë¦¬í¼ ì—´ê³  ë‹«ê¸°
            if steps % 50 == 0:
                action[6:8] = 1.0 if (steps // 50) % 2 == 0 else -1.0
            
            obs, reward, done, truncated, info = env.step(action)
            episode_reward += reward
            steps += 1
            
            if steps % 50 == 0:
                print(f"   Step {steps}: Reward={reward:.2f}, Total={episode_reward:.2f}")
        
        if done:
            print(f"   Episode ì¢…ë£Œ: {info}")
        print(f"   Total Reward: {episode_reward:.2f}")
    
    env.close()
    print("\nâœ… ë°ëª¨ ì™„ë£Œ!")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=" * 60)
    print("ğŸ¤– RoArm M3 Pick & Place - Isaac Assets RL")
    print("=" * 60)
    
    use_warehouse = args.warehouse == "true"
    
    try:
        if args.mode == "train":
            train_ppo(
                curriculum_level=args.level,
                use_warehouse=use_warehouse,
                total_timesteps=args.timesteps
            )
        
        elif args.mode == "test":
            test_trained_model(
                model_path=args.model,
                num_episodes=args.episodes,
                curriculum_level=args.level
            )
        
        elif args.mode == "demo":
            demo_environment(
                curriculum_level=args.level,
                use_warehouse=use_warehouse,
                num_episodes=args.episodes
            )
    
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        simulation_app.close()
        print("\nâœ… í”„ë¡œê·¸ë¨ ì¢…ë£Œ")


if __name__ == "__main__":
    main()
