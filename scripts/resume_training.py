#!/usr/bin/env python3
"""
RoArm-M3 Dense Reward RL í•™ìŠµ ì¬ê°œ ìŠ¤í¬ë¦½íŠ¸
ë§ˆì§€ë§‰ ì²´í¬í¬ì¸íŠ¸ë¶€í„° ê³„ì† í•™ìŠµ
"""

import sys
from pathlib import Path

# stdout buffering í•´ê²°
sys.stdout = sys.stderr

from isaacsim import SimulationApp

# Isaac Sim ì´ˆê¸°í™” (headless=True for faster training)
simulation_app = SimulationApp({
    "headless": True,
    "width": 640,
    "height": 480,
})

print("âœ… Isaac Sim ì´ˆê¸°í™” ì™„ë£Œ (í•™ìŠµ ì¬ê°œ)\n")

import numpy as np
import torch
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.monitor import Monitor
import gymnasium as gym
from gymnasium import spaces
import time
from datetime import datetime

# í™˜ê²½ ì„í¬íŠ¸
sys.path.append(str(Path(__file__).parent.parent))
from envs.roarm_pick_place_env import RoArmPickPlaceEnv, RoArmPickPlaceEnvCfg


class GymWrapper(gym.Env):
    """Isaac Sim í™˜ê²½ì„ Gymnasium í˜•ì‹ìœ¼ë¡œ ë˜í•‘"""
    
    def __init__(self):
        super().__init__()
        
        print("ğŸ”§ í™˜ê²½ ìƒì„± ì¤‘ (Dense Reward)...")
        cfg = RoArmPickPlaceEnvCfg()
        cfg.episode_length_s = 10.0
        self.env = RoArmPickPlaceEnv(cfg)
        
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf,
            shape=(self.env.observation_space_dim,), dtype=np.float32
        )
        self.action_space = spaces.Box(
            low=-1.0, high=1.0,
            shape=(self.env.action_space_dim,), dtype=np.float32
        )
        
        self.episode_count = 0
        self.total_steps = 0
        print("âœ… í™˜ê²½ ìƒì„± ì™„ë£Œ\n")
    
    def reset(self, seed=None, options=None):
        if seed is not None:
            np.random.seed(seed)
        obs = self.env.reset()
        self.episode_count += 1
        if self.episode_count % 10 == 0:
            print(f"ğŸ“Š Episode {self.episode_count} | Total steps: {self.total_steps}")
        return obs, {}
    
    def step(self, action):
        obs, reward, done, info = self.env.step(action)
        self.total_steps += 1
        terminated = done
        truncated = False
        return obs, reward, terminated, truncated, info


def resume_training(checkpoint_path: str, additional_timesteps: int = 50000):
    """ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ"""
    print("=" * 60)
    print("ğŸ”„ í•™ìŠµ ì¬ê°œ")
    print("=" * 60)
    print(f"  ì²´í¬í¬ì¸íŠ¸: {checkpoint_path}")
    print(f"  ì¶”ê°€ timesteps: {additional_timesteps:,}")
    print(f"  ì‹œì‘: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # í™˜ê²½ ìƒì„±
    env = GymWrapper()
    
    # Monitorë¡œ ë˜í•‘
    log_dir = Path("logs/rl_training_dense")
    env = Monitor(env, str(log_dir / "monitor.monitor.csv"), allow_early_resets=True)
    
    print(f"ğŸ“‚ ë¡œê·¸ ë””ë ‰í† ë¦¬: {log_dir}\n")
    
    # ì²´í¬í¬ì¸íŠ¸ ì½œë°±
    checkpoint_callback = CheckpointCallback(
        save_freq=5000,
        save_path=str(log_dir / "checkpoints"),
        name_prefix="roarm_ppo",
        save_replay_buffer=False,
        save_vecnormalize=False,
    )
    
    # ëª¨ë¸ ë¡œë“œ
    print(f"ğŸ¤– ì²´í¬í¬ì¸íŠ¸ì—ì„œ ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = PPO.load(checkpoint_path, env=env)
    print(f"  Device: {model.device}")
    print()
    
    # í•™ìŠµ ì¬ê°œ
    print("ğŸ“ í•™ìŠµ ì¬ê°œ...\n")
    start_time = time.time()
    
    try:
        model.learn(
            total_timesteps=additional_timesteps,
            callback=checkpoint_callback,
            progress_bar=False,
            reset_num_timesteps=False,  # ì¤‘ìš”! ìŠ¤í… ì¹´ìš´íŠ¸ ìœ ì§€
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸ í•™ìŠµ ì¤‘ë‹¨ (Ctrl+C)")
    
    # ì™„ë£Œ
    elapsed_time = time.time() - start_time
    print("\n" + "=" * 60)
    print("âœ… í•™ìŠµ ì™„ë£Œ!")
    print("=" * 60)
    print(f"  ì´ ì‹œê°„: {elapsed_time / 60:.1f}ë¶„")
    print(f"  ì¢…ë£Œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ìµœì¢… ëª¨ë¸ ì €ì¥
    final_model_dir = log_dir / "final_model"
    final_model_dir.mkdir(parents=True, exist_ok=True)
    model_path = final_model_dir / "roarm_ppo_dense_final.zip"
    model.save(str(model_path))
    print(f"\nğŸ’¾ ìµœì¢… ëª¨ë¸ ì €ì¥: {model_path}")
    
    return model


def find_latest_checkpoint():
    """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°"""
    checkpoint_dir = Path("logs/rl_training_dense/checkpoints")
    
    if not checkpoint_dir.exists():
        return None
    
    checkpoints = sorted(checkpoint_dir.glob("roarm_ppo_*_steps.zip"))
    
    if not checkpoints:
        return None
    
    # ê°€ì¥ í° ìŠ¤í… ìˆ˜ë¥¼ ê°€ì§„ ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
    latest = None
    max_steps = 0
    
    for ckpt in checkpoints:
        try:
            steps = int(ckpt.stem.split("_")[-2])
            if steps > max_steps:
                max_steps = steps
                latest = ckpt
        except:
            continue
    
    return latest


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="RoArm-M3 Dense Reward RL Resume Training")
    parser.add_argument(
        "--checkpoint",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ (ìë™ íƒìƒ‰: ìµœì‹  ì²´í¬í¬ì¸íŠ¸)"
    )
    parser.add_argument(
        "--timesteps",
        type=int,
        default=50000,
        help="ì¶”ê°€ í•™ìŠµ timesteps (ê¸°ë³¸: 50,000)"
    )
    
    args = parser.parse_args()
    
    # ì²´í¬í¬ì¸íŠ¸ ì°¾ê¸°
    if args.checkpoint is None:
        print("ğŸ” ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ê²€ìƒ‰ ì¤‘...\n")
        checkpoint_path = find_latest_checkpoint()
        
        if checkpoint_path is None:
            print("âŒ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\nğŸ’¡ ì²˜ìŒë¶€í„° í•™ìŠµí•˜ë ¤ë©´:")
            print("   PYTHONUNBUFFERED=1 ~/isaacsim/python.sh scripts/train_dense_reward.py")
            simulation_app.close()
            return
        
        print(f"âœ… ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_path.name}\n")
    else:
        checkpoint_path = Path(args.checkpoint)
        
        if not checkpoint_path.exists():
            print(f"âŒ ì²´í¬í¬ì¸íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤: {checkpoint_path}")
            simulation_app.close()
            return
    
    try:
        resume_training(str(checkpoint_path), args.timesteps)
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nğŸ”š Isaac Sim ì¢…ë£Œ ì¤‘...")
        simulation_app.close()


if __name__ == "__main__":
    main()
