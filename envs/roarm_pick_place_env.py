#!/usr/bin/env python3
"""
RoArm-M3 Pick and Place ê°•í™”í•™ìŠµ í™˜ê²½
Isaac Sim 5.0 + omni.isaac.lab ê¸°ë°˜
"""

import numpy as np
import torch
from typing import Dict, Tuple

# Isaac Sim imports
import omni.isaac.core.utils.prims as prim_utils
from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid
from omni.isaac.core.articulations import Articulation
from omni.isaac.core.utils.stage import add_reference_to_stage
from pxr import Gf

# Isaac Lab imports (Isaac Sim 5.0)
try:
    from omni.isaac.lab.envs import DirectRLEnv, DirectRLEnvCfg
    from omni.isaac.lab.utils import configclass
except ImportError:
    # Fallback for older API
    print("âš ï¸ Isaac Lab API not found. Using basic implementation.")
    DirectRLEnv = object
    DirectRLEnvCfg = object
    configclass = lambda x: x


@configclass
class RoArmPickPlaceEnvCfg(DirectRLEnvCfg):
    """RoArm-M3 Pick and Place í™˜ê²½ ì„¤ì •"""
    
    # Environment settings
    num_envs: int = 1
    env_spacing: float = 2.5
    episode_length_s: float = 10.0
    
    # Robot settings
    urdf_path: str = "/home/roarm_m3/roarm_isaac_clean/assets/roarm_m3/urdf/roarm_m3_multiprim.urdf"
    robot_position: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    
    # Object settings
    object_position: Tuple[float, float, float] = (0.3, 0.0, 0.05)
    object_size: Tuple[float, float, float] = (0.04, 0.04, 0.04)
    target_position: Tuple[float, float, float] = (0.0, 0.3, 0.2)
    
    # Reward settings (Shaped-Sparse)
    reach_reward_scale: float = 1.0      # (Deprecated)
    grasp_reward: float = 5.0            # (Deprecated)
    lift_reward_scale: float = 2.0       # (Deprecated)
    move_reward_scale: float = 2.0       # (Deprecated)
    success_reward: float = 100.0        # Success bonus
    success_threshold: float = 0.05      # 5cm
    time_penalty: float = 0.01           # Efficiency penalty
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ“š CURRICULUM LEARNING ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    curriculum_enabled: bool = True      # Curriculum í™œì„±í™”
    curriculum_phase: int = 0            # í˜„ì¬ Phase (0: Easy, 1: Normal)
    
    # Phase 0: Easy Mode (ê°€ê¹Œìš´ ê±°ë¦¬)
    easy_cube_distance: Tuple[float, float] = (0.10, 0.15)  # 10~15cm
    easy_target_distance: Tuple[float, float] = (0.20, 0.25)  # 20~25cm
    
    # Phase 1: Normal Mode (ì›ë˜ ê±°ë¦¬)
    normal_cube_distance: Tuple[float, float] = (0.25, 0.35)  # 25~35cm
    normal_target_distance: Tuple[float, float] = (0.25, 0.35)  # 25~35cm
    
    # ìë™ ìŠ¹ê¸‰ ì¡°ê±´
    success_rate_window: int = 200       # ìµœê·¼ 200 ì—í”¼ì†Œë“œ
    success_rate_threshold: float = 0.60  # ì„±ê³µë¥  60% ì´ìƒ


class RoArmPickPlaceEnv:
    """
    RoArm-M3 Pick and Place ê°•í™”í•™ìŠµ í™˜ê²½ (Dense Reward)
    
    Task: íë¸Œë¥¼ ì§‘ì–´ì„œ íƒ€ê²Ÿ ìœ„ì¹˜ë¡œ ì˜®ê¸°ê¸°
    
    Observation Space (25 dim):
        - Joint positions (6 dim): joint_1 ~ joint_6
        - Gripper state (2 dim): left_finger, right_finger positions
        - End-effector position (3 dim): x, y, z
        - Cube position (3 dim): x, y, z
        - Target position (3 dim): x, y, z
        - EE â†’ Cube vector (3 dim): dx, dy, dz
        - Cube â†’ Target vector (3 dim): dx, dy, dz
        - Gripper width (1 dim): distance between fingers
        - Is grasped (1 dim): 1.0 if grasping, 0.0 otherwise
    
    Action Space (8 dim):
        - Joint position deltas (6 dim): joint_1 ~ joint_6
        - Gripper position deltas (2 dim): left_finger, right_finger
    """
    
    def __init__(self, cfg: RoArmPickPlaceEnvCfg = None):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.cfg = cfg if cfg else RoArmPickPlaceEnvCfg()
        
        # Isaac Sim World ì´ˆê¸°í™”
        self.world = World(stage_units_in_meters=1.0)
        self.world.scene.add_default_ground_plane()
        
        print("=" * 60)
        print("ğŸ¤– RoArm-M3 Pick and Place Environment")
        print("=" * 60)
        
        # ë¡œë´‡ ë¡œë“œ
        self._load_robot()
        
        # ë¬¼ì²´ ìƒì„±
        self._create_objects()
        
        # í™˜ê²½ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.current_step = 0
        self.max_steps = int(self.cfg.episode_length_s * 60)  # 60 FPS ê°€ì •
        
        # Observation/Action space ì •ì˜ (Dense Reward: ë” ë§ì€ ì •ë³´)
        self.observation_space_dim = 25  # joint(8) + ee(3) + cube(3) + target(3) + ee2cube(3) + cube2target(3) + gripper_width(1) + is_grasped(1)
        self.action_space_dim = 8
        
        # ì´ì „ ìƒíƒœ ì €ì¥ (ë³´ìƒ ê³„ì‚°ìš©)
        self.prev_ee_to_cube_dist = None
        self.prev_cube_to_target_dist = None
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ¯ SHAPED-SPARSE: 1íšŒì„± ì´ë²¤íŠ¸ í”Œë˜ê·¸
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.first_reach = False      # EEê°€ íë¸Œì— ì²˜ìŒ ê·¼ì ‘ (5cm)
        self.valid_grip = False       # ìœ íš¨í•œ ê·¸ë¦½ ë‹¬ì„± (3í”„ë ˆì„)
        self.lifted = False           # íë¸Œë¥¼ ë“¤ì–´ì˜¬ë¦¼ (5cm)
        self.goal_near = False        # íë¸Œê°€ ëª©í‘œì— ê·¼ì ‘ (8cm)
        
        # m í”„ë ˆì„ íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì¹´ìš´í„°
        self.grip_frames = 0          # ì—°ì† ê·¸ë¦½ í”„ë ˆì„
        self.success_frames = 0       # ì—°ì† ì„±ê³µ í”„ë ˆì„
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“š CURRICULUM: ì„±ê³µë¥  ì¶”ì 
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.episode_successes = []   # ìµœê·¼ ì—í”¼ì†Œë“œ ì„±ê³µ ì—¬ë¶€
        
        print(f"\nğŸ“Š í™˜ê²½ ì •ë³´:")
        print(f"  - Observation dim: {self.observation_space_dim}")
        print(f"  - Action dim: {self.action_space_dim}")
        print(f"  - Max steps: {self.max_steps}")
        print(f"  - Success threshold: {self.cfg.success_threshold}m")
        print(f"  - Reward type: Shaped-Sparse (ê²Œì´íŒ… + 1íšŒì„± ì´ë²¤íŠ¸)")
        print(f"  - Curriculum: Phase {self.cfg.curriculum_phase} ({'Easy' if self.cfg.curriculum_phase == 0 else 'Normal'})")
    
    def _load_robot(self):
        """ë¡œë´‡ URDF ë¡œë“œ"""
        print(f"\nğŸ”§ ë¡œë´‡ ë¡œë”©: {self.cfg.urdf_path}")
        
        # URDF ì„í¬íŠ¸ (Isaac Sim 5.0 ë°©ì‹)
        import omni.kit.commands
        from isaacsim.asset.importer.urdf import _urdf  # âœ… Isaac Sim 5.0 ëª¨ë“ˆ!
        
        import_config = _urdf.ImportConfig()
        import_config.merge_fixed_joints = False
        import_config.fix_base = True
        import_config.import_inertia_tensor = True
        import_config.self_collision = False
        import_config.distance_scale = 1.0
        
        # URDF ì„í¬íŠ¸ (Isaac Sim 5.0 ê³µì‹ ë°©ì‹)
        # âœ… URDFParseAndImportFileëŠ” stageì— ì§ì ‘ importí•˜ë©° prim_pathë¥¼ ë°˜í™˜
        print("  â³ URDF íŒŒì‹± ì¤‘...")
        
        success, prim_path = omni.kit.commands.execute(
            "URDFParseAndImportFile",
            urdf_path=self.cfg.urdf_path,
            import_config=import_config,
            get_articulation_root=True,  # âœ… ê³µì‹ ì˜ˆì œ íŒ¨í„´!
        )
        
        if not success:
            raise RuntimeError(f"âŒ URDF ì„í¬íŠ¸ ì‹¤íŒ¨: {self.cfg.urdf_path}")
        
        print(f"  âœ… ë¡œë´‡ ì„í¬íŠ¸ ì„±ê³µ!")
        print(f"  ï¿½ Prim path: {prim_path}")
        
        # Articulation ìƒì„± (ë°˜í™˜ëœ prim_path ì‚¬ìš©)
        self.robot = self.world.scene.add(
            Articulation(prim_path=prim_path, name="roarm_m3")
        )
        
        # World resetìœ¼ë¡œ articulation ì´ˆê¸°í™”
        print(f"  â³ Articulation ì´ˆê¸°í™” ì¤‘...")
        self.world.reset()
        
        # Joint ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        self.joint_names = self.robot.dof_names
        print(f"  âœ… Joints ({len(self.joint_names) if self.joint_names else 0}): {self.joint_names[:3] if self.joint_names and len(self.joint_names) > 3 else self.joint_names}...")
        
        # âœ… Joint drive ì„¤ì • (USD API ì‚¬ìš©)
        print(f"  â³ Joint drive ì„¤ì • ì¤‘...")
        from pxr import UsdPhysics, PhysxSchema
        stage = self.world.stage
        
        # ëª¨ë“  jointì— ëŒ€í•´ drive ì„¤ì •
        for i, joint_name in enumerate(self.joint_names):
            joint_prim = stage.GetPrimAtPath(f"{prim_path}/{joint_name}")
            if joint_prim and joint_prim.IsValid():
                # Drive API ì ìš© (ì™„í™”ëœ ê°’ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ ì›€ì§ì„)
                drive_api = UsdPhysics.DriveAPI.Apply(joint_prim, "angular")
                if i < 6:  # íŒ” joint (ì™„í™”: 10000 â†’ 5000)
                    drive_api.GetStiffnessAttr().Set(5000.0)  # ì™„í™”
                    drive_api.GetDampingAttr().Set(500.0)     # ì™„í™”
                    drive_api.GetMaxForceAttr().Set(500.0)    # ì™„í™”
                else:  # ê·¸ë¦¬í¼
                    drive_api.GetStiffnessAttr().Set(1000.0)
                    drive_api.GetDampingAttr().Set(100.0)
                    drive_api.GetMaxForceAttr().Set(100.0)
        
        print(f"  âœ… Joint drive ì„¤ì • ì™„ë£Œ! (ì™„í™”ëœ ê°’)")
    
    def _create_objects(self):
        """ë¬¼ì²´ ë° íƒ€ê²Ÿ ìƒì„±"""
        print(f"\nğŸ“¦ ë¬¼ì²´ ìƒì„± ì¤‘...")
        
        # íë¸Œ ìƒì„± (Pick ëŒ€ìƒ)
        self.cube = self.world.scene.add(
            DynamicCuboid(
                prim_path="/World/cube",
                name="cube",
                position=np.array(self.cfg.object_position),
                size=self.cfg.object_size[0],  # ì •ìœ¡ë©´ì²´
                color=np.array([0.8, 0.2, 0.2]),  # ë¹¨ê°„ìƒ‰
            )
        )
        print(f"  âœ… íë¸Œ ìƒì„±: {self.cfg.object_position}")
        
        # íƒ€ê²Ÿ ë§ˆì»¤ ìƒì„± (ì‹œê°ì  ëª©í‘œ)
        self.target = self.world.scene.add(
            DynamicCuboid(
                prim_path="/World/target",
                name="target",
                position=np.array(self.cfg.target_position),
                size=self.cfg.object_size[0],
                color=np.array([0.2, 0.8, 0.2]),  # ì´ˆë¡ìƒ‰
            )
        )
        # íƒ€ê²Ÿì€ ì •ì  (kinematic)ìœ¼ë¡œ ì„¤ì •
        self.target.set_default_state(position=np.array(self.cfg.target_position))
        
        print(f"  âœ… íƒ€ê²Ÿ ìƒì„±: {self.cfg.target_position}")
    
    def reset(self) -> np.ndarray:
        """í™˜ê²½ ë¦¬ì…‹"""
        print(f"\nğŸ”„ í™˜ê²½ ë¦¬ì…‹ (Step {self.current_step})")
        
        # World ë¦¬ì…‹
        self.world.reset()
        
        # ë¡œë´‡ ì´ˆê¸° ìì„¸ (Home position) - ì•½ê°„ ìœ„ë¡œ ì˜¬ë¦° ìì„¸
        # ì¤‘ë ¥ì— ì˜í•´ ë–¨ì–´ì§€ì§€ ì•Šë„ë¡ ì•ˆì •ì ì¸ ìì„¸
        home_positions = np.array([0.0, -0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0])  # 8 DOF
        self.robot.set_joint_positions(home_positions)
        self.robot.set_joint_velocities(np.zeros(8))
        
        # Physics ìŠ¤í… ì‹¤í–‰í•˜ì—¬ ì•ˆì •í™”
        for _ in range(10):
            self.world.step(render=False)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“š CURRICULUM: Phaseì— ë”°ë¥¸ íë¸Œ/íƒ€ê²Ÿ ìœ„ì¹˜ ì„¤ì •
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.cfg.curriculum_enabled:
            if self.cfg.curriculum_phase == 0:  # Easy Mode
                # íë¸Œë¥¼ ë¡œë´‡ ê°€ê¹Œì´ (10~15cm)
                distance = np.random.uniform(*self.cfg.easy_cube_distance)
                angle = np.random.uniform(0, 2 * np.pi)
                cube_pos = np.array([
                    distance * np.cos(angle),
                    distance * np.sin(angle),
                    0.05  # í…Œì´ë¸” ë†’ì´
                ])
                
                # íƒ€ê²Ÿë„ ê°€ê¹Œì´ (20~25cm)
                target_distance = np.random.uniform(*self.cfg.easy_target_distance)
                target_angle = np.random.uniform(0, 2 * np.pi)
                target_pos = np.array([
                    target_distance * np.cos(target_angle),
                    target_distance * np.sin(target_angle),
                    0.2  # íƒ€ê²Ÿ ë†’ì´
                ])
                
                print(f"  ğŸ“š Phase 0 (Easy): cube={distance:.2f}m, target={target_distance:.2f}m")
            else:  # Normal Mode (Phase 1+)
                # ì›ë˜ ê±°ë¦¬ (25~35cm)
                distance = np.random.uniform(*self.cfg.normal_cube_distance)
                angle = np.random.uniform(0, 2 * np.pi)
                cube_pos = np.array([
                    distance * np.cos(angle),
                    distance * np.sin(angle),
                    0.05
                ])
                
                target_distance = np.random.uniform(*self.cfg.normal_target_distance)
                target_angle = np.random.uniform(0, 2 * np.pi)
                target_pos = np.array([
                    target_distance * np.cos(target_angle),
                    target_distance * np.sin(target_angle),
                    0.2
                ])
                
                print(f"  ğŸ“š Phase {self.cfg.curriculum_phase} (Normal): cube={distance:.2f}m, target={target_distance:.2f}m")
        else:
            # Curriculum ë¹„í™œì„±í™”: ê¸°ë³¸ ìœ„ì¹˜ + ëœë¤
            cube_pos = np.array(self.cfg.object_position)
            cube_pos[:2] += np.random.uniform(-0.05, 0.05, size=2)
            target_pos = np.array(self.cfg.target_position)
        
        self.cube.set_world_pose(position=cube_pos)
        self.cube.set_linear_velocity(np.zeros(3))
        self.cube.set_angular_velocity(np.zeros(3))
        
        # íƒ€ê²Ÿ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (Curriculum ì ìš©)
        if self.cfg.curriculum_enabled:
            self.target.set_world_pose(position=target_pos)
        
        # ìŠ¤í… ì¹´ìš´í„° ì´ˆê¸°í™”
        self.current_step = 0
        
        # ì´ì „ ê±°ë¦¬ ì´ˆê¸°í™” (ë³´ìƒ ê³„ì‚°ìš©)
        self.prev_ee_to_cube_dist = None
        self.prev_cube_to_target_dist = None
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ¯ SHAPED-SPARSE: 1íšŒì„± í”Œë˜ê·¸ ë¦¬ì…‹
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.first_reach = False
        self.valid_grip = False
        self.lifted = False
        self.goal_near = False
        self.grip_frames = 0
        self.success_frames = 0
        
        # ì´ˆê¸° observation ë°˜í™˜
        return self._get_observation()
    
    def _get_observation(self) -> np.ndarray:
        """í˜„ì¬ ìƒíƒœ ê´€ì¸¡ (Dense Reward: ë” ë§ì€ ì •ë³´)"""
        # Joint positions (6 revolute + 2 prismatic)
        joint_positions = self.robot.get_joint_positions()[:8]
        
        # End-effector position
        ee_pos = self._get_ee_position()
        
        # Cube position
        cube_pos, _ = self.cube.get_world_pose()
        
        # Target position
        target_pos = np.array(self.cfg.target_position)
        
        # EE â†’ Cube vector
        ee_to_cube = cube_pos - ee_pos
        
        # Cube â†’ Target vector
        cube_to_target = target_pos - cube_pos
        
        # Gripper width (distance between fingers)
        gripper_width = joint_positions[6] + joint_positions[7]
        
        # Is grasped (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±: EEê°€ íë¸Œ ê°€ê¹Œì´ + ê·¸ë¦¬í¼ ë‹«í˜)
        ee_to_cube_dist = np.linalg.norm(ee_to_cube)
        is_grasped = 1.0 if (ee_to_cube_dist < 0.08 and gripper_width < 0.02) else 0.0
        
        # Observation ë²¡í„° ìƒì„± (25 dim)
        obs = np.concatenate([
            joint_positions[:6],      # Joint positions (6)
            joint_positions[6:8],     # Gripper state (2)
            ee_pos,                   # EE position (3)
            cube_pos,                 # Cube position (3)
            target_pos,               # Target position (3)
            ee_to_cube,               # EE â†’ Cube vector (3)
            cube_to_target,           # Cube â†’ Target vector (3)
            [gripper_width],          # Gripper width (1)
            [is_grasped],             # Is grasped (1)
        ])
        
        return obs
    
    def _get_ee_position(self) -> np.ndarray:
        """End-effector ìœ„ì¹˜ ê³„ì‚° (ê°„ë‹¨í•œ forward kinematics)"""
        # ì‹¤ì œë¡œëŠ” robot.get_link_world_pose()ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ joint ê°’ ê¸°ë°˜ìœ¼ë¡œ ê·¼ì‚¬
        joint_positions = self.robot.get_joint_positions()
        
        # ë² ì´ìŠ¤ì—ì„œ ì‹œì‘
        z_base = 0.06  # base_link height
        z_link1 = 0.08  # link_1 height
        
        # Joint 2-4ëŠ” ìˆ˜í‰ ì•” (link_2, link_3, link_4)
        link2_length = 0.16
        link3_length = 0.15
        
        # ê°„ë‹¨íˆ Xì¶• ë°©í–¥ìœ¼ë¡œ íˆ¬ì˜ (ì‹¤ì œëŠ” ë” ë³µì¡)
        x_reach = link2_length * np.cos(joint_positions[1]) + \
                  link3_length * np.cos(joint_positions[1] + joint_positions[2])
        
        z_reach = z_base + z_link1 + \
                  link2_length * np.sin(joint_positions[1]) + \
                  link3_length * np.sin(joint_positions[1] + joint_positions[2])
        
        # Joint 1ì€ Zì¶• íšŒì „
        y_offset = x_reach * np.sin(joint_positions[0])
        x_offset = x_reach * np.cos(joint_positions[0])
        
        return np.array([x_offset, y_offset, z_reach])
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        # Action: joint position deltas (8 dim)
        action = np.clip(action, -1.0, 1.0)  # [-1, 1] ë²”ìœ„ë¡œ ì œí•œ
        
        # í˜„ì¬ joint positions ê°€ì ¸ì˜¤ê¸°
        current_positions = self.robot.get_joint_positions()
        
        # Position delta ìŠ¤ì¼€ì¼ë§ (ì‘ì€ ì›€ì§ì„)
        max_deltas = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.005, 0.005])
        position_deltas = action * max_deltas
        
        # ìƒˆë¡œìš´ ëª©í‘œ ìœ„ì¹˜ ê³„ì‚°
        target_positions = current_positions + position_deltas
        
        # Joint limits ì ìš©
        target_positions = np.clip(
            target_positions,
            [-3.14, -1.57, -1.57, -3.14, -3.14, -3.14, 0.0, 0.0],  # lower limits
            [3.14, 1.57, 1.57, 3.14, 3.14, 3.14, 0.04, 0.04]  # upper limits
        )
        
        # ë¡œë´‡ì— position ëª…ë ¹ ì „ì†¡ (ì§ì ‘ ì„¤ì •)
        self.robot.set_joint_positions(target_positions)
        
        # Physics ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í…
        self.world.step(render=True)
        
        # í˜„ì¬ ìƒíƒœ ê´€ì¸¡
        obs = self._get_observation()
        
        # Reward ê³„ì‚°
        reward = self._calculate_reward(obs)
        
        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        done = self._check_done(obs)
        
        # ìŠ¤í… ì¹´ìš´í„° ì¦ê°€
        self.current_step += 1
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“Š ë¡œê¹…: ì´ë²¤íŠ¸ ë° ì§„í–‰ ìƒí™© ì¶”ì 
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        cube_to_target_dist = np.linalg.norm(obs[20:23])
        
        # ì„±ê³µë¥  ê³„ì‚° (ìµœê·¼ ì—í”¼ì†Œë“œ ê¸°ì¤€)
        success_rate = np.mean(self.episode_successes) if len(self.episode_successes) > 0 else 0.0
        
        info = {
            "step": self.current_step,
            "cube_position": obs[11:14].tolist(),
            "distance_to_target": float(cube_to_target_dist),
            # ë§ˆì¼ìŠ¤í†¤ ì´ë²¤íŠ¸ ì¶”ì 
            "events": {
                "first_reach": self.first_reach,
                "valid_grip": self.valid_grip,
                "lifted": self.lifted,
                "goal_near": self.goal_near,
                "success": cube_to_target_dist < self.cfg.success_threshold,
            },
            # ì¢…ë£Œ ì‚¬ìœ 
            "done_reason": "ongoing" if not done else (
                "success" if cube_to_target_dist < self.cfg.success_threshold else
                "timeout" if self.current_step >= self.max_steps else
                "safety"
            ),
            # Curriculum ì •ë³´
            "curriculum_phase": self.cfg.curriculum_phase,
            "success_rate": float(success_rate),
        }
        
        return obs, reward, done, info
    
    def _calculate_reward(self, obs: np.ndarray) -> float:
        """
        ğŸ¯ SHAPED-SPARSE REWARD: ê²Œì´íŒ… + 1íšŒì„± ë§ˆì¼ìŠ¤í†¤ ë³´ìƒ
        
        ì „ë¬¸ê°€ ìµœì¢… ê¶Œì¥ ì‚¬í•­ ì ìš©:
        1. ê²Œì´íŒ… ì‹œìŠ¤í…œ: grasp_valid ì²´í¬ë¡œ ë³´ìƒ í­ë°œ ì°¨ë‹¨
        2. 1íšŒì„± ì´ë²¤íŠ¸: ê° ë§ˆì¼ìŠ¤í†¤ì€ í•œ ë²ˆë§Œ ë³´ìƒ
        3. m í”„ë ˆì„ íˆìŠ¤í…Œë¦¬ì‹œìŠ¤: ë…¸ì´ì¦ˆ í•„í„°ë§
        
        ë³´ìƒ êµ¬ì¡°:
        - ê·¼ì ‘ (+5): EEê°€ íë¸Œ 5cm ì´ë‚´ ì§„ì… (1íšŒ)
        - ê·¸ë¦½ (+10): ìœ íš¨ ê·¸ë¦½ 3í”„ë ˆì„ ìœ ì§€ (1íšŒ, ê²Œì´íŒ…)
        - ë¦¬í”„íŠ¸ (+15): íë¸Œ 5cm ì´ìƒ ë“¤ì–´ì˜¬ë¦¼ (1íšŒ, ê²Œì´íŒ…)
        - ëª©í‘œ ê·¼ì ‘ (+20): íë¸Œê°€ ëª©í‘œ 8cm ì´ë‚´ (1íšŒ, ê²Œì´íŒ…)
        - Success (+100): ëª©í‘œ 5cm ì´ë‚´ 5í”„ë ˆì„ ìœ ì§€ (1íšŒ)
        - Time penalty (-0.01): íš¨ìœ¨ì„± ìœ ë„
        """
        # ê´€ì°°ì—ì„œ í•„ìš”í•œ ê°’ ì¶”ì¶œ
        ee_pos = obs[8:11]
        cube_pos = obs[11:14]
        target_pos = obs[14:17]
        ee_to_cube_vec = obs[17:20]
        cube_to_target_vec = obs[20:23]
        gripper_width = obs[23]
        is_grasped = obs[24]
        
        # ê±°ë¦¬ ê³„ì‚°
        ee_to_cube_dist = np.linalg.norm(ee_to_cube_vec)
        cube_to_target_dist = np.linalg.norm(cube_to_target_vec)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”’ GATING: grasp_valid ì²´í¬
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ìœ íš¨ ê·¸ë¦½ ì¡°ê±´: EE ê·¼ì ‘ + ê·¸ë¦¬í¼ ë‹«í˜ + íë¸Œ ë†’ì´
        grasp_valid = (
            ee_to_cube_dist < 0.08 and      # EEê°€ 8cm ì´ë‚´
            gripper_width < 0.02 and        # ê·¸ë¦¬í¼ê°€ ë‹«í˜
            cube_pos[2] > 0.03              # íë¸Œê°€ ë°”ë‹¥ ìœ„
        )
        
        # íˆìŠ¤í…Œë¦¬ì‹œìŠ¤: ì—°ì† í”„ë ˆì„ ì¹´ìš´íŠ¸
        if grasp_valid:
            self.grip_frames += 1
        else:
            self.grip_frames = 0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ SHAPED-SPARSE REWARDS (1íšŒì„± ì´ë²¤íŠ¸)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        reward = 0.0
        
        # 1ï¸âƒ£ ê·¼ì ‘ ë³´ìƒ (+5): EEê°€ íë¸Œì— ì²˜ìŒ ê·¼ì ‘
        if not self.first_reach and ee_to_cube_dist < 0.05:
            reward += 5.0
            self.first_reach = True
            print(f"  ğŸ¯ Milestone: REACH! (+5.0)")
        
        # 2ï¸âƒ£ ê·¸ë¦½ ë³´ìƒ (+10): ìœ íš¨ ê·¸ë¦½ 3í”„ë ˆì„ ìœ ì§€ [ê²Œì´íŒ…]
        if not self.valid_grip and grasp_valid and self.grip_frames >= 3:
            reward += 10.0
            self.valid_grip = True
            print(f"  âœŠ Milestone: GRIP! (+10.0)")
        
        # 3ï¸âƒ£ ë¦¬í”„íŠ¸ ë³´ìƒ (+15): íë¸Œ 5cm ì´ìƒ ë“¤ì–´ì˜¬ë¦¼ [ê²Œì´íŒ…]
        if not self.lifted and grasp_valid and cube_pos[2] > 0.05:
            reward += 15.0
            self.lifted = True
            print(f"  â¬†ï¸ Milestone: LIFT! (+15.0)")
        
        # 4ï¸âƒ£ ëª©í‘œ ê·¼ì ‘ ë³´ìƒ (+20): íë¸Œê°€ ëª©í‘œ 8cm ì´ë‚´ [ê²Œì´íŒ…]
        if not self.goal_near and grasp_valid and cube_to_target_dist < 0.08:
            reward += 20.0
            self.goal_near = True
            print(f"  ğŸ¯ Milestone: GOAL NEAR! (+20.0)")
        
        # 5ï¸âƒ£ Success ë³´ìƒ (+100): ëª©í‘œ 5cm ì´ë‚´ 5í”„ë ˆì„ ìœ ì§€
        if cube_to_target_dist < self.cfg.success_threshold:
            self.success_frames += 1
            if self.success_frames >= 5:
                reward += self.cfg.success_reward  # +100
                print(f"  ğŸ† Milestone: SUCCESS! (+100.0)")
        else:
            self.success_frames = 0
        
        # Time Penalty: íš¨ìœ¨ì„± ìœ ë„ (ë§¤ ìŠ¤í… -0.01)
        time_penalty = -self.cfg.time_penalty
        reward += time_penalty
        
        return reward
    
    def _check_done(self, obs: np.ndarray) -> bool:
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ + ì„±ê³µë¥  ì¶”ì """
        # íë¸Œ ìœ„ì¹˜
        cube_pos = obs[11:14]
        
        # Cube â†’ Target ê±°ë¦¬
        cube_to_target_vec = obs[20:23]
        cube_to_target_dist = np.linalg.norm(cube_to_target_vec)
        
        # íƒ€ê²Ÿ ë„ë‹¬
        if cube_to_target_dist < self.cfg.success_threshold:
            print(f"  âœ… SUCCESS! Distance: {cube_to_target_dist:.3f}m")
            self._record_success(True)
            return True
        
        # ìµœëŒ€ ìŠ¤í… ë„ë‹¬
        if self.current_step >= self.max_steps:
            print(f"  â±ï¸ Timeout (Max steps: {self.max_steps})")
            self._record_success(False)
            return True
        
        # ë¬¼ì²´ê°€ í…Œì´ë¸” ë°–ìœ¼ë¡œ ë–¨ì–´ì§ (Z < 0)
        if cube_pos[2] < -0.1:
            print(f"  âŒ Cube fell off table (Z: {cube_pos[2]:.3f}m)")
            self._record_success(False)
            return True
        
        return False
    
    def _record_success(self, success: bool):
        """
        ì„±ê³µë¥  ì¶”ì  ë° ìë™ ìŠ¹ê¸‰ ì²´í¬
        """
        # ì„±ê³µ ì—¬ë¶€ ê¸°ë¡
        self.episode_successes.append(1.0 if success else 0.0)
        
        # ìœˆë„ìš° í¬ê¸° ì œí•œ
        if len(self.episode_successes) > self.cfg.success_rate_window:
            self.episode_successes.pop(0)
        
        # ì„±ê³µë¥  ê³„ì‚° (ìµœì†Œ 50 ì—í”¼ì†Œë“œ ì´ìƒ)
        if len(self.episode_successes) >= 50:
            success_rate = np.mean(self.episode_successes)
            
            # Curriculum ìŠ¹ê¸‰ ì²´í¬ (Phase 0 â†’ Phase 1)
            if (self.cfg.curriculum_enabled and 
                self.cfg.curriculum_phase == 0 and
                success_rate >= self.cfg.success_rate_threshold):
                
                print("\n" + "=" * 60)
                print(f"ğŸ“ CURRICULUM UPGRADE! Phase 0 â†’ Phase 1")
                print(f"   Success Rate: {success_rate:.1%} (â‰¥{self.cfg.success_rate_threshold:.0%})")
                print(f"   Window: {len(self.episode_successes)} episodes")
                print("=" * 60 + "\n")
                
                # Phase 1ìœ¼ë¡œ ìŠ¹ê¸‰
                self.cfg.curriculum_phase = 1
                self.episode_successes.clear()  # ì„±ê³µë¥  ë¦¬ì…‹
    
    def render(self):
        """ë Œë”ë§ (Isaac Simì—ì„œ ìë™ ì²˜ë¦¬)"""
        pass
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        print("\nğŸ›‘ í™˜ê²½ ì¢…ë£Œ")
        self.world.stop()


# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    print("ğŸš€ RoArm-M3 Pick and Place í™˜ê²½ í…ŒìŠ¤íŠ¸\n")
    
    # í™˜ê²½ ìƒì„±
    cfg = RoArmPickPlaceEnvCfg()
    cfg.episode_length_s = 5.0  # ì§§ê²Œ í…ŒìŠ¤íŠ¸
    
    env = RoArmPickPlaceEnv(cfg)
    
    # ëª‡ ì—í”¼ì†Œë“œ í…ŒìŠ¤íŠ¸
    for episode in range(2):
        print(f"\n{'='*60}")
        print(f"Episode {episode + 1}")
        print(f"{'='*60}")
        
        obs = env.reset()
        print(f"Initial observation shape: {obs.shape}")
        
        done = False
        total_reward = 0
        step = 0
        
        while not done and step < 100:
            # ëœë¤ ì•¡ì…˜
            action = np.random.uniform(-0.5, 0.5, size=8)
            
            obs, reward, done, info = env.step(action)
            total_reward += reward
            step += 1
            
            if step % 20 == 0:
                print(f"  Step {step}: Reward={reward:.2f}, Distance={info['distance_to_target']:.3f}m")
        
        print(f"\n  ğŸ“Š Episode {episode + 1} ê²°ê³¼:")
        print(f"    - Total steps: {step}")
        print(f"    - Total reward: {total_reward:.2f}")
        print(f"    - Final distance: {info['distance_to_target']:.3f}m")
    
    env.close()
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
