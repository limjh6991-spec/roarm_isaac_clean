#!/usr/bin/env python3
"""
RoArm-M3 Pick and Place ê°•í™”í•™ìŠµ í™˜ê²½
Isaac Sim 5.0 + omni.isaac.lab ê¸°ë°˜
"""

import numpy as np
import torch
from typing import Dict, Tuple
import sys
import os

# ğŸ”¥ v3.5: í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python pathì— ì¶”ê°€ (ëª¨ë“ˆ importìš©)
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

# ğŸ” USD/pxr ë¡œë”© ê²½ë¡œ ì§„ë‹¨
print("=" * 80)
print("ğŸ” USD/pxr ëª¨ë“ˆ ë¡œë”© ì§„ë‹¨")
print("=" * 80)

try:
    from pxr import Usd, UsdGeom, Gf, Sdf, UsdPhysics, UsdShade
    pxr_path = sys.modules['pxr'].__file__
    print(f"âœ… pxr ë¡œë”© ì„±ê³µ")
    print(f"   ê²½ë¡œ: {pxr_path}")
    print(f"   ë²„ì „: {Usd.GetVersion()}")
    
    # USD ìŠ¤í‚¤ë§ˆ ë¬´ê²°ì„± ì ê²€
    print("\nğŸ“‹ USD ìŠ¤í‚¤ë§ˆ ë¬´ê²°ì„± ì ê²€:")
    schema_checks = {
        "UsdGeom.Xform": hasattr(UsdGeom, 'Xform'),
        "UsdGeom.Mesh": hasattr(UsdGeom, 'Mesh'),
        "UsdGeom.Cube": hasattr(UsdGeom, 'Cube'),
        "UsdGeom.Sphere": hasattr(UsdGeom, 'Sphere'),
        "Gf.Vec3f": hasattr(Gf, 'Vec3f'),
        "Gf.Matrix4d": hasattr(Gf, 'Matrix4d'),
        "Sdf.Path": hasattr(Sdf, 'Path'),
    }
    
    all_ok = True
    for schema_name, exists in schema_checks.items():
        status = "âœ…" if exists else "âŒ"
        print(f"   {status} {schema_name}")
        if not exists:
            all_ok = False
    
    if all_ok:
        print("âœ… ëª¨ë“  USD ìŠ¤í‚¤ë§ˆ ì •ìƒ")
    else:
        print("âš ï¸ ì¼ë¶€ USD ìŠ¤í‚¤ë§ˆ ëˆ„ë½ë¨")
        
except ImportError as e:
    print(f"âŒ pxr ë¡œë”© ì‹¤íŒ¨: {e}")
    print(f"   Python ê²½ë¡œ: {sys.executable}")
    print(f"   sys.path: {sys.path[:3]}...")
    raise

print("=" * 80)
print()

# Isaac Sim imports
import omni.isaac.core.utils.prims as prim_utils
from omni.isaac.core import World
from omni.isaac.core.objects import DynamicCuboid
from omni.isaac.core.articulations import Articulation
from omni.isaac.core.utils.stage import add_reference_to_stage

# Isaac Lab imports (Isaac Sim 5.0)
try:
    from omni.isaac.lab.envs import DirectRLEnv, DirectRLEnvCfg
    from omni.isaac.lab.utils import configclass
except ImportError:
    # Fallback for older API
    print("âš ï¸ Isaac Lab API not found. Using basic implementation.")
    DirectRLEnv = object
    DirectRLEnvCfg = object
    configclass = lambda x: x

# ğŸ”¥ v3.5: ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸ ì„í¬íŠ¸
from controllers.gripper import Gripper
from robot_utils.ee_pose import find_ee_prim, get_ee_position
from rewards.pick_place import GateConfig, grasp_gate, compute_hybrid_reward


@configclass
class RoArmPickPlaceEnvCfg(DirectRLEnvCfg):
    """RoArm-M3 Pick and Place í™˜ê²½ ì„¤ì •"""
    
    # Environment settings
    num_envs: int = 1
    env_spacing: float = 2.5
    episode_length_s: float = 10.0
    
    # Robot settings
    urdf_path: str = "/home/roarm_m3/roarm_isaac_clean/assets/roarm_m3/urdf/roarm_m3_multiprim.urdf"
    robot_position: Tuple[float, float, float] = (0.0, 0.0, 0.0)
    
    # Object settings
    object_position: Tuple[float, float, float] = (0.3, 0.0, 0.05)
    object_size: Tuple[float, float, float] = (0.04, 0.04, 0.04)
    target_position: Tuple[float, float, float] = (0.0, 0.3, 0.2)
    
    # Reward settings (Shaped-Sparse)
    reach_reward_scale: float = 1.0      # (Deprecated)
    grasp_reward: float = 5.0            # (Deprecated)
    lift_reward_scale: float = 2.0       # (Deprecated)
    move_reward_scale: float = 2.0       # (Deprecated)
    success_reward: float = 100.0        # Success bonus
    success_threshold: float = 0.02      # 2cm (5cm â†’ 2cm, ë” ì •ë°€í•œ ì œì–´!)
    success_hold_frames: int = 10        # 10í”„ë ˆì„ ì—°ì† ìœ ì§€ (5 â†’ 10)
    time_penalty: float = 0.01           # Efficiency penalty
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ“š CURRICULUM LEARNING ì„¤ì •
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    curriculum_enabled: bool = True      # Curriculum í™œì„±í™”
    curriculum_phase: int = 0            # í˜„ì¬ Phase (0: Easy, 1: Normal)
    
    # Phase 0: Easy Mode (ì¤‘ê°„ ê±°ë¦¬, 25~35cm)
    easy_cube_distance: Tuple[float, float] = (0.25, 0.35)  # 25~35cm (ë„ˆë¬´ ê°€ê¹Œìš°ë©´ í•™ìŠµ ì•ˆ ë¨)
    easy_target_distance: Tuple[float, float] = (0.30, 0.40)  # 30~40cm
    
    # Phase 1: Normal Mode (ë¨¼ ê±°ë¦¬, 35~50cm â†’ ë” ì–´ë µê²Œ!)
    normal_cube_distance: Tuple[float, float] = (0.35, 0.50)  # 35~50cm (25-35cm â†’ 35-50cm)
    normal_target_distance: Tuple[float, float] = (0.35, 0.50)  # 35~50cm (25-35cm â†’ 35-50cm)
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # ğŸ“š ìë™ ìŠ¹ê¸‰ ì¡°ê±´ (ì™„í™”ë¨!)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    success_rate_window: int = 100       # ìµœê·¼ 100 ì—í”¼ì†Œë“œ (200 â†’ 100)
    success_rate_threshold: float = 0.30  # ì„±ê³µë¥  30% ì´ìƒ (60% â†’ 30%)
    reach_milestone_threshold: int = 5    # REACH 5íšŒ ë‹¬ì„± (10íšŒ â†’ 5íšŒ)


class RoArmPickPlaceEnv:
    """
    RoArm-M3 Pick and Place ê°•í™”í•™ìŠµ í™˜ê²½ (Dense Reward)
    
    Task: íë¸Œë¥¼ ì§‘ì–´ì„œ íƒ€ê²Ÿ ìœ„ì¹˜ë¡œ ì˜®ê¸°ê¸°
    
    Observation Space (25 dim):
        - Joint positions (6 dim): joint_1 ~ joint_6
        - Gripper state (2 dim): left_finger, right_finger positions
        - End-effector position (3 dim): x, y, z
        - Cube position (3 dim): x, y, z
        - Target position (3 dim): x, y, z
        - EE â†’ Cube vector (3 dim): dx, dy, dz
        - Cube â†’ Target vector (3 dim): dx, dy, dz
        - Gripper width (1 dim): distance between fingers
        - Is grasped (1 dim): 1.0 if grasping, 0.0 otherwise
    
    Action Space (8 dim):
        - Joint position deltas (6 dim): joint_1 ~ joint_6
        - Gripper position deltas (2 dim): left_finger, right_finger
    """
    
    def __init__(self, cfg: RoArmPickPlaceEnvCfg = None):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.cfg = cfg if cfg else RoArmPickPlaceEnvCfg()
        
        # Isaac Sim World ì´ˆê¸°í™”
        self.world = World(stage_units_in_meters=1.0)
        self.world.scene.add_default_ground_plane()
        
        print("=" * 60)
        print("ğŸ¤– RoArm-M3 Pick and Place Environment")
        print("ğŸ”¥ ENV_VERSION = v3.7.3 (expanded joint limits Â±180Â°)")
        print("   - All arm joints: Â±3.14 rad (was Â±1.57 for joints 2,3,4,6)")
        print("   - Gripper width tracking: enabled")
        print("=" * 60)
        
        # ë¡œë´‡ ë¡œë“œ
        self._load_robot()
        
        # ë¬¼ì²´ ìƒì„±
        self._create_objects()
        
        # í™˜ê²½ ë³€ìˆ˜ ì´ˆê¸°í™”
        self.current_step = 0
        self.step_count = 0  # ğŸ”¥ v3.7.1: ë””ë²„ê·¸ ë¡œê¹…ìš© ì „ì—­ ìŠ¤í… ì¹´ìš´í„°
        self.max_steps = int(self.cfg.episode_length_s * 60)  # 60 FPS ê°€ì •
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“Š Observation/Action Space (ê°œì„ : EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Observation: 28 dim (EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ!)
        #   - Joint positions (8)
        #   - Cube pos relative to EE (3) â† í•µì‹¬!
        #   - Target pos relative to EE (3) â† í•µì‹¬!
        #   - Cube to Target vector (3)
        #   - EE velocity (3)
        #   - Cube velocity (3)
        #   - Gripper width (1)
        #   - Is grasped (1)
        #   - Distance to cube (1)
        #   - Distance cube to target (1)
        #   - Previous reward (1)
        self.observation_space_dim = 28
        # ğŸ”¥ v3.7: Action space ì¶•ì†Œ (8 â†’ 7)
        # êµ¬ì¡°: [6 DoF arm joints] + [1 gripper scalar]
        # ê·¸ë¦¬í¼ ìŠ¤ì¹¼ë¼: -1 (ì™„ì „ ë‹«í˜) ~ +1 (ì™„ì „ ì—´ë¦¼)
        self.action_space_dim = 7
        
        # ì´ì „ ìƒíƒœ ì €ì¥ (ì†ë„ ê³„ì‚° & Dense Reward)
        self.prev_ee_pos = None
        self.prev_cube_pos = None
        self.prev_ee_to_cube_dist = None
        self.prev_cube_to_target_dist = None
        self.previous_reward = 0.0
        
        # ğŸ”¥ v3.7.2: ê·¸ë¦¬í¼ í­ ì§ì ‘ ì¶”ì  (physics lag íšŒí”¼)
        self.current_gripper_width = 0.0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ¯ SHAPED-SPARSE: 1íšŒì„± ì´ë²¤íŠ¸ í”Œë˜ê·¸
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.first_reach = False      # EEê°€ íë¸Œì— ì²˜ìŒ ê·¼ì ‘ (5cm)
        self.valid_grip = False       # ìœ íš¨í•œ ê·¸ë¦½ ë‹¬ì„± (3í”„ë ˆì„)
        self.lifted = False           # íë¸Œë¥¼ ë“¤ì–´ì˜¬ë¦¼ (5cm)
        self.goal_near = False        # íë¸Œê°€ ëª©í‘œì— ê·¼ì ‘ (8cm)
        
        # í”„ë ˆì„ íˆìŠ¤í…Œë¦¬ì‹œìŠ¤ ì¹´ìš´í„°
        self.grip_frames = 0          # ì—°ì† ê·¸ë¦½ í”„ë ˆì„
        self.success_frames = 0       # ì—°ì† ì„±ê³µ í”„ë ˆì„
        
        # ë§ˆì¼ìŠ¤í†¤ ì¹´ìš´í„° (ì—í”¼ì†Œë“œ í†µê³„ìš©)
        self.episode_reach_count = 0
        self.episode_grip_count = 0
        self.episode_lift_count = 0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“š CURRICULUM: ì„±ê³µë¥  ì¶”ì 
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.episode_successes = []   # ìµœê·¼ ì—í”¼ì†Œë“œ ì„±ê³µ ì—¬ë¶€
        
        # ğŸ”¥ v3.5: ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” (ë¡œë´‡ ë¡œë“œ í›„ ì„¤ì •ë¨)
        self.gripper = None           # Gripper controller
        self.ee_prim_path = None      # EE prim ê²½ë¡œ
        self.gate_config = GateConfig(cube_size=self.cfg.object_size[0])
        
        print(f"\nğŸ“Š í™˜ê²½ ì •ë³´:")
        print(f"  - Observation dim: {self.observation_space_dim}")
        print(f"  - Action dim: {self.action_space_dim}")
        print(f"  - Episode length: {self.cfg.episode_length_s}s * 60 FPS = {self.max_steps} steps")
        print(f"  - Success threshold: {self.cfg.success_threshold}m ({int(self.cfg.success_threshold*100)}cm)")
        print(f"  ğŸ”¥ v3.5: ëª¨ë“ˆí™” í™œì„±í™” (Gripper/EE/Rewards)")
        print(f"  - Success hold frames: {self.cfg.success_hold_frames} (ì—°ì† ìœ ì§€)")
        print(f"  - Reward type: Shaped-Sparse (ê²Œì´íŒ… + 1íšŒì„± ì´ë²¤íŠ¸)")
        print(f"  - Curriculum: Phase {self.cfg.curriculum_phase} ({'Easy' if self.cfg.curriculum_phase == 0 else 'Normal'})")
        if self.cfg.curriculum_phase == 0:
            print(f"    â€¢ Cube: {self.cfg.easy_cube_distance[0]*100:.0f}-{self.cfg.easy_cube_distance[1]*100:.0f}cm")
            print(f"    â€¢ Target: {self.cfg.easy_target_distance[0]*100:.0f}-{self.cfg.easy_target_distance[1]*100:.0f}cm")
        else:
            print(f"    â€¢ Cube: {self.cfg.normal_cube_distance[0]*100:.0f}-{self.cfg.normal_cube_distance[1]*100:.0f}cm")
            print(f"    â€¢ Target: {self.cfg.normal_target_distance[0]*100:.0f}-{self.cfg.normal_target_distance[1]*100:.0f}cm")
    
    def _load_robot(self):
        """ë¡œë´‡ URDF ë¡œë“œ"""
        print(f"\nğŸ”§ ë¡œë´‡ ë¡œë”©: {self.cfg.urdf_path}")
        
        # URDF ì„í¬íŠ¸ (Isaac Sim 5.0 ë°©ì‹)
        import omni.kit.commands
        from isaacsim.asset.importer.urdf import _urdf  # âœ… Isaac Sim 5.0 ëª¨ë“ˆ!
        
        import_config = _urdf.ImportConfig()
        import_config.merge_fixed_joints = False
        import_config.fix_base = True
        import_config.import_inertia_tensor = True
        import_config.self_collision = False
        import_config.distance_scale = 1.0
        
        # URDF ì„í¬íŠ¸ (Isaac Sim 5.0 ê³µì‹ ë°©ì‹)
        # âœ… URDFParseAndImportFileëŠ” stageì— ì§ì ‘ importí•˜ë©° prim_pathë¥¼ ë°˜í™˜
        print("  â³ URDF íŒŒì‹± ì¤‘...")
        
        success, prim_path = omni.kit.commands.execute(
            "URDFParseAndImportFile",
            urdf_path=self.cfg.urdf_path,
            import_config=import_config,
            get_articulation_root=True,  # âœ… ê³µì‹ ì˜ˆì œ íŒ¨í„´!
        )
        
        if not success:
            raise RuntimeError(f"âŒ URDF ì„í¬íŠ¸ ì‹¤íŒ¨: {self.cfg.urdf_path}")
        
        print(f"  âœ… ë¡œë´‡ ì„í¬íŠ¸ ì„±ê³µ!")
        print(f"  ï¿½ Prim path: {prim_path}")
        
        # Articulation ìƒì„± (ë°˜í™˜ëœ prim_path ì‚¬ìš©)
        self.robot = self.world.scene.add(
            Articulation(prim_path=prim_path, name="roarm_m3")
        )
        
        # World resetìœ¼ë¡œ articulation ì´ˆê¸°í™”
        print(f"  â³ Articulation ì´ˆê¸°í™” ì¤‘...")
        self.world.reset()
        
        # Joint ì´ë¦„ ê°€ì ¸ì˜¤ê¸°
        self.joint_names = self.robot.dof_names
        print(f"  âœ… Joints ({len(self.joint_names) if self.joint_names else 0}): {self.joint_names[:3] if self.joint_names and len(self.joint_names) > 3 else self.joint_names}...")
        
        # ğŸ”¥ v3.5 FIX #3: Prismatic jointì— linear drive ì ìš©
        print(f"  â³ Joint drive ì„¤ì • ì¤‘...")
        stage = self.world.stage
        
        # ëª¨ë“  jointì— ëŒ€í•´ drive ì„¤ì •
        for i, joint_name in enumerate(self.joint_names):
            joint_prim = stage.GetPrimAtPath(f"{prim_path}/{joint_name}")
            if joint_prim and joint_prim.IsValid():
                # ğŸ”¥ v3.5: Prismatic joint êµ¬ë¶„
                is_gripper = ("gripper" in joint_name)
                
                if is_gripper:
                    # ğŸ”¥ FIX #3: Prismatic jointì—ëŠ” LINEAR drive!
                    drive_api = UsdPhysics.DriveAPI.Apply(joint_prim, "linear")
                    drive_api.GetStiffnessAttr().Set(8000.0)   # ê°•í•œ í˜ìœ¼ë¡œ ì£„ê¸°
                    drive_api.GetDampingAttr().Set(800.0)      # ì•ˆì •ì ì¸ ì œì–´
                    drive_api.GetMaxForceAttr().Set(50.0)      # ğŸ”¥ ì¶©ë¶„í•œ í˜ (8â†’50N)
                    print(f"    âœ… {joint_name}: LINEAR drive (stiff=8000, damp=800, force=50N)")
                elif i < 6:  # íŒ” joint
                    drive_api = UsdPhysics.DriveAPI.Apply(joint_prim, "angular")
                    drive_api.GetStiffnessAttr().Set(5000.0)  # ì™„í™”
                    drive_api.GetDampingAttr().Set(500.0)     # ì™„í™”
                    drive_api.GetMaxForceAttr().Set(500.0)    # ì™„í™”
                else:  # ê·¸ë¦¬í¼
                    drive_api.GetStiffnessAttr().Set(1000.0)
                    drive_api.GetDampingAttr().Set(100.0)
                    drive_api.GetMaxForceAttr().Set(100.0)
        
        print(f"  âœ… Joint drive ì„¤ì • ì™„ë£Œ! (ì™„í™”ëœ ê°’)")
        
        # ğŸ”¥ v3.5: Gripper controller ì´ˆê¸°í™”
        print(f"  â³ Gripper controller ì´ˆê¸°í™” ì¤‘...")
        self.gripper = Gripper(
            stage=stage,
            robot_prim_path=prim_path,
            finger_joint_names=["gripper_left_joint", "gripper_right_joint"]
        )
        # DriveëŠ” ì´ë¯¸ ìœ„ì—ì„œ ì„¤ì •ë¨ (ìˆ˜ë™ìœ¼ë¡œ)
        
        # ğŸ”¥ v3.5: EE prim íƒìƒ‰
        print(f"  â³ End-Effector prim íƒìƒ‰ ì¤‘...")
        self.ee_prim_path = find_ee_prim(stage, prim_path)
        if self.ee_prim_path:
            print(f"  âœ… EE prim: {self.ee_prim_path}")
    
    def _create_objects(self):
        """ë¬¼ì²´ ë° íƒ€ê²Ÿ ìƒì„±"""
        print(f"\nğŸ“¦ ë¬¼ì²´ ìƒì„± ì¤‘...")
        
        # íë¸Œ ìƒì„± (Pick ëŒ€ìƒ)
        self.cube = self.world.scene.add(
            DynamicCuboid(
                prim_path="/World/cube",
                name="cube",
                position=np.array(self.cfg.object_position),
                size=self.cfg.object_size[0],  # ì •ìœ¡ë©´ì²´
                color=np.array([0.8, 0.2, 0.2]),  # ë¹¨ê°„ìƒ‰
            )
        )
        print(f"  âœ… íë¸Œ ìƒì„±: {self.cfg.object_position}")
        
        # ğŸ”¥ v3.5 FIX #4: íë¸Œì— ê³ ë§ˆì°° Physics Material ì ìš©
        stage = self.world.stage
        cube_prim = stage.GetPrimAtPath("/World/cube")
        
        if cube_prim and cube_prim.IsValid():
            # Physics Material ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ì¬ì‚¬ìš©)
            mat_path = "/World/PhysicsMaterials/HighFriction"
            mat_prim = stage.GetPrimAtPath(mat_path)
            
            if not mat_prim or not mat_prim.IsValid():
                # ìƒˆë¡œ ìƒì„±
                mat_prim = stage.DefinePrim(mat_path, "Material")
                
                # âœ… ê¸°ë³¸ ë§ˆì°°/ë°˜ë°œ ì†ì„±: UsdPhysics.MaterialAPI ì‚¬ìš©
                material_api = UsdPhysics.MaterialAPI.Apply(mat_prim)
                material_api.CreateStaticFrictionAttr(1.2)   # Static friction
                material_api.CreateDynamicFrictionAttr(1.0)  # Dynamic friction
                material_api.CreateRestitutionAttr(0.1)      # Restitution (ë‚®ì€ ë°˜ë°œ)
                
                print(f"    âœ… ê³ ë§ˆì°° Material ìƒì„±: friction=1.2/1.0")
            
            # íë¸Œì— Material ë°”ì¸ë”© (UsdShade.MaterialBindingAPI ì‚¬ìš©)
            binding_api = UsdShade.MaterialBindingAPI.Apply(cube_prim)
            binding_api.Bind(UsdShade.Material(mat_prim))
            print(f"    âœ… íë¸Œì— ê³ ë§ˆì°° Material ì ìš©")
        
        # íƒ€ê²Ÿ ë§ˆì»¤ ìƒì„± (ì‹œê°ì  ëª©í‘œ)
        self.target = self.world.scene.add(
            DynamicCuboid(
                prim_path="/World/target",
                name="target",
                position=np.array(self.cfg.target_position),
                size=self.cfg.object_size[0],
                color=np.array([0.2, 0.8, 0.2]),  # ì´ˆë¡ìƒ‰
            )
        )
        # íƒ€ê²Ÿì€ ì •ì  (kinematic)ìœ¼ë¡œ ì„¤ì •
        self.target.set_default_state(position=np.array(self.cfg.target_position))
        
        print(f"  âœ… íƒ€ê²Ÿ ìƒì„±: {self.cfg.target_position}")
    
    def reset(self) -> np.ndarray:
        """í™˜ê²½ ë¦¬ì…‹"""
        print(f"\nğŸ”„ í™˜ê²½ ë¦¬ì…‹ (Step {self.current_step})")
        
        # World ë¦¬ì…‹
        self.world.reset()
        
        # ë¡œë´‡ ì´ˆê¸° ìì„¸ (Home position) - ì•½ê°„ ìœ„ë¡œ ì˜¬ë¦° ìì„¸
        # ì¤‘ë ¥ì— ì˜í•´ ë–¨ì–´ì§€ì§€ ì•Šë„ë¡ ì•ˆì •ì ì¸ ìì„¸
        home_positions = np.array([0.0, -0.5, 0.5, 0.0, 0.0, 0.0, 0.0, 0.0])  # 8 DOF
        self.robot.set_joint_positions(home_positions)
        self.robot.set_joint_velocities(np.zeros(8))
        
        # Physics ìŠ¤í… ì‹¤í–‰í•˜ì—¬ ì•ˆì •í™”
        for _ in range(10):
            self.world.step(render=False)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“š CURRICULUM: Phaseì— ë”°ë¥¸ íë¸Œ/íƒ€ê²Ÿ ìœ„ì¹˜ ì„¤ì •
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.cfg.curriculum_enabled:
            if self.cfg.curriculum_phase == 0:  # Easy Mode
                # íë¸Œë¥¼ ë¡œë´‡ ê°€ê¹Œì´ (10~15cm)
                distance = np.random.uniform(*self.cfg.easy_cube_distance)
                angle = np.random.uniform(0, 2 * np.pi)
                cube_pos = np.array([
                    distance * np.cos(angle),
                    distance * np.sin(angle),
                    0.025  # ğŸ”§ FIX: ë°”ë‹¥ì— ì•ˆì°© (íë¸Œ ë†’ì´ì˜ ì ˆë°˜ = 0.05/2)
                ])
                
                # íƒ€ê²Ÿë„ ê°€ê¹Œì´ (20~25cm)
                target_distance = np.random.uniform(*self.cfg.easy_target_distance)
                target_angle = np.random.uniform(0, 2 * np.pi)
                target_pos = np.array([
                    target_distance * np.cos(target_angle),
                    target_distance * np.sin(target_angle),
                    0.2  # íƒ€ê²Ÿ ë†’ì´
                ])
                
                print(f"  ğŸ“š Phase 0 (Easy): cube={distance:.2f}m, target={target_distance:.2f}m")
            else:  # Normal Mode (Phase 1+)
                # ì›ë˜ ê±°ë¦¬ (25~35cm)
                distance = np.random.uniform(*self.cfg.normal_cube_distance)
                angle = np.random.uniform(0, 2 * np.pi)
                cube_pos = np.array([
                    distance * np.cos(angle),
                    distance * np.sin(angle),
                    0.025  # ğŸ”§ FIX: ë°”ë‹¥ì— ì•ˆì°©
                ])
                
                target_distance = np.random.uniform(*self.cfg.normal_target_distance)
                target_angle = np.random.uniform(0, 2 * np.pi)
                target_pos = np.array([
                    target_distance * np.cos(target_angle),
                    target_distance * np.sin(target_angle),
                    0.2
                ])
                
                print(f"  ğŸ“š Phase {self.cfg.curriculum_phase} (Normal): cube={distance:.2f}m, target={target_distance:.2f}m")
        else:
            # Curriculum ë¹„í™œì„±í™”: ê¸°ë³¸ ìœ„ì¹˜ + ëœë¤
            cube_pos = np.array(self.cfg.object_position)
            cube_pos[:2] += np.random.uniform(-0.05, 0.05, size=2)
            target_pos = np.array(self.cfg.target_position)
        
        self.cube.set_world_pose(position=cube_pos)
        self.cube.set_linear_velocity(np.zeros(3))
        self.cube.set_angular_velocity(np.zeros(3))
        
        # ğŸ”§ FIX: íë¸Œê°€ ë°”ë‹¥ì— ì•ˆì°©í•  ì‹œê°„ ì œê³µ (Physics ì•ˆì •í™”)
        for _ in range(30):  # 30 í”„ë ˆì„ ë™ì•ˆ íë¸Œê°€ ë–¨ì–´ì§€ê³  ì•ˆì •í™”
            self.world.step(render=False)
        
        # íƒ€ê²Ÿ ìœ„ì¹˜ ì—…ë°ì´íŠ¸ (Curriculum ì ìš©)
        if self.cfg.curriculum_enabled:
            self.target.set_world_pose(position=target_pos)
        
        # ğŸ”§ v3.3: ì´ˆê¸° ìƒíƒœ ì•ˆì •í™”ë¥¼ ìœ„í•œ 1ì´ˆ ëŒ€ê¸° (60 í”„ë ˆì„)
        # ë¡œë´‡ì´ ì´ˆê¸° ìì„¸ì—ì„œ ì•ˆì •í™”ë˜ê³ , íë¸Œ/íƒ€ê²Ÿ ìœ„ì¹˜ í™•ì¸ ê°€ëŠ¥
        for _ in range(60):
            self.world.step(render=False)
        
        # ìŠ¤í… ì¹´ìš´í„° ì´ˆê¸°í™”
        self.current_step = 0
        
        # ğŸ”¥ v3.7.2: ê·¸ë¦¬í¼ í­ ì¶”ì  ì´ˆê¸°í™”
        self.current_gripper_width = 0.0
        
        # ì´ì „ ê±°ë¦¬ ì´ˆê¸°í™” (ë³´ìƒ ê³„ì‚°ìš©)
        self.prev_ee_to_cube_dist = None
        self.prev_cube_to_target_dist = None
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ¯ SHAPED-SPARSE: 1íšŒì„± í”Œë˜ê·¸ ë¦¬ì…‹
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.first_reach = False
        self.valid_grip = False
        self.lifted = False
        self.goal_near = False
        self.grip_frames = 0
        self.success_frames = 0
        
        # ğŸ”¥ v3.5: Gripper attach ìƒíƒœ ë¦¬ì…‹
        if self.gripper:
            self.gripper.reset()
        
        # ë§ˆì¼ìŠ¤í†¤ ì¹´ìš´í„° ë¦¬ì…‹
        self.episode_reach_count = 0
        self.episode_grip_count = 0
        self.episode_lift_count = 0
        
        # ğŸ”¥ v3.5: FixedJoint ìƒíƒœ ë¦¬ì…‹ (ì´ì „ ì—í”¼ì†Œë“œ attach ì œê±°)
        if self.gripper and self.gripper.is_attached:
            try:
                self.gripper.detach(self.world.stage)
            except Exception as e:
                print(f"    âš ï¸ Reset ì‹œ FixedJoint ì •ë¦¬ ì‹¤íŒ¨: {e}")
        
        # ì´ˆê¸° observation ë°˜í™˜
        return self._get_observation()
    
    def _get_observation(self) -> np.ndarray:
        """
        í˜„ì¬ ìƒíƒœ ê´€ì¸¡ (ê°œì„ : EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ!)
        
        í•µì‹¬ ê°œì„ :
        1. ì›”ë“œ ì¢Œí‘œ â†’ EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ ë³€í™˜
        2. ì†ë„ ì •ë³´ ì¶”ê°€ (EE, Cube)
        3. ë””ë²„ê¹… ì •ë³´ ì¶”ê°€
        """
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. ê¸°ë³¸ ì •ë³´ ìˆ˜ì§‘ (ì›”ë“œ ì¢Œí‘œ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        joint_positions = self.robot.get_joint_positions()[:8]
        ee_pos = self._get_ee_position()  # ì›”ë“œ ì¢Œí‘œ
        cube_pos, _ = self.cube.get_world_pose()  # ì›”ë“œ ì¢Œí‘œ
        target_pos, _ = self.target.get_world_pose()  # ğŸ”§ v3.2: ì‹¤ì œ íƒ€ê¹ƒ ìœ„ì¹˜ (Curriculum ì ìš©)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 2. EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ ë³€í™˜ (í•µì‹¬!)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ì •ì±…ì´ í•™ìŠµí•˜ê¸° ì‰½ë„ë¡ "EEì—ì„œ ë³¸" íë¸Œ/íƒ€ê²Ÿ ìœ„ì¹˜ ì œê³µ
        cube_relative_to_ee = cube_pos - ee_pos       # EE â†’ Cube ë²¡í„°
        target_relative_to_ee = target_pos - ee_pos   # EE â†’ Target ë²¡í„°
        cube_to_target = target_pos - cube_pos        # Cube â†’ Target ë²¡í„°
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3. ì†ë„ ê³„ì‚° (ì‹œê°„ì  ì •ë³´)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.prev_ee_pos is not None:
            ee_velocity = (ee_pos - self.prev_ee_pos) * 60.0  # 60 FPS ê°€ì •
            cube_velocity = (cube_pos - self.prev_cube_pos) * 60.0
        else:
            ee_velocity = np.zeros(3)
            cube_velocity = np.zeros(3)
        
        self.prev_ee_pos = ee_pos.copy()
        self.prev_cube_pos = cube_pos.copy()
        
        # ===============================================================
        # 4. Gripper state (v3.7.2: use tracked width)
        # ===============================================================
        # ğŸ”¥ v3.7.2 FIX: Physics lag ë¬¸ì œë¡œ ì¸í•´ ì¶”ì ëœ ëª©í‘œê°’ ì‚¬ìš©
        # joint_positionsì—ì„œ ì½ëŠ” ëŒ€ì‹ , step()ì—ì„œ ì„¤ì •í•œ gripper_width ì‚¬ìš©
        gripper_width = self.current_gripper_width
        if self.step_count % 100 == 1:  # DEBUG: ë²„ì „ í™•ì¸
            print(f"[OBS-v3.7.2] step={self.step_count}, tracked gripper_width={gripper_width:.4f}")
        
        # v3.5: Modularized grasp detection
        if self.gripper is not None:
            is_grasped = 1.0 if self.gripper.is_grasped(
                ee_pos=ee_pos,
                cube_pos=cube_pos,
                gripper_width=gripper_width,
                cube_size=self.gate_config.cube_size,
                dist_tol=self.gate_config.grip_dist_tol,
                z_tol=self.gate_config.grip_z_tol,
                width_margin=self.gate_config.grip_width_margin
            ) else 0.0
        else:
            # Fallback: gripper not initialized yet
            # ğŸ”¥ v3.7.3 FIX: gripper_widthë¥¼ ë®ì–´ì“°ì§€ ì•ŠìŒ! (tracked value ìœ ì§€)
            is_grasped = 0.0
        
        # Distance calculations
        ee_to_cube_dist = np.linalg.norm(cube_relative_to_ee)
        z_alignment = abs(cube_relative_to_ee[2])
        
        # ===============================================================
        # 5. Distance information
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        dist_to_cube = ee_to_cube_dist
        dist_cube_to_target = np.linalg.norm(cube_to_target)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 6. Observation ë²¡í„° êµ¬ì„± (28 dim)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        obs = np.concatenate([
            joint_positions[:6],          # Joint positions (6)
            joint_positions[6:8],         # Gripper state (2)
            cube_relative_to_ee,          # Cube relative to EE (3) â† í•µì‹¬!
            target_relative_to_ee,        # Target relative to EE (3) â† í•µì‹¬!
            cube_to_target,               # Cube to Target (3)
            ee_velocity,                  # EE velocity (3)
            cube_velocity,                # Cube velocity (3)
            [gripper_width],              # Gripper width (1)
            [is_grasped],                 # Is grasped (1)
            [dist_to_cube],               # Distance to cube (1)
            [dist_cube_to_target],        # Distance cube to target (1)
            [self.previous_reward],       # Previous reward (1)
        ])
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 7. ë””ë²„ê¹… (ì²« ìŠ¤í…ì—ë§Œ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if self.current_step == 0:
            print(f"\nğŸ” ê´€ì¸¡ ì‹ í˜¸ ì ê²€:")
            print(f"  - Observation dim: {len(obs)} (expected: {self.observation_space_dim})")
            print(f"  - EE pos (world): {ee_pos}")
            print(f"  - Cube pos (world): {cube_pos}")
            print(f"  - Cube relative to EE: {cube_relative_to_ee}")
            print(f"  - Distance to cube: {dist_to_cube:.3f}m")
            print(f"  - Is grasped: {is_grasped}")
        
        return obs
    
    def _get_ee_position(self) -> np.ndarray:
        """
        ğŸ”¥ v3.5: ëª¨ë“ˆí™”ëœ EE í¬ì¦ˆ ì¶”ì¶œ
        
        utils/ee_pose.py ì‚¬ìš©
        """
        if self.ee_prim_path:
            ee_pos, _ = get_ee_position(self.world.stage, self.ee_prim_path)
            
            # ì„±ê³µ ì²´í¬
            if np.linalg.norm(ee_pos) > 0.01:  # ì˜ë²¡í„°ê°€ ì•„ë‹ˆë©´ ì„±ê³µ
                return ee_pos
        
        # Fallback: FK ë°©ì‹
        from robot_utils.ee_pose import get_ee_position_fallback
        joint_positions = self.robot.get_joint_positions()
        return get_ee_position_fallback(joint_positions)
    
    def step(self, action: np.ndarray) -> Tuple[np.ndarray, float, bool, Dict]:
        """í™˜ê²½ ìŠ¤í… ì‹¤í–‰"""
        # ğŸ”¥ v3.7 FIX #1: ì•¡ì…˜ ë§¤í•‘ ëª…í™•í™” (7 DoF + 1 ê·¸ë¦¬í¼ ìŠ¤ì¹¼ë¼)
        # ë¬¸ì œ: ì •ì±…ì´ ê·¸ë¦¬í¼ ì œì–´ í†µë¡œë¥¼ ë°œê²¬í•˜ì§€ ëª»í•¨ (60K ìŠ¤í… ë™ì•ˆ width í•­ìƒ 0.0)
        # í•´ê²°: ë§ˆì§€ë§‰ ì•¡ì…˜ì„ ê·¸ë¦¬í¼ ìŠ¤ì¹¼ë¼ë¡œ ëª…í™•íˆ ë§¤í•‘ (âˆ’1..1 â†’ ì–‘ìª½ í•‘ê±° ëŒ€ì¹­ ì´ë™)
        
        action = np.clip(action, -1.0, 1.0)  # [-1, 1] ë²”ìœ„ë¡œ ì œí•œ
        
        # í˜„ì¬ joint positions ê°€ì ¸ì˜¤ê¸°
        current_positions = self.robot.get_joint_positions()
        
        # ì•¡ì…˜ ë¶„ë¦¬: action[0:6] = ê´€ì ˆ, action[6] = ê·¸ë¦¬í¼ ìŠ¤ì¹¼ë¼
        arm_action = action[:6]  # 6-DOF íŒ” (joint 0-5)
        gripper_scalar = action[6] if len(action) > 6 else 0.0  # ê·¸ë¦¬í¼ ìŠ¤ì¹¼ë¼ (âˆ’1=ì™„ì „ë‹«í˜, +1=ì™„ì „ì—´ë¦¼)
        
        # íŒ” ê´€ì ˆ: position delta ë°©ì‹ (ê¸°ì¡´ ìœ ì§€)
        arm_deltas = arm_action * 0.1  # 10cm/step (rad ë‹¨ìœ„ì´ì§€ë§Œ ì‹¤ì œë¡œëŠ” joint space)
        arm_targets = current_positions[:6] + arm_deltas
        
        # ê·¸ë¦¬í¼: ìŠ¤ì¹¼ë¼ â†’ ì–‘ìª½ í•‘ê±° ëŒ€ì¹­ position (0~0.04m)
        # gripper_scalar: -1 (ì™„ì „ ë‹«í˜=0m) â†’ +1 (ì™„ì „ ì—´ë¦¼=0.04m)
        # 1cm/step ì´ë™ ê°€ëŠ¥ (40mmë¥¼ 40ìŠ¤í…ì— ì™„ì „ ê°œí)
        gripper_position = (gripper_scalar + 1.0) * 0.02  # [0, 0.04] ë²”ìœ„ë¡œ ë§¤í•‘
        gripper_position = np.clip(gripper_position, 0.0, 0.04)
        
        # ğŸ”¥ v3.7.2: ê·¸ë¦¬í¼ í­ ì§ì ‘ ì¶”ì  (ì–‘ìª½ fingerì´ë¯€ë¡œ *2)
        self.current_gripper_width = gripper_position * 2.0
        
        # ğŸ”¥ DEBUG: ê·¸ë¦¬í¼ ì•¡ì…˜ ë¡œê¹… (ë§¤ 100 ìŠ¤í…ë§ˆë‹¤)
        if self.step_count % 100 == 0:
            print(f"[DEBUG-v3.7.2] step={self.step_count}, gripper_scalar={gripper_scalar:.3f}, gripper_pos={gripper_position:.4f}, tracked_width={self.current_gripper_width:.4f}, current_gripper=[{current_positions[6]:.4f}, {current_positions[7]:.4f}]")
        
        # ëª©í‘œ positions ì¡°í•©
        target_positions = np.concatenate([
            arm_targets,
            [gripper_position, gripper_position]  # ì–‘ìª½ í•‘ê±° ë™ì¼
        ])
        
        # Joint limits ì ìš© (âœ… v3.7.3: ëª¨ë“  ê´€ì ˆ Â±180Â° í™•ì¥)
        target_positions = np.clip(
            target_positions,
            [-3.14, -3.14, -3.14, -3.14, -3.14, -3.14, 0.0, 0.0],  # lower limits
            [3.14, 3.14, 3.14, 3.14, 3.14, 3.14, 0.04, 0.04]  # upper limits
        )
        
        # ë¡œë´‡ì— position ëª…ë ¹ ì „ì†¡ (ì§ì ‘ ì„¤ì •)
        self.robot.set_joint_positions(target_positions)
        
        # Physics ì‹œë®¬ë ˆì´ì…˜ ìŠ¤í… (í•™ìŠµ ì‹œ render=False ê¶Œì¥)
        self.world.step(render=False)
        
        # í˜„ì¬ ìƒíƒœ ê´€ì¸¡
        obs = self._get_observation()
        
        # Reward ê³„ì‚°
        reward = self._calculate_reward(obs)
        
        # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
        done = self._check_done(obs)
        
        # ìŠ¤í… ì¹´ìš´í„° ì¦ê°€
        self.current_step += 1
        self.step_count += 1  # ğŸ”¥ v3.7.1: ì „ì—­ ìŠ¤í… ì¹´ìš´í„°
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ“Š ë¡œê¹…: ì´ë²¤íŠ¸ ë° ì§„í–‰ ìƒí™© ì¶”ì 
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”§ v3.2: ì›”ë“œ ì¢Œí‘œë¡œ ì¬ê³„ì‚° (ì‹¤ì œ íƒ€ê¹ƒ ìœ„ì¹˜ ì‚¬ìš©)
        cube_pos, _ = self.cube.get_world_pose()
        target_pos, _ = self.target.get_world_pose()  # ğŸ”§ v3.2: Curriculum ë°˜ì˜
        cube_to_target_dist = float(np.linalg.norm(target_pos - cube_pos))
        
        # ì„±ê³µë¥  ê³„ì‚° (ìµœê·¼ ì—í”¼ì†Œë“œ ê¸°ì¤€)
        success_rate = np.mean(self.episode_successes) if len(self.episode_successes) > 0 else 0.0
        
        # ê·¸ë¦¬í¼ ì •ë³´ ì¶”ê°€
        gripper_width = obs[23]
        is_grasped = obs[24]
        
        info = {
            "step": self.current_step,
            "cube_position": cube_pos.tolist(),  # ğŸ”§ v3.2: ì›”ë“œ ì¢Œí‘œ (obs[11:14]ëŠ” íƒ€ê¹ƒ ìƒëŒ€ì¢Œí‘œ)
            "distance_to_target": float(cube_to_target_dist),
            # ì½œë°±ìš© ë‹¨ê³„ë³„ ë‹¬ì„± í”Œë˜ê·¸ (TrainingProgressCallback)
            "reached_near_cube": self.first_reach,
            "reached_grasp": self.valid_grip,
            "reached_lift": self.lifted,
            "reached_near_target": self.goal_near,
            "is_success": cube_to_target_dist < self.cfg.success_threshold,
            # ë§ˆì¼ìŠ¤í†¤ ì´ë²¤íŠ¸ ì¶”ì 
            "events": {
                "first_reach": self.first_reach,
                "valid_grip": self.valid_grip,
                "lifted": self.lifted,
                "goal_near": self.goal_near,
                "success": cube_to_target_dist < self.cfg.success_threshold,
            },
            # ë§ˆì¼ìŠ¤í†¤ ì¹´ìš´í„°
            "milestone_counts": {
                "reach": self.episode_reach_count,
                "grip": self.episode_grip_count,
                "lift": self.episode_lift_count,
            },
            # ê·¸ë¦¬í¼ ì •ë³´
            "gripper": {
                "width": float(gripper_width),
                "is_grasped": float(is_grasped),
                "grip_frames": self.grip_frames,
            },
            # ì¢…ë£Œ ì‚¬ìœ 
            "done_reason": "ongoing" if not done else (
                "success" if cube_to_target_dist < self.cfg.success_threshold else
                "timeout" if self.current_step >= self.max_steps else
                "safety"
            ),
            # Curriculum ì •ë³´
            "curriculum_phase": self.cfg.curriculum_phase,
            "success_rate": float(success_rate),
        }
        
        return obs, reward, done, info
    
    def _calculate_reward(self, obs: np.ndarray) -> float:
        """
        ğŸ¯ ê°œì„ ëœ HYBRID REWARD: Shaped-Sparse + Dense
        
        ê°œì„  ì‚¬í•­:
        1. Dense Reward ì¶”ê°€: ë§¤ ìŠ¤í… ê±°ë¦¬ ê¸°ë°˜ í”¼ë“œë°±
        2. Shaped-Sparse: ë§ˆì¼ìŠ¤í†¤ ì´ë²¤íŠ¸ ë³´ìƒ ìœ ì§€
        3. ê²Œì´íŒ… ê°•í™”: grasp_valid ì²´í¬
        
        ë³´ìƒ êµ¬ì¡°:
        A. Dense Reward (ë§¤ ìŠ¤í…):
          - EE â†’ Cube ì ‘ê·¼: -distance * 3.0
          - Cube â†’ Target ì ‘ê·¼: -distance * 2.0 (grasp_valid ì‹œ)
          - ì§„ì „ ë³´ë„ˆìŠ¤: +0.5 (ê±°ë¦¬ ì¤„ì–´ë“¤ ë•Œ)
        
        B. Shaped-Sparse (1íšŒì„±):
          - ê·¼ì ‘ (+5): EEê°€ íë¸Œ 5cm ì´ë‚´
          - ê·¸ë¦½ (+10): ìœ íš¨ ê·¸ë¦½ 3í”„ë ˆì„
          - ë¦¬í”„íŠ¸ (+15): íë¸Œ 5cm ì´ìƒ
          - ëª©í‘œ ê·¼ì ‘ (+20): íë¸Œê°€ ëª©í‘œ 8cm ì´ë‚´
          - Success (+100): ëª©í‘œ 5cm ì´ë‚´
        
        C. Penalty:
          - Time penalty: -0.01
        """
        # ê´€ì°°ì—ì„œ í•„ìš”í•œ ê°’ ì¶”ì¶œ (ê°œì„ ëœ 28 dim ê´€ì¸¡)
        # ğŸ”¥ DEBUG: observation ê¸¸ì´ í™•ì¸
        if self.step_count % 100 == 1:
            print(f"[REWARD-DEBUG-PRE] step={self.step_count}, obs.shape={obs.shape}, len={len(obs)}")
        
        cube_relative_to_ee = obs[8:11]    # EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ
        target_relative_to_ee = obs[11:14]  # EE ê¸°ì¤€ ìƒëŒ€ ì¢Œí‘œ
        cube_to_target = obs[14:17]
        ee_velocity = obs[17:20]           # ğŸ”§ FIX: EE ì†ë„ ì¶”ì¶œ
        cube_velocity = obs[20:23]
        gripper_width = obs[23]
        is_grasped = obs[24]
        dist_to_cube = obs[25]
        dist_cube_to_target = obs[26]
        
        # ğŸ”¥ DEBUG: reward ê³„ì‚° ì‹œ gripper_width ê²€ì¦
        if self.step_count % 100 == 1:
            print(f"[REWARD-DEBUG] step={self.step_count}, obs[23]={obs[23]:.4f}, gripper_width={gripper_width:.4f}")
        
        # ì›”ë“œ ì¢Œí‘œ ì¬êµ¬ì„± (ë””ë²„ê¹…ìš©)
        ee_pos = self._get_ee_position()
        cube_pos = ee_pos + cube_relative_to_ee
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ”’ GATING: grasp_valid ì²´í¬ (ğŸ”¥ v3.5: íë¸Œë¥¼ ë¼ìš´ ìƒíƒœ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        CUBE_WIDTH = 0.04  # 4cm íë¸Œ
        is_grasping_cube = (0.035 < gripper_width < 0.045)  # íë¸Œë¥¼ ë¼ìš´ ìƒíƒœ
        
        grasp_valid = (
            dist_to_cube < 0.03 and            # ğŸ”¥ 3cm ì´ë‚´ (ì‹¤ì œ ê·¼ì ‘)
            is_grasping_cube and               # ğŸ”¥ v3.5: íë¸Œë¥¼ ë¼ìš´ ìƒíƒœ (3.5~4.5cm)
            abs(cube_relative_to_ee[2]) < 0.01  # ğŸ”¥ 1cm ì´ë‚´ (Zì¶• ì •ë ¬)
        )
        
        # ğŸ”¥ v3.5: ëª¨ë“ˆí™”ëœ attach/detach ê´€ë¦¬
        if self.gripper is not None:
            if grasp_valid and not self.gripper.is_attached:
                gripper_path = f"{self.robot.prim_path}/gripper_base"
                self.gripper.attach(self.world.stage, gripper_path, "/World/cube", self.current_step)
            elif not grasp_valid and self.gripper.is_attached:
                self.gripper.detach(self.world.stage)
        
        # ğŸ”§ v3.2: grip_frames ê´€ë¦¬ë¥¼ ì•„ë˜ ë³´ìƒ ë¸”ë¡ìœ¼ë¡œ ì´ë™ (ì´ì¤‘ ì¦ê°€ ë²„ê·¸ ë°©ì§€)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ A. DENSE REWARD (Î”í˜• - ê°œì„ ëŸ‰ ê¸°ë°˜) - v3.7 DENSE-HEAVY
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        reward = 0.0
        
        # Time penalty ì œê±° (dense rewardë§Œìœ¼ë¡œ ì¶©ë¶„)
        # reward -= 0.001  # ğŸ”¥ v3.7: Time penalty ì œê±°
        
        # ğŸ”§ v3.7.1 NEW: EE Orientation ë³´ìƒ (ê·¸ë¦¬í¼ê°€ íë¸Œë¥¼ í–¥í•˜ë„ë¡)
        # ë¬¸ì œ: ë¡œë´‡ì´ íŒ”ê¿ˆì¹˜ë¥¼ íë¸Œì— ê°€ê¹Œì´ í•˜ê³ , ê·¸ë¦¬í¼ëŠ” ë°˜ëŒ€ ë°©í–¥ì„ í–¥í•¨
        # í•´ê²°: EEì˜ forward vectorì™€ íë¸Œ ë°©í–¥ì„ ì •ë ¬
        try:
            from pxr import UsdGeom, Gf
            ee_prim = self.world.stage.GetPrimAtPath(self.ee_prim_path)
            if ee_prim.IsValid():
                xformable = UsdGeom.Xformable(ee_prim)
                world_transform = xformable.ComputeLocalToWorldTransform(0)
                
                # Forward vector: -Z axis in world space (USD convention)
                forward_vector = np.array([-world_transform.GetRow(2)[0],
                                          -world_transform.GetRow(2)[1],
                                          -world_transform.GetRow(2)[2]])
                
                # Cube direction (normalized)
                if dist_to_cube > 1e-6:
                    cube_direction = cube_relative_to_ee / dist_to_cube
                    
                    # Alignment: 1.0 = perfectly aligned, -1.0 = opposite
                    orientation_alignment = np.dot(forward_vector, cube_direction)
                    
                    # Strong reward for pointing gripper at cube
                    reward += 10.0 * max(0, orientation_alignment)  # Only positive alignment
        except Exception as e:
            pass  # Fallback: skip orientation reward if error
        
        # ğŸ”§ FIX: 1. EE â†’ Cube ë°©í–¥ì„± ë³´ìƒ (íë¸Œë¥¼ í–¥í•´ ì›€ì§ì´ëŠ” í–‰ë™ ê°•í™”)
        # EEì˜ ì†ë„ ë²¡í„°ì™€ íë¸Œ ë°©í–¥ ë²¡í„°ì˜ ë‚´ì ìœ¼ë¡œ ë°©í–¥ì„± í‰ê°€
        if np.linalg.norm(ee_velocity) > 0.01:  # EEê°€ ì›€ì§ì´ê³  ìˆì„ ë•Œë§Œ
            cube_direction = cube_relative_to_ee / (dist_to_cube + 1e-6)  # ì •ê·œí™”ëœ ë°©í–¥
            ee_velocity_norm = ee_velocity / (np.linalg.norm(ee_velocity) + 1e-6)
            
            # ë‚´ì : ê°™ì€ ë°©í–¥ì´ë©´ +1, ë°˜ëŒ€ë©´ -1
            direction_alignment = np.dot(ee_velocity_norm, cube_direction)
            reward += 5.0 * direction_alignment  # ğŸ”¥ v3.7: 2.0 â†’ 5.0 (2.5ë°° ê°•í™”)
        
        # 2. EE â†’ Cube ì ‘ê·¼ ë³´ìƒ (ê°œì„ ëŸ‰ ê¸°ë°˜) - ë” ê°•í™”!
        if self.prev_ee_to_cube_dist is not None:
            ee_progress = self.prev_ee_to_cube_dist - dist_to_cube
            reward += 20.0 * ee_progress  # ğŸ”¥ v3.7: 10.0 â†’ 20.0 (2ë°° ê°•í™”)
        
        # 3. ê±°ë¦¬ ê¸°ë°˜ ì¶”ê°€ ë³´ìƒ (ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ë³´ìƒ) - ë” ê°•í™”!
        # íë¸Œì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì§€ì†ì ìœ¼ë¡œ ì–‘ì˜ ë³´ìƒ
        distance_reward = max(0, 0.3 - dist_to_cube) * 10.0  # ğŸ”¥ v3.7: 5.0 â†’ 10.0 (2ë°° ê°•í™”)
        reward += distance_reward
        
        # ğŸ”¥ v3.7 NEW: 4. ê·¸ë¦¬í¼ ê°œí ë³´ìƒ (ê·¸ë¦¬í¼ ì‚¬ìš© ê°•ë ¥ ìœ ë„)
        # ë¬¸ì œ: 60K ìŠ¤í… ë™ì•ˆ ê·¸ë¦¬í¼ width í•­ìƒ 0.0 â†’ ì •ì±…ì´ ê·¸ë¦¬í¼ë¥¼ ì „í˜€ ì‚¬ìš© ì•ˆ í•¨
        # í•´ê²°: ê·¸ë¦¬í¼ë¥¼ ì—¬ëŠ” í–‰ë™ì— ëª…ì‹œì  ë³´ìƒ
        if gripper_width > 0.01:  # 1cm ì´ìƒ ì—´ë©´
            reward += 3.0  # ë§¤ ìŠ¤í… +3.0 (ê·¸ë¦¬í¼ ì‚¬ìš© ê°•ë ¥ ìœ ë„)
        
        # ğŸ”¥ v3.7 NEW: 5. ì ì ˆí•œ ê·¸ë¦¬í¼ width ë³´ìƒ (íë¸Œ í¬ê¸° ê³ ë ¤)
        # íë¸Œê°€ ê°€ê¹Œìš¸ ë•Œ ì ì ˆí•œ ê·¸ë¦¬í¼ width ìœ ì§€ ì‹œ ë³´ìƒ
        if dist_to_cube < 0.1:  # 10cm ì´ë‚´ì—ì„œ
            CUBE_WIDTH = 0.04
            ideal_width = CUBE_WIDTH * 1.1  # íë¸Œë³´ë‹¤ ì•½ê°„ ë„“ê²Œ (4.4cm)
            width_diff = abs(gripper_width - ideal_width)
            # widthê°€ idealì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ë³´ìƒ (0~2.0)
            width_reward = max(0, 2.0 - 50.0 * width_diff)  # 1mm ì°¨ì´ë‹¹ -0.05
            reward += width_reward
        
        # 6. Cube â†’ Target ì ‘ê·¼ ë³´ìƒ (grasp_valid ì‹œë§Œ, ê°œì„ ëŸ‰ ê¸°ë°˜) - ê°•í™”!
        if grasp_valid and self.prev_cube_to_target_dist is not None:
            cube_progress = self.prev_cube_to_target_dist - dist_cube_to_target
            reward += 15.0 * cube_progress  # ğŸ”¥ v3.7: 8.0 â†’ 15.0 (ê±°ì˜ 2ë°° ê°•í™”)
        
        # ê±°ë¦¬ ì´ë ¥ ì—…ë°ì´íŠ¸
        self.prev_ee_to_cube_dist = dist_to_cube
        self.prev_cube_to_target_dist = dist_cube_to_target
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ B. SHAPED-SPARSE REWARDS (1íšŒì„± ì´ë²¤íŠ¸) - v3.7 ê±°ì˜ ì œê±°
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        
        # ğŸ”¥ v3.7: Milestone ë³´ìƒ ëŒ€í­ ê°ì†Œ (Dense rewardê°€ ì£¼ë„)
        # ì´ìœ : REACH +2 í›„ì—ë„ ì—ì´ì „íŠ¸ê°€ "ë„ì°©=ì´ë“" ì°©ê° â†’ í–‰ë™ ì •ì²´
        # í•´ê²°: Milestoneì„ "ì¶•í•˜ ë©”ì‹œì§€" ìˆ˜ì¤€ìœ¼ë¡œë§Œ ìœ ì§€
        
        # 1ï¸âƒ£ ê·¼ì ‘ ë³´ìƒ (+0.5): EEê°€ íë¸Œì— ì²˜ìŒ ê·¼ì ‘ (ê°ì†Œ: 2â†’0.5)
        if not self.first_reach and dist_to_cube < 0.05:
            reward += 0.5  # ğŸ”¥ v3.7: 2.0 â†’ 0.5 (ì¶•í•˜ ë©”ì‹œì§€ ìˆ˜ì¤€)
            self.first_reach = True
            self.episode_reach_count += 1
            print(f"  ğŸ¯ Milestone: REACH! (+0.5)")
        
        # ğŸš¨ v3.7: GRIP ì¡°ê±´ ë””ë²„ê¹… ë¡œê·¸ (ê·¸ë¦¬í¼ width ì¶”ì )
        if not self.valid_grip:
            if self.current_step % 50 == 0:  # 50 ìŠ¤í…ë§ˆë‹¤ (ë” ìì£¼ ì²´í¬)
                CUBE_WIDTH = 0.04
                is_grasping = (0.035 < gripper_width < 0.045)
                print(f"  ğŸ” GRIP ì²´í¬ (v3.7): dist={dist_to_cube:.3f}, "
                      f"width={gripper_width:.4f} (cube={CUBE_WIDTH*100:.0f}cm), "
                      f"valid={grasp_valid}")
        
        # 2ï¸âƒ£ ê·¸ë¦½ ë³´ìƒ (+10): ìœ íš¨ ê·¸ë¦½ 3í”„ë ˆì„ ìœ ì§€ [ê²Œì´íŒ…]
        # Dense rewardê°€ ì£¼ë„í•˜ë¯€ë¡œ milestoneì€ ì¶•í•˜ ìˆ˜ì¤€ (20 â†’ 10)
        if grasp_valid:
            self.grip_frames += 1
        else:
            self.grip_frames = 0
            
        if not self.valid_grip and grasp_valid and self.grip_frames >= 3:
            reward += 10.0  # ğŸ”¥ v3.7: 20.0 â†’ 10.0
            self.valid_grip = True
            self.episode_grip_count += 1
            print(f"  âœŠ Milestone: GRIP! (+10.0) [dist={dist_to_cube:.3f}, width={gripper_width:.4f}]")
        
        # 3ï¸âƒ£ ë¦¬í”„íŠ¸ ë³´ìƒ (+15): íë¸Œ 5cm ì´ìƒ ë“¤ì–´ì˜¬ë¦¼ [ê²Œì´íŒ…]
        # Dense rewardê°€ ì£¼ë„í•˜ë¯€ë¡œ milestoneì€ ì¶•í•˜ ìˆ˜ì¤€ (30 â†’ 15)
        if not self.lifted and grasp_valid and cube_pos[2] > 0.05:
            reward += 15.0  # ğŸ”¥ v3.7: 30.0 â†’ 15.0
            self.lifted = True
            self.episode_lift_count += 1
            print(f"  â¬†ï¸ Milestone: LIFT! (+15.0)")
        
        # 4ï¸âƒ£ ëª©í‘œ ê·¼ì ‘ ë³´ìƒ (ì œê±°): LIFTì™€ SUCCESS ì‚¬ì´ ê°„ê²©ì´ í¬ì§€ ì•Šì•„ ì œê±°
        # if not self.goal_near and grasp_valid and dist_cube_to_target < 0.08:
        #     reward += 20.0
        #     self.goal_near = True
        #     print(f"  ğŸ¯ Milestone: GOAL NEAR! (+20.0)")
        
        # 5ï¸âƒ£ Success ë³´ìƒ (+50): ëª©í‘œ threshold ì´ë‚´ Ní”„ë ˆì„ ì—°ì† ìœ ì§€
        # ğŸ”¥ v3.7: 100 â†’ 50 (Dense rewardê°€ ì£¼ë„, Milestoneì€ ì¶•í•˜ ìˆ˜ì¤€)
        if dist_cube_to_target < self.cfg.success_threshold:
            self.success_frames += 1
            if self.success_frames >= self.cfg.success_hold_frames:
                reward += 50.0  # ğŸ”¥ v3.7: 100.0 â†’ 50.0
                print(f"  ğŸ† Milestone: SUCCESS! (+100.0) [{self.success_frames} frames]")
        else:
            self.success_frames = 0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ğŸ C. REWARD CLIPPING (1ìŠ¤í… ë³´ìƒ ì œí•œ)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # ìŠ¤íŒŒì´í¬ ë³´ìƒ ì œì™¸í•˜ê³  Dense ë³´ìƒë§Œ í´ë¨í•‘
        if reward < 90.0:  # í° ì´ë²¤íŠ¸ ë³´ìƒ ì œì™¸
            reward = np.clip(reward, -2.0, 2.0)
        
        # Previous reward ì €ì¥ (ê´€ì¸¡ì— í¬í•¨)
        self.previous_reward = reward
        
        return reward
    
    def _check_done(self, obs: np.ndarray) -> bool:
        """ì—í”¼ì†Œë“œ ì¢…ë£Œ ì¡°ê±´ + ì„±ê³µë¥  ì¶”ì 
        
        âœ… SUCCESS ì¡°ê±´ ê°•í™”:
        - threshold ì´ë‚´ + Ní”„ë ˆì„ ì—°ì† ìœ ì§€ í•„ìˆ˜!
        """
        # ğŸ”§ BUG FIX: ì›”ë“œ ì¢Œí‘œë¡œ ì •í™•í•˜ê²Œ ì¬ê³„ì‚°
        cube_pos, _ = self.cube.get_world_pose()
        target_pos = np.array(self.cfg.target_position)
        cube_to_target_dist = float(np.linalg.norm(target_pos - cube_pos))
        
        # ê´€ì¸¡ ë²¡í„°ì˜ íë¸Œ ìœ„ì¹˜ë„ ì°¸ê³  (ë””ë²„ê¹…ìš©)
        cube_pos_obs = obs[11:14]
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # âœ… SUCCESS ì¡°ê±´: threshold ì´ë‚´ + Ní”„ë ˆì„ ì—°ì† ìœ ì§€
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        if cube_to_target_dist < self.cfg.success_threshold:
            # ì•„ì§ Ní”„ë ˆì„ ìœ ì§€ ì•ˆ ë¨ â†’ ê³„ì† ì§„í–‰
            if self.success_frames < self.cfg.success_hold_frames:
                # ì§„í–‰ ìƒí™© ë¡œê·¸ (ë§¤ í”„ë ˆì„ë§ˆë‹¤ëŠ” ì•„ë‹ˆê³  5í”„ë ˆì„ë§ˆë‹¤)
                if self.success_frames % 5 == 0 and self.success_frames > 0:
                    print(f"  â³ Holding... {self.success_frames}/{self.cfg.success_hold_frames} frames (dist: {cube_to_target_dist:.3f}m)")
                return False  # ì•„ì§ ì¢…ë£Œí•˜ì§€ ì•ŠìŒ!
            else:
                # Ní”„ë ˆì„ ìœ ì§€ ì™„ë£Œ â†’ SUCCESS!
                print(f"  âœ… SUCCESS CONFIRMED! Distance: {cube_to_target_dist:.3f}m (held {self.success_frames} frames)")
                self._record_success(True)
                return True
        
        # ìµœëŒ€ ìŠ¤í… ë„ë‹¬
        if self.current_step >= self.max_steps:
            print(f"  â±ï¸ Timeout (Max steps: {self.max_steps})")
            self._record_success(False)
            return True
        
        # ë¬¼ì²´ê°€ í…Œì´ë¸” ë°–ìœ¼ë¡œ ë–¨ì–´ì§ (Z < 0)
        if cube_pos[2] < -0.1:
            print(f"  âŒ Cube fell off table (Z: {cube_pos[2]:.3f}m)")
            self._record_success(False)
            return True
            self._record_success(False)
            return True
        
        return False
    
    def _record_success(self, success: bool):
        """
        ì„±ê³µë¥  ì¶”ì  ë° ìë™ ìŠ¹ê¸‰ ì²´í¬
        """
        # ì„±ê³µ ì—¬ë¶€ ê¸°ë¡
        self.episode_successes.append(1.0 if success else 0.0)
        
        # ìœˆë„ìš° í¬ê¸° ì œí•œ
        if len(self.episode_successes) > self.cfg.success_rate_window:
            self.episode_successes.pop(0)
        
        # ì„±ê³µë¥  ê³„ì‚° (ìµœì†Œ 50 ì—í”¼ì†Œë“œ ì´ìƒ)
        if len(self.episode_successes) >= 50:
            success_rate = np.mean(self.episode_successes)
            
            # Curriculum ìŠ¹ê¸‰ ì²´í¬ (Phase 0 â†’ Phase 1)
            if (self.cfg.curriculum_enabled and 
                self.cfg.curriculum_phase == 0 and
                success_rate >= self.cfg.success_rate_threshold):
                
                print("\n" + "=" * 60)
                print(f"ğŸ“ CURRICULUM UPGRADE! Phase 0 â†’ Phase 1")
                print(f"   Success Rate: {success_rate:.1%} (â‰¥{self.cfg.success_rate_threshold:.0%})")
                print(f"   Window: {len(self.episode_successes)} episodes")
                print("=" * 60 + "\n")
                
                # Phase 1ìœ¼ë¡œ ìŠ¹ê¸‰
                self.cfg.curriculum_phase = 1
                self.episode_successes.clear()  # ì„±ê³µë¥  ë¦¬ì…‹
    
    def render(self):
        """ë Œë”ë§ (Isaac Simì—ì„œ ìë™ ì²˜ë¦¬)"""
        pass
    
    def close(self):
        """í™˜ê²½ ì¢…ë£Œ"""
        print("\nğŸ›‘ í™˜ê²½ ì¢…ë£Œ")
        self.world.stop()
    
if __name__ == "__main__":
    print("ğŸš€ RoArm-M3 Pick and Place í™˜ê²½ í…ŒìŠ¤íŠ¸\n")
    
    # í™˜ê²½ ìƒì„±
    cfg = RoArmPickPlaceEnvCfg()
    cfg.episode_length_s = 5.0  # ì§§ê²Œ í…ŒìŠ¤íŠ¸
    
    env = RoArmPickPlaceEnv(cfg)
    
    # ëª‡ ì—í”¼ì†Œë“œ í…ŒìŠ¤íŠ¸
    for episode in range(2):
        print(f"\n{'='*60}")
        print(f"Episode {episode + 1}")
        print(f"{'='*60}")
        
        obs = env.reset()
        print(f"Initial observation shape: {obs.shape}")
        
        done = False
        total_reward = 0
        step = 0
        
        while not done and step < 100:
            # ëœë¤ ì•¡ì…˜
            action = np.random.uniform(-0.5, 0.5, size=8)
            
            obs, reward, done, info = env.step(action)
            total_reward += reward
            step += 1
            
            if step % 20 == 0:
                print(f"  Step {step}: Reward={reward:.2f}, Distance={info['distance_to_target']:.3f}m")
        
        print(f"\n  ğŸ“Š Episode {episode + 1} ê²°ê³¼:")
        print(f"    - Total steps: {step}")
        print(f"    - Total reward: {total_reward:.2f}")
        print(f"    - Final distance: {info['distance_to_target']:.3f}m")
    
    env.close()
    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
