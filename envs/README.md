# RoArm-M3 ê°•í™”í•™ìŠµ í™˜ê²½ README

## ğŸ“‹ ê°œìš”

RoArm-M3 ë¡œë´‡íŒ”ì˜ Pick and Place ì‘ì—…ì„ ìœ„í•œ ê°•í™”í•™ìŠµ í™˜ê²½ì…ë‹ˆë‹¤.

- **í™˜ê²½**: Isaac Sim 5.0
- **ì•Œê³ ë¦¬ì¦˜**: PPO (Proximal Policy Optimization)
- **í”„ë ˆì„ì›Œí¬**: Stable-Baselines3
- **ì‘ì—…**: íë¸Œë¥¼ ì§‘ì–´ì„œ íƒ€ê²Ÿ ìœ„ì¹˜ë¡œ ì˜®ê¸°ê¸°

## ğŸ¯ í™˜ê²½ ìŠ¤í™

### Observation Space (15 dim)
- Joint positions (6 dim): `joint_1 ~ joint_6`
- Gripper state (2 dim): `left_finger, right_finger` positions
- End-effector position (3 dim): `x, y, z`
- Object position (3 dim): `x, y, z`
- Object-target distance (1 dim)

### Action Space (8 dim)
- Joint velocities (6 dim): `joint_1 ~ joint_6`
- Gripper velocities (2 dim): `left_finger, right_finger`

### Reward Function
```python
reward = -distance * 10.0  # Distance-based penalty
        + 100.0           # Success bonus (distance < 5cm)
```

### Termination Conditions
1. Success: Object reaches target (distance < 5cm)
2. Timeout: Max steps reached (600 steps = 10 seconds)
3. Failure: Object falls off table (z < -0.1m)

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í™˜ê²½ ì„¤ì •
```bash
# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
bash scripts/setup_rl_env.sh
```

### 2. í•™ìŠµ
```bash
# 50K ìŠ¤í… í•™ìŠµ (ì•½ 30-60ë¶„)
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode train --timesteps 50000

# ë” ê¸´ í•™ìŠµ (100K ìŠ¤í…)
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode train --timesteps 100000
```

### 3. í…ŒìŠ¤íŠ¸
```bash
# Best ëª¨ë¸ë¡œ í…ŒìŠ¤íŠ¸
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode test

# íŠ¹ì • ëª¨ë¸ í…ŒìŠ¤íŠ¸
~/isaac-sim.sh -m scripts/train_roarm_rl.py --mode test \\
  --model logs/rl_training/checkpoints/roarm_ppo_10000_steps.zip
```

## ğŸ“Š í•™ìŠµ ëª¨ë‹ˆí„°ë§

### TensorBoard
```bash
# í•™ìŠµ ì¤‘ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
tensorboard --logdir logs/rl_training/tensorboard
# â†’ http://localhost:6006
```

**ì£¼ìš” ì§€í‘œ**:
- `rollout/ep_rew_mean`: ì—í”¼ì†Œë“œ í‰ê·  ë³´ìƒ
- `rollout/ep_len_mean`: ì—í”¼ì†Œë“œ í‰ê·  ê¸¸ì´
- `train/policy_loss`: Policy network loss
- `train/value_loss`: Value network loss

### ì²´í¬í¬ì¸íŠ¸
í•™ìŠµ ì¤‘ ìë™ ì €ì¥:
- `logs/rl_training/checkpoints/`: 5,000 ìŠ¤í…ë§ˆë‹¤
- `logs/rl_training/best_model/`: í‰ê°€ ì‹œ ìµœê³  ì„±ëŠ¥ ëª¨ë¸

## ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹

`scripts/train_roarm_rl.py`ì˜ PPO íŒŒë¼ë¯¸í„°:

```python
model = PPO(
    learning_rate=3e-4,      # í•™ìŠµë¥ 
    n_steps=2048,            # ë²„í¼ í¬ê¸°
    batch_size=64,           # ë¯¸ë‹ˆë°°ì¹˜ í¬ê¸°
    n_epochs=10,             # ì—…ë°ì´íŠ¸ ì—í­ ìˆ˜
    gamma=0.99,              # í• ì¸ìœ¨
    gae_lambda=0.95,         # GAE lambda
    clip_range=0.2,          # PPO clipping
    ent_coef=0.01,           # Entropy coefficient
)
```

**íŠœë‹ íŒ**:
- í•™ìŠµì´ ë„ˆë¬´ ëŠë¦¬ë©´: `learning_rate` ì¦ê°€ (3e-4 â†’ 5e-4)
- ë¶ˆì•ˆì •í•˜ë©´: `learning_rate` ê°ì†Œ, `clip_range` ê°ì†Œ
- Exploration ë¶€ì¡±: `ent_coef` ì¦ê°€ (0.01 â†’ 0.05)

## ğŸ“ˆ ê¸°ëŒ€ ì„±ëŠ¥

### í•™ìŠµ ê³¡ì„  (ì˜ˆìƒ)
| Timesteps | Success Rate | Avg Reward |
|-----------|--------------|------------|
| 10K       | ~5%          | -50        |
| 25K       | ~20%         | -20        |
| 50K       | ~40%         | 0          |
| 100K      | ~60%         | +20        |

**ì°¸ê³ **: ì‹¤ì œ ì„±ëŠ¥ì€ ì‹œë“œ, í™˜ê²½ ì„¤ì •ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸ› ë¬¸ì œ í•´ê²°

### ImportError: omni.isaac.core
**ì›ì¸**: Isaac Sim Python í™˜ê²½ì´ ì•„ë‹™ë‹ˆë‹¤.  
**í•´ê²°**: ë°˜ë“œì‹œ `~/isaac-sim.sh -m` ë˜ëŠ” `isaacsim` ëª…ë ¹ì–´ë¡œ ì‹¤í–‰

### CUDA out of memory
**ì›ì¸**: GPU ë©”ëª¨ë¦¬ ë¶€ì¡±  
**í•´ê²°**:
1. `num_envs` ê°ì†Œ (í™˜ê²½ ë³‘ë ¬í™” ìˆ˜)
2. `batch_size` ê°ì†Œ
3. Headless modeë¡œ ì‹¤í–‰ (ìŠ¤í¬ë¦½íŠ¸ì—ì„œ `headless=True`)

### í•™ìŠµì´ ì§„í–‰ë˜ì§€ ì•ŠìŒ
**ì›ì¸**: Reward shaping ë¬¸ì œ  
**í•´ê²°**:
1. `distance_reward_scale` ì¡°ì • (10.0 â†’ 5.0 ë˜ëŠ” 20.0)
2. Success threshold ì™„í™” (0.05m â†’ 0.10m)
3. Episode length ì¦ê°€ (10ì´ˆ â†’ 15ì´ˆ)

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
roarm_isaac_clean/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ roarm_pick_place_env.py    # í™˜ê²½ ì •ì˜
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_roarm_rl.py          # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
â”‚   â””â”€â”€ setup_rl_env.sh            # í™˜ê²½ ì„¤ì •
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ rl_training/               # í•™ìŠµ ë¡œê·¸
â”‚       â”œâ”€â”€ tensorboard/           # TensorBoard ë¡œê·¸
â”‚       â”œâ”€â”€ checkpoints/           # ì²´í¬í¬ì¸íŠ¸
â”‚       â”œâ”€â”€ best_model/            # Best ëª¨ë¸
â”‚       â””â”€â”€ eval_logs/             # í‰ê°€ ë¡œê·¸
â””â”€â”€ assets/
    â””â”€â”€ roarm_m3/
        â””â”€â”€ urdf/
            â””â”€â”€ roarm_m3_multiprim.urdf
```

## ğŸ”¬ ê³ ê¸‰ ì‚¬ìš©

### ì»¤ìŠ¤í…€ Reward Function
`envs/roarm_pick_place_env.py`ì˜ `_calculate_reward()` ìˆ˜ì •:

```python
def _calculate_reward(self, obs: np.ndarray) -> float:
    distance = obs[14]
    
    # ê¸°ì¡´: ê±°ë¦¬ ê¸°ë°˜ + Success bonus
    reward = -distance * 10.0
    if distance < 0.05:
        reward += 100.0
    
    # ì¶”ê°€: Gripper ë‹«í˜ ë³´ìƒ
    gripper_distance = abs(obs[6] - obs[7])  # Left - Right
    if gripper_distance < 0.01:  # ë‹«í˜”ì„ ë•Œ
        reward += 10.0
    
    # ì¶”ê°€: End-effectorê°€ ë¬¼ì²´ì— ê°€ê¹Œìš¸ ë•Œ
    ee_pos = obs[7:10]
    obj_pos = obs[9:12]
    ee_obj_dist = np.linalg.norm(ee_pos - obj_pos)
    reward -= ee_obj_dist * 5.0
    
    return reward
```

### ë³‘ë ¬ í™˜ê²½ (ì†ë„ í–¥ìƒ)
`train_roarm_rl.py`ì—ì„œ `num_envs` ì¦ê°€:

```python
from stable_baselines3.common.vec_env import SubprocVecEnv

# 4ê°œ í™˜ê²½ ë³‘ë ¬ ì‹¤í–‰
env = SubprocVecEnv([make_env for _ in range(4)])
```

**ì£¼ì˜**: GPU ë©”ëª¨ë¦¬ ì¶©ë¶„í•œì§€ í™•ì¸!

## ğŸ“š ì°¸ê³  ìë£Œ

- [Stable-Baselines3 ë¬¸ì„œ](https://stable-baselines3.readthedocs.io/)
- [Isaac Sim Python API](https://docs.omniverse.nvidia.com/isaacsim/latest/python_api.html)
- [PPO ë…¼ë¬¸](https://arxiv.org/abs/1707.06347)
- [Gymnasium ë¬¸ì„œ](https://gymnasium.farama.org/)

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

1. **í™˜ê²½ ë³µì¡ë„ ì¦ê°€**:
   - ì—¬ëŸ¬ ë¬¼ì²´ (Multi-object)
   - ë‹¤ì–‘í•œ í˜•ìƒ (êµ¬, ì‹¤ë¦°ë” ë“±)
   - ì¥ì• ë¬¼ ì¶”ê°€

2. **ì•Œê³ ë¦¬ì¦˜ ë¹„êµ**:
   - SAC (Soft Actor-Critic)
   - TD3 (Twin Delayed DDPG)
   - DreamerV3

3. **Real-to-Sim Transfer**:
   - Domain Randomization
   - System Identification
   - ì‹¤ì œ ë¡œë´‡ í…ŒìŠ¤íŠ¸

---

**ìµœì¢… ì—…ë°ì´íŠ¸**: 2025-10-19  
**ì‘ì„±ì**: Jarvis AI Assistant
