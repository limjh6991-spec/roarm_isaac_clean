# RoArm-M3 v3.8.1 GRIP 조건 강화 학습 계획

**작성일**: 2025-10-24  
**목적**: GRIP 조건을 강화하여 실제로 큐브를 물리적으로 잡을 수 있도록 학습  
**상태**: 진행 중 (5K / 300K timesteps)

---

## 📋 목차
1. [문제 정의](#문제-정의)
2. [GRIP 조건 변경사항](#grip-조건-변경사항)
3. [보상 구조 상세](#보상-구조-상세)
4. [학습 전략](#학습-전략)
5. [예상 학습 진행](#예상-학습-진행)
6. [검증 요청사항](#검증-요청사항)

---

## 🔍 문제 정의

### 이전 학습의 문제점
```yaml
문제:
  - GRIP 성공률: 38% (300K 학습 후)
  - LIFT 성공률: 0% (큐브를 실제로 잡지 못함)
  - 원인: GRIP 조건이 너무 느슨함

증상:
  - 로봇이 큐브 5cm 거리에서 "GRIP 달성"으로 인정
  - 실제로는 큐브를 잡지 못하고 있음
  - 그리퍼 너비 2.5-6.0cm로 너무 넓음 (큐브 4cm)
  - GUI 재생 시 큐브가 바닥에 남아있음
```

### 목표
```yaml
단기 목표:
  - GRIP 조건 강화: 5cm → 3cm
  - 실제 물리적 잡기 가능하도록 조건 엄격화
  
장기 목표:
  - LIFT 마일스톤 달성 (큐브 5cm 들어올리기)
  - 실제 로봇 전이 가능한 정책 학습
```

---

## 🔧 GRIP 조건 변경사항

### v3.7.7 → v3.8.1 비교

| 조건 | v3.7.7 (이전) | v3.8.1 (현재) | 변화율 |
|------|--------------|--------------|--------|
| **거리** | < 5cm | < **3cm** | **-40%** ⚠️ |
| **그리퍼 너비** | 2.5-6.0cm | **3.0-5.0cm** | **±0.5cm** ⚠️ |
| **Z축 정렬** | < 2cm | < **1.5cm** | **-25%** ⚠️ |
| **유지 프레임** | 3 frames | 3 frames | 동일 |

### 코드 변경
```python
# 이전 (v3.7.7)
is_grasping_cube = (0.025 < gripper_width < 0.060)
grasp_valid = (
    dist_to_cube < 0.05 and
    is_grasping_cube and
    abs(cube_relative_to_ee[2]) < 0.02
)

# 현재 (v3.8.1)
is_grasping_cube = (0.030 < gripper_width < 0.050)  # 강화
grasp_valid = (
    dist_to_cube < 0.03 and              # 5cm → 3cm
    is_grasping_cube and                 # 2.5-6.0 → 3.0-5.0cm
    abs(cube_relative_to_ee[2]) < 0.015  # 2cm → 1.5cm
)
```

### PERFECT GRIP 조건도 강화
```python
# 이전
if dist_to_cube < 0.03 and abs(cube_relative_to_ee[2]) < 0.01:
    reward += 100.0

# 현재
if dist_to_cube < 0.02 and abs(cube_relative_to_ee[2]) < 0.008:  # 더 엄격
    reward += 100.0
```

---

## 🎁 보상 구조 상세

### A. Dense Reward (매 스텝)

#### 1. EE → Cube 접근 (주력 보상)
```python
# 1) 방향성 보상
if np.linalg.norm(ee_velocity) > 0.01:
    direction_alignment = np.dot(ee_velocity_norm, cube_direction)
    reward += 5.0 * direction_alignment
    
# 2) 진전 보상 (거리 개선)
ee_progress = prev_dist - current_dist
reward += 20.0 * ee_progress  # ★ 가장 강력

# 3) 거리 기반 보상
distance_reward = max(0, 0.3 - dist_to_cube) * 10.0
reward += distance_reward
```

**의도**: 로봇이 큐브에 가까워질수록 지속적으로 높은 보상

#### 2. 그리퍼 제어 유도
```python
# 1) 그리퍼 사용 장려
if gripper_width > 0.01:
    reward += 3.0  # 매 스텝

# 2) Width 최적화 (4cm 선호)
if dist_to_cube < 0.15:
    IDEAL_WIDTH = 0.04  # 큐브 크기
    width_error = gripper_width - IDEAL_WIDTH
    width_penalty = -5.0 * (width_error ** 2)
    reward += width_penalty
```

**의도**: 그리퍼를 큐브 크기(4cm)에 맞게 조절하도록 유도

#### 3. Cube → Target 이동 (잡은 후)
```python
if grasp_valid and prev_cube_to_target_dist is not None:
    cube_progress = prev_dist - current_dist
    reward += 15.0 * cube_progress
```

**의도**: 큐브를 목표로 옮기는 행동 강화

### B. Milestone Rewards (1회성)

| 마일스톤 | 조건 | 보상 | 비고 |
|---------|------|------|------|
| **REACH** | EE < 5cm from cube | +0.5 | 축하 메시지 수준 |
| **GRIP** | dist<3cm + gripper 3-5cm + z<1.5cm + 3 frames | **+50** | 엄격한 조건 |
| **PERFECT GRIP** | dist<2cm + z<0.8cm | **+100** | 매우 정확한 잡기 |
| **LIFT** | GRIP + cube z > 5cm | +15 | 최종 목표 |
| **SUCCESS** | cube to target < 5cm | +50 | 태스크 완료 |

### C. 보상 클리핑
```python
if reward < 90.0:  # 큰 이벤트 제외
    reward = np.clip(reward, -10.0, 10.0)
```

---

## 🤖 학습 전략

### PPO 알고리즘 설정
```yaml
알고리즘: PPO (Proximal Policy Optimization)
학습률: 3e-4
배치 크기: 64
N-steps: 2048
Epochs: 10
Entropy 계수: 0.01  # 탐색 강화
Clip range: 0.2
Target KL: 0.03
```

### 관측 공간 (28차원)
```python
[
  joint_positions (6),     # 관절 위치
  joint_velocities (6),    # 관절 속도
  ee_position (3),         # EE 위치 (world)
  ee_velocity (3),         # EE 속도
  cube_position (3),       # 큐브 위치 (world)
  target_position (3),     # 목표 위치 (world)
  ee_to_cube_vector (3),   # EE → 큐브 벡터
  cube_to_target_vector (3), # 큐브 → 목표 벡터
  gripper_width (1),       # 그리퍼 너비
  is_grasped (1),          # 잡음 여부
  dist_to_cube (1),        # EE-큐브 거리
  dist_cube_to_target (1), # 큐브-목표 거리
]
```

### 행동 공간 (7차원)
```python
[
  joint_1_velocity,  # -1 ~ +1
  joint_2_velocity,
  joint_3_velocity,
  joint_4_velocity,
  joint_5_velocity,
  joint_6_velocity,
  gripper_command,   # -1(닫기) ~ +1(열기)
]
```

### 네트워크 구조
```
MLP Policy:
  Input: 28-dim observation
  Hidden: [256, 256] (2 layers)
  Output: 
    - Actor: 7-dim action (mean)
    - Critic: 1-dim value
```

---

## 📈 예상 학습 진행

### Phase별 예상 결과

| Phase | Timesteps | 예상 행동 | REACH | GRIP | LIFT |
|-------|-----------|----------|-------|------|------|
| **초기 탐색** | 0-50K | 랜덤 행동, 그리퍼 실험 | 0% | 0% | 0% |
| **REACH 학습** | 50-100K | EE를 큐브 근처로 | 10-30% | 0% | 0% |
| **REACH 완성** | 100-150K | 안정적으로 큐브 접근 | 40-60% | 0-5% | 0% |
| **GRIP 학습** | 150-200K | 정확한 위치 + 그리퍼 조절 | 60-80% | 10-25% | 0% |
| **GRIP 완성** | 200-250K | 강화된 조건 달성 | 80-90% | 30-50% | 0-10% |
| **LIFT 시작** | 250-300K | 큐브 들어올리기 시도 | 90%+ | 50-70% | 15-30% |

### 체크포인트 계획
```yaml
저장 주기: 5,000 timesteps
중요 체크포인트:
  - 100K: REACH 학습 완료 확인
  - 200K: GRIP 학습 진행 확인
  - 300K: LIFT 시작 여부 확인
```

### 성공 기준
```yaml
최소 성공 기준:
  - REACH: 60% 이상
  - GRIP: 30% 이상 (강화된 조건)
  - LIFT: 10% 이상
  
목표 성공 기준:
  - REACH: 80% 이상
  - GRIP: 50% 이상
  - LIFT: 20% 이상
```

---

## ⚠️ 검증 요청사항

### 1. GRIP 조건 적정성
```yaml
질문:
  - 3cm 거리는 너무 엄격한가?
  - 그리퍼 3.0-5.0cm 범위가 적절한가?
  - Z축 정렬 1.5cm가 합리적인가?

대안 제안:
  - Option A: 거리 3.5cm로 완화
  - Option B: 그리퍼 2.8-5.2cm로 완화
  - Option C: Z축 1.8cm로 완화
```

### 2. 보상 균형
```yaml
질문:
  - Dense Reward (+20 * progress)가 너무 강한가?
  - Milestone Reward (+50 GRIP)가 적절한가?
  - Width penalty (-5 * error²)가 효과적인가?

대안 제안:
  - Dense 보상 계수 조정 (20 → 15)
  - GRIP 보상 증가 (+50 → +80)
  - Width penalty 강화 (-5 → -8)
```

### 3. 학습 안정성
```yaml
우려사항:
  - 조건 강화로 초기 학습 너무 느릴 수 있음
  - Exploration 부족으로 local minimum 가능
  - 300K로 LIFT 달성 못할 수도 있음

제안:
  - Curriculum learning 도입 (점진적 조건 강화)
  - Entropy 계수 증가 (0.01 → 0.02)
  - Timesteps 확장 (300K → 500K)
```

### 4. 물리적 제약
```yaml
질문:
  - 실제 RoArm-M3 스펙에서 3cm 거리 접근 가능한가?
  - 그리퍼 3-5cm 제어가 하드웨어적으로 가능한가?
  - Z축 정렬 1.5cm가 실제 센서 정밀도로 달성 가능한가?

검증 필요:
  - 실제 로봇 워크스페이스 확인
  - 그리퍼 제어 분해능 확인
  - 센서/제어 노이즈 수준 확인
```

---

## 📊 현재 학습 상태

```yaml
날짜: 2025-10-24 20:50
Timesteps: 5,000 / 300,000 (1.7%)
FPS: 384
예상 완료 시간: 12-15분

마일스톤 현황:
  REACH: 0% (예상대로, 초기 탐색)
  GRIP: 0% (예상대로, 조건 강화)
  LIFT: 0%

메트릭:
  평균 보상: 2,310
  Explained Variance: 0.032 (초기)
  Value Loss: 0.186 (높음, 초기)
```

---

## 🔄 다음 단계

### 학습 완료 후
1. GUI 재생으로 실제 행동 확인
2. GRIP 달성 시 큐브 실제 잡히는지 확인
3. LIFT 시도 여부 확인

### 추가 실험
```yaml
실험 1: Curriculum Learning
  - Phase 1: dist<5cm (쉬움)
  - Phase 2: dist<4cm (중간)
  - Phase 3: dist<3cm (어려움)
  
실험 2: 보상 튜닝
  - GRIP 보상 증가 (+50 → +100)
  - Dense 보상 계수 조정
  
실험 3: 학습 시간 연장
  - 300K → 500K
  - 더 긴 학습으로 LIFT 달성
```

---

## 📝 참고사항

### 이전 학습 결과 (v3.7.7)
```yaml
100K 학습:
  REACH: 40%
  GRIP: 0%
  LIFT: 0%
  문제: GRIP 조건 느슨함

300K 학습:
  REACH: 49%
  GRIP: 38%
  LIFT: 0%
  문제: 실제로 큐브 안 잡힘

500K 학습 (실패):
  - 실제로는 200K만 학습됨 (SB3 learn() 버그)
  - 300K 대비 개선 없음
```

### 관련 파일
```
환경: envs/roarm_pick_place_env.py
학습: scripts/rl/train_dense_reward.py
재생: scripts/rl/replay_roarm_gui.py
로그: logs/rl_training_curriculum/
```

---

## 🤔 AI 검증 요청 질문

**AI 전문가들에게 검증받고 싶은 핵심 질문들:**

1. **GRIP 조건 강화가 적절한가?**
   - 5cm → 3cm로 40% 줄인 것이 너무 극단적인가?
   - 단계적 강화 (5cm → 4cm → 3cm)가 더 효과적인가?

2. **보상 구조의 균형이 맞는가?**
   - Dense Reward 20x vs Sparse GRIP +50
   - 비율이 적절한가? Dense가 너무 강한가?

3. **학습 시간 300K가 충분한가?**
   - 강화된 조건에서 LIFT 달성 가능한가?
   - 500K나 1M까지 필요한가?

4. **Curriculum Learning 도입해야 하는가?**
   - 점진적 조건 강화가 더 효율적인가?
   - 아니면 한 번에 강화된 조건으로 학습하는 게 나은가?

5. **실제 로봇 전이 가능성은?**
   - 시뮬레이션에서 3cm 접근이 실제 로봇에서도 가능한가?
   - Sim-to-Real gap 해결 방안은?

---

**작성자**: GitHub Copilot  
**검토 요청**: 여러 AI 시스템에게 교차 검증 요청  
**업데이트 예정**: 학습 완료 후 결과 추가
