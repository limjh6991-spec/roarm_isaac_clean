# ê°•í™”í•™ìŠµ ë¦¬ì†ŒìŠ¤ ì¸ë±ìŠ¤
> ìˆ˜ì§‘ ë‚ ì§œ: 2025-10-19  
> í”„ë¡œì íŠ¸: RoArm-M3 Isaac Sim RL

## ğŸ“š ë¡œì»¬ ë¦¬ì†ŒìŠ¤

### 1. ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤
**íŒŒì¼**: `rl_best_practices.md`  
**ë‚´ìš©**:
- ë³´ìƒ ì„¤ê³„ (Reward Shaping)
- PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ê°€ì´ë“œ
- í•™ìŠµ ë””ë²„ê¹… ë°©ë²•
- í™˜ê²½ ì„¤ê³„ ì›ì¹™
- ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í•´ê²°ë²•

**ì£¼ìš” ë°œê²¬ì‚¬í•­**:
- âœ… RoArm-M3 ë³´ìƒ ìŠ¤ì¼€ì¼ ë¬¸ì œ ì§„ë‹¨
- âœ… ë³´ìƒ ë²”ìœ„: -50 ~ +50 ê¶Œì¥
- âœ… Dense Reward êµ¬ì„±: 3-5ê°œ ìš”ì†Œ

---

### 2. PPO ì•Œê³ ë¦¬ì¦˜ ì™„ì „ ê°€ì´ë“œ
**íŒŒì¼**: `ppo_algorithm_guide.md`  
**ë‚´ìš©**:
- PPO ì‘ë™ ì›ë¦¬ ìƒì„¸
- ìˆ˜ì‹ ë° ì½”ë“œ ì˜ˆì œ
- ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµ (SAC, TD3, A2C)
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒì„¸ ì„¤ëª…
- ì‹¤ì „ íŒ ë° ê³ ê¸‰ ê¸°ë²•

**í•µì‹¬ í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
```python
learning_rate: 3e-4 (1e-5 ~ 1e-3)
n_steps: 2048 (512 ~ 8192)
batch_size: 64 (32 ~ 512)
n_epochs: 10 (3 ~ 30)
gamma: 0.99 (0.9 ~ 0.9999)
clip_range: 0.2 (0.1 ~ 0.3)
```

---

### 3. ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸
**íŒŒì¼**: `rl_debugging_checklist.md`  
**ë‚´ìš©**:
- ë¬¸ì œ ì¦ìƒë³„ í•´ê²° ê°€ì´ë“œ
- 5ê°€ì§€ ì£¼ìš” ë¬¸ì œ ìœ í˜•
- ë””ë²„ê¹… ë„êµ¬ ì‚¬ìš©ë²•
- í•™ìŠµ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸
- ê³ ê¸‰ ë””ë²„ê¹… ê¸°ë²•

**ë¬¸ì œ ìœ í˜•**:
1. í•™ìŠµì´ ì „í˜€ ì•ˆ ë¨ (ë³´ìƒ flat)
2. í•™ìŠµì´ ë¶ˆì•ˆì • (ë³´ìƒ ì§„ë™)
3. ìˆ˜ë ´ì´ ëŠë¦¼
4. NaN/Inf ë°œìƒ
5. ê³¼ì í•©

---

## ğŸŒ ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤

### ê³µì‹ ë¬¸ì„œ
1. **Stable-Baselines3**: https://stable-baselines3.readthedocs.io/
   - RL Tips: `/guide/rl_tips.html`
   - PPO Docs: `/modules/ppo.html`
   - Examples: `/guide/examples.html`

2. **OpenAI Spinning Up**: https://spinningup.openai.com/
   - Key Concepts: `/spinningup/rl_intro.html`
   - PPO Algorithm: `/algorithms/ppo.html`
   - Exercises: `/spinningup/exercises.html`

3. **Ray RLlib**: https://docs.ray.io/en/latest/rllib/
   - Algorithms: `/rllib-algorithms.html`
   - Environments: `/rllib-env.html`

4. **HuggingFace Deep RL Course**: https://huggingface.co/deep-rl-course
   - Unit 1: Introduction to RL
   - Unit 2: Q-Learning
   - Unit 8: PPO

### í•µì‹¬ ë…¼ë¬¸
1. **PPO (2017)**: [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
   - ì €ì: Schulman et al.
   - í•µì‹¬: Clipped surrogate objective

2. **GAE (2016)**: [Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)
   - ì €ì: Schulman et al.
   - í•µì‹¬: Bias-variance tradeoff

3. **TRPO (2015)**: [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)
   - ì €ì: Schulman et al.
   - í•µì‹¬: Constrained optimization

4. **Reward Shaping (1999)**: [Policy Invariance Under Reward Transformations](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)
   - ì €ì: Ng, Harada, Russell
   - í•µì‹¬: Potential-based shaping

### ì½”ë“œ ì €ì¥ì†Œ
1. **Stable-Baselines3**: https://github.com/DLR-RM/stable-baselines3
2. **RL Baselines3 Zoo**: https://github.com/DLR-RM/rl-baselines3-zoo
3. **CleanRL**: https://github.com/vwxyzjn/cleanrl
4. **OpenAI Spinning Up**: https://github.com/openai/spinningup

### íŠœí† ë¦¬ì–¼
1. **PyTorch RL Tutorial**: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html
2. **HuggingFace Lunar Lander**: https://huggingface.co/blog/deep-rl-intro
3. **Stable-Baselines3 Getting Started**: https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html

---

## ğŸ¯ RoArm-M3 í”„ë¡œì íŠ¸ ì ìš©

### í˜„ì¬ ìƒí™©
- **í™˜ê²½**: RoArm-M3 Pick and Place (Isaac Sim 5.0.0)
- **ì•Œê³ ë¦¬ì¦˜**: PPO (Stable-Baselines3 2.7.0)
- **ë¬¸ì œ**: ë³´ìƒ ìŠ¤ì¼€ì¼ ê³¼ë„ (ìˆ˜ì²œ ë‹¨ìœ„)

### ìˆ˜ì§‘í•œ ì§€ì‹ ì ìš©
1. **ë³´ìƒ í•¨ìˆ˜ ì¬ì„¤ê³„** (`rl_best_practices.md` ì°¸ê³ )
   - í˜„ì¬: reach 5.0, move 10.0 (ê³¼ë„)
   - ê¶Œì¥: reach 0.5~1.0, move 1.0~2.0

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •** (`ppo_algorithm_guide.md` ì°¸ê³ )
   - learning_rate: 3e-4 â†’ 1e-4 (ì•ˆì •ì„±)
   - clip_range: 0.2 â†’ 0.1 (ì•ˆì •ì„±)
   - ent_coef: 0.0 â†’ 0.005 (exploration)

3. **ë””ë²„ê¹… ì²´ê³„í™”** (`rl_debugging_checklist.md` ì°¸ê³ )
   - Tensorboard í•„ìˆ˜ ì§€í‘œ ëª¨ë‹ˆí„°ë§
   - VecCheckNanìœ¼ë¡œ NaN ì¡°ê¸° ê°ì§€
   - Monitor CSV ë¶„ì„

---

## ğŸ“– í•™ìŠµ ê³„íš

### 1ë‹¨ê³„: ê¸°ì´ˆ ì´ë¡  (ì™„ë£Œ)
- [x] RL ê¸°ë³¸ ê°œë… (MDP, Policy, Value Function)
- [x] PPO ì•Œê³ ë¦¬ì¦˜ ì´í•´
- [x] ë³´ìƒ ì„¤ê³„ ì›ì¹™

### 2ë‹¨ê³„: ì‹¤ì „ ì ìš© (ì§„í–‰ ì¤‘)
- [x] RoArm-M3 í™˜ê²½ êµ¬í˜„
- [x] Dense Reward í•¨ìˆ˜ ì„¤ê³„
- [ ] ë³´ìƒ ìŠ¤ì¼€ì¼ ì¡°ì • ë° ì¬í•™ìŠµ
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- [ ] ì„±ê³µë¥  10% ì´ìƒ ë‹¬ì„±

### 3ë‹¨ê³„: ê³ ê¸‰ ê¸°ë²• (ì˜ˆì •)
- [ ] Curriculum Learning ë„ì…
- [ ] Multi-task Learning
- [ ] Sim-to-Real Transfer
- [ ] RoArm ì‹¤ì œ í•˜ë“œì›¨ì–´ ë°°í¬

---

## ğŸ”— ë¹ ë¥¸ ì°¸ì¡°

### ë³´ìƒ ìŠ¤ì¼€ì¼ ë¬¸ì œ?
â†’ `rl_best_practices.md` ì„¹ì…˜ "ë³´ìƒ ì„¤ê³„"

### í•™ìŠµì´ ì•ˆ ë¨?
â†’ `rl_debugging_checklist.md` ë¬¸ì œ 1

### PPO íŒŒë¼ë¯¸í„° ëª¨ë¥´ê² ìŒ?
â†’ `ppo_algorithm_guide.md` ì„¹ì…˜ "í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒì„¸"

### Tensorboard ë­˜ ë´ì•¼ í•¨?
â†’ `rl_debugging_checklist.md` ì„¹ì…˜ "ë””ë²„ê¹… ë„êµ¬"

---

## ğŸ“ ë©”ëª¨

### 2025-10-19
- ì˜¨ë¼ì¸ ë¦¬ì†ŒìŠ¤ 3ê°œ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘ ì™„ë£Œ
- ë¡œì»¬ì— 3ê°œ ê°€ì´ë“œ ë¬¸ì„œ ì‘ì„±
- RoArm-M3 ë³´ìƒ ìŠ¤ì¼€ì¼ ë¬¸ì œ ì§„ë‹¨
- ë‹¤ìŒ: ë³´ìƒ í•¨ìˆ˜ ìˆ˜ì • í›„ ì¬í•™ìŠµ

### í•µì‹¬ ë°œê²¬ì‚¬í•­
1. **ë³´ìƒ ìŠ¤ì¼€ì¼ì´ í•™ìŠµì˜ 90%**: ë‹¤ë¥¸ ëª¨ë“  ê²ƒë³´ë‹¤ ì¤‘ìš”!
2. **PPOëŠ” ì•ˆì •ì **: ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œë„ ì˜ ì‘ë™
3. **ë””ë²„ê¹…ì€ Tensorboard**: ë°˜ë“œì‹œ ì‚¬ìš©í•´ì•¼ í•¨
4. **ë³‘ë ¬ í™˜ê²½ í•„ìˆ˜**: í•™ìŠµ ì†ë„ 4-8ë°° í–¥ìƒ

