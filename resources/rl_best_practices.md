# ê°•í™”í•™ìŠµ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤ ê°€ì´ë“œ
> ìˆ˜ì§‘ ë‚ ì§œ: 2025-10-19  
> ì¶œì²˜: Stable-Baselines3, OpenAI Spinning Up, RLlib, HuggingFace

## ğŸ“Œ ëª©ì°¨
1. [ë³´ìƒ ì„¤ê³„ (Reward Shaping)](#ë³´ìƒ-ì„¤ê³„)
2. [PPO ì•Œê³ ë¦¬ì¦˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°](#ppo-í•˜ì´í¼íŒŒë¼ë¯¸í„°)
3. [í•™ìŠµ ë””ë²„ê¹…](#í•™ìŠµ-ë””ë²„ê¹…)
4. [í™˜ê²½ ì„¤ê³„](#í™˜ê²½-ì„¤ê³„)
5. [ì¼ë°˜ì ì¸ ì‹¤ìˆ˜ì™€ í•´ê²°ë²•](#ì¼ë°˜ì ì¸-ì‹¤ìˆ˜)

---

## ë³´ìƒ ì„¤ê³„ (Reward Shaping)

### âœ… í•µì‹¬ ì›ì¹™
1. **ë³´ìƒ ìŠ¤ì¼€ì¼ë§**: ë³´ìƒì„ -1 ~ +1 ë˜ëŠ” 0 ~ 1 ë²”ìœ„ë¡œ ì •ê·œí™”
2. **Dense vs Sparse**:
   - **Sparse**: ëª©í‘œ ë‹¬ì„± ì‹œì—ë§Œ ë³´ìƒ (í•™ìŠµ ì–´ë ¤ì›€, ì•ˆì •ì )
   - **Dense**: ë§¤ ìŠ¤í…ë§ˆë‹¤ ì¤‘ê°„ ë³´ìƒ (í•™ìŠµ ë¹ ë¦„, ë¶ˆì•ˆì • ê°€ëŠ¥)
3. **ë³´ìƒ í•¨ìˆ˜ êµ¬ì„±ìš”ì†Œ**:
   - **Progress Reward**: ëª©í‘œì— ê°€ê¹Œì›Œì§ˆìˆ˜ë¡ ì¦ê°€
   - **Success Reward**: ëª©í‘œ ë‹¬ì„± ì‹œ í° ë³´ìƒ
   - **Penalty**: ì‹¤íŒ¨/ì¶©ëŒ ì‹œ ê°ì 
   - **Time Penalty**: íš¨ìœ¨ì„± ìœ ë„ (ì‘ì€ ê°’)

### âš ï¸ RoArm-M3 Pick & Place ë¶„ì„

**ë¬¸ì œ**: ë³´ìƒì´ ìˆ˜ì²œ ë‹¨ìœ„ë¡œ í­ë°œ (+3719, -3625)

**ì›ì¸**:
```python
# ì˜ëª»ëœ ì˜ˆ (ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒì´ ë„ˆë¬´ í¼)
reach_reward = -ee_to_cube_dist * 5.0   # ê±°ë¦¬ 0.5m â†’ -2.5
move_reward = -cube_to_target_dist * 10.0  # ê±°ë¦¬ 1.0m â†’ -10
```

**í•´ê²°ì±…**:
```python
# âœ… ì˜¬ë°”ë¥¸ ìŠ¤ì¼€ì¼ (Stable-Baselines3 ê¶Œì¥)
reach_reward_scale: 0.5 ~ 1.0
grasp_reward: 5.0
lift_reward_scale: 1.0 ~ 2.0
move_reward_scale: 1.0 ~ 2.0
success_reward: 10.0 ~ 50.0

# ì˜ˆìƒ ë³´ìƒ ë²”ìœ„: -50 ~ +50
```

### ğŸ“ ë³´ìƒ ì •ê·œí™” ê³µì‹
```python
# ë°©ë²• 1: ê±°ë¦¬ ê¸°ë°˜ ë³´ìƒ í´ë¦¬í•‘
reward = np.clip(-distance * scale, -max_val, max_val)

# ë°©ë²• 2: ì§€ìˆ˜ ê°ì‡ 
reward = scale * np.exp(-distance / threshold)

# ë°©ë²• 3: Tanh ì •ê·œí™”
reward = scale * np.tanh(distance / threshold)
```

---

## PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°

### ğŸ¯ ê¸°ë³¸ ì„¤ì • (Stable-Baselines3 ê¶Œì¥)

```python
from stable_baselines3 import PPO

model = PPO(
    "MlpPolicy",
    env,
    # === í•µì‹¬ íŒŒë¼ë¯¸í„° ===
    learning_rate=3e-4,        # í•™ìŠµë¥  (3e-4 ~ 1e-3)
    n_steps=2048,              # ë°°ì¹˜ í¬ê¸° (2048 ~ 4096)
    batch_size=64,             # ë¯¸ë‹ˆë°°ì¹˜ (32 ~ 128)
    n_epochs=10,               # PPO ì—…ë°ì´íŠ¸ ë°˜ë³µ (10 ~ 20)
    gamma=0.99,                # í• ì¸ìœ¨ (0.99 ~ 0.995)
    gae_lambda=0.95,           # GAE ëŒë‹¤ (0.9 ~ 0.99)
    clip_range=0.2,            # PPO í´ë¦¬í•‘ (0.1 ~ 0.3)
    ent_coef=0.0,              # ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜ (0.0 ~ 0.01)
    
    # === ìµœì í™” ===
    max_grad_norm=0.5,         # ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘ (0.5 ~ 1.0)
    vf_coef=0.5,               # Value function ê³„ìˆ˜
    
    # === ë¡œê¹… ===
    verbose=1,
    tensorboard_log="logs/tensorboard/",
)
```

### ğŸ“Š í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê°€ì´ë“œ

| ë¬¸ì œ ì¦ìƒ | ì¡°ì • ë°©ë²• |
|---------|---------|
| í•™ìŠµ ì•ˆ ë¨ (ë³´ìƒ flat) | `learning_rate` â†‘ (1e-3), `n_steps` â†‘ (4096) |
| í•™ìŠµ ë¶ˆì•ˆì • (ë³´ìƒ ì§„ë™) | `learning_rate` â†“ (1e-4), `clip_range` â†“ (0.1) |
| ìˆ˜ë ´ ëŠë¦¼ | `n_epochs` â†‘ (20), `batch_size` â†‘ (128) |
| ê³¼ì í•© | `ent_coef` â†‘ (0.01), `gamma` â†“ (0.95) |

---

## í•™ìŠµ ë””ë²„ê¹…

### ğŸ” ì²´í¬ë¦¬ìŠ¤íŠ¸

1. **ë³´ìƒ ì¶”ì´ í™•ì¸**:
   ```python
   # Tensorboard ì‚¬ìš©
   tensorboard --logdir logs/tensorboard/
   
   # ë˜ëŠ” Monitor CSV íŒŒì‹±
   import pandas as pd
   df = pd.read_csv("logs/monitor.csv", skiprows=1)
   print(df['r'].describe())  # í‰ê· , ìµœì†Œ, ìµœëŒ€ í™•ì¸
   ```

2. **ë³´ìƒ ìŠ¤ì¼€ì¼ ì§„ë‹¨**:
   ```python
   # âœ… ì •ìƒ: -100 ~ +100 ë²”ìœ„
   # âš ï¸ ë¬¸ì œ: -10000 ~ +10000 (ìŠ¤ì¼€ì¼ ê³¼ë„)
   # âš ï¸ ë¬¸ì œ: -1 ~ 0 (í•™ìŠµ ì‹ í˜¸ ì•½í•¨)
   ```

3. **ì—í”¼ì†Œë“œ ê¸¸ì´ í™•ì¸**:
   ```python
   # ëª¨ë“  ì—í”¼ì†Œë“œê°€ íƒ€ì„ì•„ì›ƒ?
   # â†’ ë³´ìƒ í•¨ìˆ˜ ë¬¸ì œ ë˜ëŠ” ëª©í‘œê°€ ë„ˆë¬´ ì–´ë ¤ì›€
   
   # ì—í”¼ì†Œë“œê°€ ë„ˆë¬´ ì§§ìŒ?
   # â†’ Penaltyê°€ ë„ˆë¬´ ê°•í•¨
   ```

4. **Value Loss ëª¨ë‹ˆí„°ë§**:
   ```python
   # Value lossê°€ ê³„ì† ì¦ê°€?
   # â†’ ë³´ìƒ ìŠ¤ì¼€ì¼ ë¬¸ì œ ë˜ëŠ” í•™ìŠµë¥  ë„ˆë¬´ ë†’ìŒ
   
   # Policy lossê°€ 0ì— ìˆ˜ë ´?
   # â†’ í•™ìŠµì´ ë©ˆì¶¤, exploration ë¶€ì¡±
   ```

### ğŸ› ì¼ë°˜ì ì¸ ì—ëŸ¬

#### 1. **NaN/Inf ë°œìƒ**
```python
# í•´ê²°ì±…
from stable_baselines3.common.vec_env import VecCheckNan

env = VecCheckNan(env, raise_exception=True)

# ë˜ëŠ” ë³´ìƒ í´ë¦¬í•‘
reward = np.clip(reward, -10, 10)
```

#### 2. **í•™ìŠµì´ ì•ˆ ë¨**
```python
# ì²´í¬ ì‚¬í•­:
# 1. ë³´ìƒì´ í•­ìƒ 0 ë˜ëŠ” ìŒìˆ˜?
# 2. ê´€ì°° ê³µê°„ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìŒ?
# 3. í–‰ë™ì´ í™˜ê²½ì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ?

# ë””ë²„ê·¸ ë°©ë²•
env.reset()
for i in range(100):
    obs, reward, done, info = env.step(env.action_space.sample())
    print(f"Step {i}: reward={reward:.2f}, obs_range=({obs.min():.2f}, {obs.max():.2f})")
```

---

## í™˜ê²½ ì„¤ê³„

### âœ… ê´€ì°° ê³µê°„ (Observation Space)

```python
# ì¢‹ì€ ì˜ˆ: ì •ê·œí™”ëœ ê´€ì°°
obs = np.concatenate([
    joint_positions / np.pi,              # [-1, 1]
    joint_velocities / max_vel,          # [-1, 1]
    (ee_pos - center) / workspace_size,  # [-1, 1]
    cube_pos / workspace_size,           # [0, 1]
])

# ë‚˜ìœ ì˜ˆ: ìŠ¤ì¼€ì¼ì´ ë‹¤ë¥¸ ê°’ë“¤
obs = np.concatenate([
    joint_positions,      # [-3.14, 3.14]
    joint_velocities,     # [-10, 10]
    ee_pos,               # [0, 1]
    cube_pos,             # [0, 0.5]
])  # â†’ NNì´ í•™ìŠµí•˜ê¸° ì–´ë ¤ì›€!
```

### âœ… í–‰ë™ ê³µê°„ (Action Space)

```python
# ì—°ì† í–‰ë™ (-1, +1)
action_space = gym.spaces.Box(
    low=-1.0,
    high=1.0,
    shape=(num_joints,),
    dtype=np.float32
)

# í–‰ë™ì„ ì‹¤ì œ ì œì–´ê°’ìœ¼ë¡œ ë³€í™˜
target_positions = current_positions + actions * max_delta
```

### âœ… ì¢…ë£Œ ì¡°ê±´ (Termination)

```python
def _is_done(self):
    # ì„±ê³µ
    if distance_to_target < threshold:
        return True, "success"
    
    # ì‹¤íŒ¨
    if cube_fell_off or robot_out_of_bounds:
        return True, "failure"
    
    # íƒ€ì„ì•„ì›ƒ
    if self.steps >= max_steps:
        return True, "timeout"
    
    return False, None
```

---

## ì¼ë°˜ì ì¸ ì‹¤ìˆ˜

### âŒ ì‹¤ìˆ˜ 1: ë³´ìƒ í•¨ìˆ˜ ë„ˆë¬´ ë³µì¡
```python
# ë‚˜ìœ ì˜ˆ: 10ê°œ ì´ìƒì˜ ë³´ìƒ ìš”ì†Œ
reward = (
    reach_reward + grasp_reward + lift_reward + 
    move_reward + success_reward + stability_reward +
    smoothness_reward + energy_reward + ...
)  # â†’ í•™ìŠµ ë¶ˆì•ˆì •!

# âœ… ì¢‹ì€ ì˜ˆ: 3-5ê°œ ìš”ì†Œ
reward = reach_reward + grasp_reward + success_reward
```

### âŒ ì‹¤ìˆ˜ 2: ë³´ìƒ ìŠ¤ì¼€ì¼ ë¬´ì‹œ
```python
# ë‚˜ìœ ì˜ˆ
reward = -1000 * distance  # ë³´ìƒ -10000 ~ 0

# âœ… ì¢‹ì€ ì˜ˆ
reward = -np.clip(distance * 5.0, 0, 10)  # ë³´ìƒ -10 ~ 0
```

### âŒ ì‹¤ìˆ˜ 3: Curriculum Learning ì—†ì´ ì–´ë ¤ìš´ ëª©í‘œ
```python
# ë‚˜ìœ ì˜ˆ: ì²˜ìŒë¶€í„° ì–´ë ¤ìš´ ëª©í‘œ
target_pos = [0.5, 0.5, 0.5]  # ë¨¼ ê±°ë¦¬

# âœ… ì¢‹ì€ ì˜ˆ: ì ì§„ì ìœ¼ë¡œ ì–´ë ¤ì›Œì§
difficulty = min(1.0, episode_count / 10000)
target_pos = initial_pos + direction * 0.1 * difficulty
```

---

## ğŸ“š ì¶”ê°€ ë¦¬ì†ŒìŠ¤

### í•„ë… ìë£Œ
1. **Stable-Baselines3 RL Tips**: https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html
2. **OpenAI Spinning Up**: https://spinningup.openai.com/
3. **HuggingFace Deep RL Course**: https://huggingface.co/deep-rl-course

### í•µì‹¬ ë…¼ë¬¸
1. **PPO**: [Proximal Policy Optimization](https://arxiv.org/abs/1707.06347)
2. **Reward Shaping**: [Policy Invariance Under Reward Transformations](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09/readings/NgHaradaRussell-shaping-ICML1999.pdf)
3. **Curriculum Learning**: [Automatic Curriculum Learning](https://arxiv.org/abs/2003.04960)

---

## ğŸ¯ RoArm-M3 ì ìš© ìš”ì•½

### í˜„ì¬ ë¬¸ì œ
- âœ… Dense Reward êµ¬í˜„ ì™„ë£Œ
- âŒ ë³´ìƒ ìŠ¤ì¼€ì¼ ê³¼ë„ (ìˆ˜ì²œ ë‹¨ìœ„)
- âŒ íƒ€ì„ì•„ì›ƒ 100%
- âŒ ì„±ê³µë¥  0%

### í•´ê²° ë°©ì•ˆ
1. **ë³´ìƒ ìŠ¤ì¼€ì¼ 1/5 ~ 1/10 ì¶•ì†Œ**
2. **ê´€ì°° ê³µê°„ ì •ê·œí™” í™•ì¸**
3. **í•™ìŠµë¥  ì¡°ì •** (3e-4 â†’ 1e-4)
4. **ë” ê¸´ í•™ìŠµ** (150K ~ 200K steps)
5. **Curriculum Learning ë„ì…** (íë¸Œ ìœ„ì¹˜ ì ì§„ì  ì¦ê°€)

