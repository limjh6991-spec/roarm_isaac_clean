# ê°•í™”í•™ìŠµ ë””ë²„ê¹… ì²´í¬ë¦¬ìŠ¤íŠ¸
> ì¶œì²˜: Stable-Baselines3, Spinning Up, ì‹¤ì „ ê²½í—˜  
> ë‚ ì§œ: 2025-10-19

## ğŸ” ë¬¸ì œ ì¦ìƒë³„ í•´ê²° ê°€ì´ë“œ

### ë¬¸ì œ 1: í•™ìŠµì´ ì „í˜€ ì•ˆ ë¨ (ë³´ìƒ flat)

#### ì¦ìƒ
```
ep_rew_mean: -500 â†’ -500 â†’ -500 (ë³€í™” ì—†ìŒ)
ep_len_mean: 600 â†’ 600 â†’ 600 (íƒ€ì„ì•„ì›ƒ)
```

#### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] **ë³´ìƒ í•¨ìˆ˜ ê²€ì¦**
  ```python
  # í…ŒìŠ¤íŠ¸: ëœë¤ í–‰ë™ìœ¼ë¡œ ë³´ìƒ í™•ì¸
  env.reset()
  rewards = []
  for _ in range(1000):
      action = env.action_space.sample()
      obs, reward, done, info = env.step(action)
      rewards.append(reward)
  
  print(f"Min: {min(rewards)}, Max: {max(rewards)}, Mean: {np.mean(rewards)}")
  # âœ… ì •ìƒ: ë³´ìƒì´ ë³€í™”í•¨ (-10 ~ +10)
  # âŒ ë¬¸ì œ: ë³´ìƒì´ í•­ìƒ 0 ë˜ëŠ” ìŒìˆ˜
  ```

- [ ] **ê´€ì°° ê³µê°„ í™•ì¸**
  ```python
  # ê´€ì°°ê°’ì´ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì§€ ì•Šì€ê°€?
  obs = env.reset()
  print(f"Obs range: [{obs.min():.2f}, {obs.max():.2f}]")
  # âœ… ì •ìƒ: [-1, 1] ë˜ëŠ” [0, 1]
  # âŒ ë¬¸ì œ: [-1000, 1000] (ì •ê·œí™” í•„ìš”)
  ```

- [ ] **í–‰ë™ì´ í™˜ê²½ì— ì˜í–¥**
  ```python
  # ê°™ì€ í–‰ë™ ë°˜ë³µ
  obs1 = env.reset()
  for _ in range(10):
      obs2, _, _, _ = env.step(np.ones(env.action_space.shape))
  
  # obs1ê³¼ obs2ê°€ ë‹¤ë¥¸ê°€?
  print(f"Obs changed: {not np.allclose(obs1, obs2)}")
  # âœ… ì •ìƒ: True
  # âŒ ë¬¸ì œ: False (í™˜ê²½ì´ ë³€í•˜ì§€ ì•ŠìŒ)
  ```

#### í•´ê²° ë°©ë²•
1. **ë³´ìƒ í•¨ìˆ˜ ì¬ì„¤ê³„** (ê°€ì¥ ì¤‘ìš”!)
2. **ê´€ì°° ê³µê°„ ì •ê·œí™”**
3. `learning_rate` ì¦ê°€ (3e-4 â†’ 1e-3)
4. `n_steps` ì¦ê°€ (2048 â†’ 4096)

---

### ë¬¸ì œ 2: í•™ìŠµì´ ë¶ˆì•ˆì • (ë³´ìƒ ì§„ë™)

#### ì¦ìƒ
```
ep_rew_mean: -100 â†’ +200 â†’ -50 â†’ +300 â†’ -200 (ì‹¬í•˜ê²Œ ì§„ë™)
value_loss: 1000 â†’ 5000 â†’ 500 (ë¶ˆì•ˆì •)
```

#### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] **ë³´ìƒ ìŠ¤ì¼€ì¼ í™•ì¸**
  ```python
  # ë³´ìƒì´ ìˆ˜ì²œ ë‹¨ìœ„?
  print(f"Reward range: [{rewards.min()}, {rewards.max()}]")
  # âŒ ë¬¸ì œ: [-5000, +3000]
  # âœ… í•´ê²°: ë³´ìƒ ìŠ¤ì¼€ì¼ 1/10 ë˜ëŠ” 1/100
  ```

- [ ] **approx_kl í™•ì¸**
  ```python
  # Tensorboardì—ì„œ í™•ì¸
  # approx_kl > 0.1 â†’ ì—…ë°ì´íŠ¸ê°€ ë„ˆë¬´ í¼
  # approx_kl < 0.001 â†’ ì—…ë°ì´íŠ¸ê°€ ë„ˆë¬´ ì‘ìŒ
  # âœ… ì •ìƒ: 0.01 ~ 0.05
  ```

- [ ] **clip_fraction í™•ì¸**
  ```python
  # clip_fraction > 0.5 â†’ clipping ë„ˆë¬´ ìì£¼ ë°œìƒ
  # âœ… ì •ìƒ: 0.1 ~ 0.3
  ```

#### í•´ê²° ë°©ë²•
1. **ë³´ìƒ ì •ê·œí™”**
   ```python
   from stable_baselines3.common.vec_env import VecNormalize
   env = VecNormalize(env, norm_reward=True)
   ```

2. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •**
   - `learning_rate` ê°ì†Œ (3e-4 â†’ 1e-4)
   - `clip_range` ê°ì†Œ (0.2 â†’ 0.1)
   - `max_grad_norm` ê°ì†Œ (0.5 â†’ 0.3)

3. **ë°°ì¹˜ í¬ê¸° ì¦ê°€**
   - `n_steps` ì¦ê°€ (2048 â†’ 4096)
   - `batch_size` ì¦ê°€ (64 â†’ 128)

---

### ë¬¸ì œ 3: ìˆ˜ë ´ì´ ëŠë¦¼

#### ì¦ìƒ
```
100K steps: ep_rew_mean = -200
200K steps: ep_rew_mean = -180
300K steps: ep_rew_mean = -170 (ë§¤ìš° ëŠë¦° ê°œì„ )
```

#### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] **Policy loss í™•ì¸**
  ```python
  # policy_loss â‰ˆ 0?
  # â†’ í•™ìŠµì´ ë©ˆì¶¤, exploration ë¶€ì¡±
  ```

- [ ] **Entropy í™•ì¸**
  ```python
  # entropyê°€ ë„ˆë¬´ ì‘ìŒ?
  # â†’ ì •ì±…ì´ ë„ˆë¬´ ê²°ì •ì  (Exploration ë¶€ì¡±)
  ```

#### í•´ê²° ë°©ë²•
1. **Exploration ì¦ê°€**
   ```python
   ent_coef = 0.01  # ê¸°ë³¸ 0.0 â†’ 0.01
   ```

2. **í•™ìŠµë¥  ì¦ê°€**
   ```python
   learning_rate = 1e-3  # ê¸°ë³¸ 3e-4 â†’ 1e-3
   ```

3. **ì—…ë°ì´íŠ¸ íšŸìˆ˜ ì¦ê°€**
   ```python
   n_epochs = 20  # ê¸°ë³¸ 10 â†’ 20
   ```

4. **Curriculum Learning**
   ```python
   # ì‰¬ìš´ ëª©í‘œë¶€í„° ì‹œì‘
   target_distance = 0.1 + progress * 0.4
   ```

---

### ë¬¸ì œ 4: NaN ë˜ëŠ” Inf ë°œìƒ

#### ì¦ìƒ
```
RuntimeError: CUDA error: device-side assert triggered
value_loss: nan
policy_loss: inf
```

#### ì²´í¬ë¦¬ìŠ¤íŠ¸
- [ ] **ë³´ìƒ ë²”ìœ„ í™•ì¸**
  ```python
  assert not np.isnan(reward).any()
  assert not np.isinf(reward).any()
  ```

- [ ] **ê´€ì°° ë²”ìœ„ í™•ì¸**
  ```python
  assert not np.isnan(obs).any()
  assert not np.isinf(obs).any()
  ```

#### í•´ê²° ë°©ë²•
1. **VecCheckNan ì‚¬ìš©**
   ```python
   from stable_baselines3.common.vec_env import VecCheckNan
   env = VecCheckNan(env, raise_exception=True)
   ```

2. **ë³´ìƒ í´ë¦¬í•‘**
   ```python
   reward = np.clip(reward, -10, 10)
   ```

3. **ê·¸ë˜ë””ì–¸íŠ¸ í´ë¦¬í•‘**
   ```python
   max_grad_norm = 0.5  # ê¸°ë³¸ê°’ ìœ ì§€
   ```

4. **í•™ìŠµë¥  ê°ì†Œ**
   ```python
   learning_rate = 1e-5  # ë§¤ìš° ì‘ê²Œ
   ```

---

### ë¬¸ì œ 5: ê³¼ì í•© (Training ì¢‹ìŒ, Evaluation ë‚˜ì¨)

#### ì¦ìƒ
```
Training: ep_rew_mean = +200
Evaluation: ep_rew_mean = -100
```

#### í•´ê²° ë°©ë²•
1. **Regularization**
   ```python
   ent_coef = 0.01  # Entropy ì¦ê°€
   ```

2. **Gamma ê°ì†Œ**
   ```python
   gamma = 0.95  # 0.99 â†’ 0.95
   ```

3. **í™˜ê²½ ëœë¤í™”**
   ```python
   # ì´ˆê¸° ìƒíƒœ ëœë¤í™”
   cube_pos += np.random.uniform(-0.05, 0.05, size=3)
   ```

---

## ğŸ› ï¸ ë””ë²„ê¹… ë„êµ¬

### 1. Tensorboard
```bash
# ì‹¤í–‰
tensorboard --logdir logs/tensorboard/

# í™•ì¸ í•­ëª©
# - ep_rew_mean: ì¦ê°€?
# - ep_len_mean: ê°ì†Œ?
# - value_loss: ê°ì†Œ?
# - policy_loss: ì•ˆì •?
# - approx_kl: 0.01 ~ 0.05?
```

### 2. Monitor CSV ë¶„ì„
```python
import pandas as pd
import matplotlib.pyplot as plt

df = pd.read_csv("logs/monitor.csv", skiprows=1)
df.columns = ['r', 'l', 't']

# ë³´ìƒ ì¶”ì´
df['r'].rolling(100).mean().plot(title="Reward (100-episode MA)")
plt.show()

# í†µê³„
print(df['r'].describe())
```

### 3. í”„ë¡œíŒŒì¼ë§
```python
import cProfile
import pstats

# í•™ìŠµ í”„ë¡œíŒŒì¼ë§
profiler = cProfile.Profile()
profiler.enable()

model.learn(total_timesteps=10000)

profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumtime')
stats.print_stats(20)
```

---

## ğŸ“‹ í•™ìŠµ ì „ ì²´í¬ë¦¬ìŠ¤íŠ¸

### âœ… í™˜ê²½ ê²€ì¦
- [ ] ë³´ìƒ í•¨ìˆ˜ê°€ ì˜ë¯¸ ìˆëŠ”ê°€?
- [ ] ê´€ì°° ê³µê°„ì´ ì •ê·œí™”ë˜ì—ˆëŠ”ê°€?
- [ ] í–‰ë™ì´ í™˜ê²½ì— ì˜í–¥ì„ ì£¼ëŠ”ê°€?
- [ ] ì¢…ë£Œ ì¡°ê±´ì´ ëª…í™•í•œê°€?
- [ ] ëœë¤ í–‰ë™ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í–ˆëŠ”ê°€?

### âœ… í•˜ì´í¼íŒŒë¼ë¯¸í„°
- [ ] `learning_rate`: 1e-5 ~ 1e-3
- [ ] `n_steps`: 512 ~ 8192
- [ ] `batch_size` < `n_steps`
- [ ] `clip_range`: 0.1 ~ 0.3
- [ ] `max_grad_norm`: 0.3 ~ 1.0

### âœ… ì¸í”„ë¼
- [ ] GPU ì‚¬ìš© ê°€ëŠ¥?
- [ ] ë³‘ë ¬ í™˜ê²½ ì‚¬ìš©?
- [ ] Tensorboard ì„¤ì •?
- [ ] ì²´í¬í¬ì¸íŠ¸ ì €ì¥?

---

## ğŸ”¬ ê³ ê¸‰ ë””ë²„ê¹…

### 1. Policy í–‰ë™ ì‹œê°í™”
```python
# í•™ìŠµëœ ì •ì±… í…ŒìŠ¤íŠ¸
env = gym.make("CartPole-v1")
obs = env.reset()

for _ in range(1000):
    action, _states = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    env.render()
    if done:
        obs = env.reset()
```

### 2. Value Function ê²€ì¦
```python
# Value functionì´ ì˜¬ë°”ë¥¸ê°€?
obs = env.reset()
value = model.policy.predict_values(obs)
print(f"Predicted value: {value}")

# ì‹¤ì œ returnê³¼ ë¹„êµ
actual_return = 0
gamma = 0.99
for t in range(100):
    action, _ = model.predict(obs)
    obs, reward, done, info = env.step(action)
    actual_return += reward * (gamma ** t)
    if done:
        break

print(f"Actual return: {actual_return}")
# Valueì™€ returnì´ ë¹„ìŠ·í•œê°€?
```

### 3. Advantage ë¶„ì„
```python
# Advantage ë¶„í¬ í™•ì¸
from stable_baselines3.common.evaluation import evaluate_policy

mean_reward, std_reward = evaluate_policy(
    model, env, n_eval_episodes=100
)
print(f"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}")
```

---

## ğŸ“š ì°¸ê³  ë¦¬ì†ŒìŠ¤

1. **Stable-Baselines3 ë””ë²„ê¹…**: https://stable-baselines3.readthedocs.io/en/master/guide/checking_nan.html
2. **RL ë””ë²„ê¹… ê°€ì´ë“œ**: https://andyljones.com/posts/rl-debugging.html
3. **OpenAI Spinning Up**: https://spinningup.openai.com/en/latest/spinningup/spinningup.html

