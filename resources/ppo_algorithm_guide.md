# PPO (Proximal Policy Optimization) ì™„ì „ ê°€ì´ë“œ
> ì¶œì²˜: OpenAI Spinning Up, Stable-Baselines3  
> ë‚ ì§œ: 2025-10-19

## ğŸ“Œ ëª©ì°¨
1. [PPOë€ ë¬´ì—‡ì¸ê°€?](#ppoë€-ë¬´ì—‡ì¸ê°€)
2. [PPO ì•Œê³ ë¦¬ì¦˜ ì‘ë™ ì›ë¦¬](#ì‘ë™-ì›ë¦¬)
3. [PPO vs ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜](#ppo-vs-ë‹¤ë¥¸-ì•Œê³ ë¦¬ì¦˜)
4. [PPO í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒì„¸](#í•˜ì´í¼íŒŒë¼ë¯¸í„°-ìƒì„¸)
5. [ì‹¤ì „ íŒ](#ì‹¤ì „-íŒ)

---

## PPOë€ ë¬´ì—‡ì¸ê°€?

**Proximal Policy Optimization (PPO)**ëŠ” 2017ë…„ OpenAIê°€ ë°œí‘œí•œ On-Policy ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.

### ğŸ¯ í•µì‹¬ ì•„ì´ë””ì–´
- **Policy Gradient** ë°©ë²•ì˜ ê°œì„ íŒ
- **Trust Region** ê°œë… ë„ì… (í° ì—…ë°ì´íŠ¸ ë°©ì§€)
- **Clipping** ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ

### âœ… ì¥ì 
1. **ì•ˆì •ì **: TRPOë³´ë‹¤ êµ¬í˜„ ê°„ë‹¨, ë¹„ìŠ·í•œ ì„±ëŠ¥
2. **ë²”ìš©ì **: ì—°ì†/ì´ì‚° í–‰ë™ ëª¨ë‘ ì‚¬ìš© ê°€ëŠ¥
3. **ìƒ˜í”Œ íš¨ìœ¨ì **: On-Policy ì¤‘ì—ì„œëŠ” ìš°ìˆ˜
4. **í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ ë‚®ìŒ**: ëŒ€ë¶€ë¶„ì˜ í™˜ê²½ì—ì„œ ê¸°ë³¸ ì„¤ì •ìœ¼ë¡œ ì‘ë™

### âŒ ë‹¨ì 
1. **ìƒ˜í”Œ ë¹„íš¨ìœ¨ì **: Off-Policy (SAC, TD3) ëŒ€ë¹„
2. **ë©”ëª¨ë¦¬ ì‚¬ìš©**: í° ë°°ì¹˜ í•„ìš”
3. **ë³‘ë ¬í™” í•„ìˆ˜**: ë¹ ë¥¸ í•™ìŠµì„ ìœ„í•´

---

## ì‘ë™ ì›ë¦¬

### 1. ë°ì´í„° ìˆ˜ì§‘ (Rollout)
```python
# n_steps ë™ì•ˆ í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©
for step in range(n_steps):
    action = policy.predict(observation)
    next_obs, reward, done, info = env.step(action)
    buffer.store(obs, action, reward, done, value)
    obs = next_obs
```

### 2. Advantage ê³„ì‚° (GAE)
```python
# Generalized Advantage Estimation
advantage = 0
for t in reversed(range(n_steps)):
    delta = reward[t] + gamma * value[t+1] - value[t]
    advantage[t] = delta + gamma * gae_lambda * advantage[t+1]

# ì •ê·œí™”
advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)
```

### 3. PPO ì†ì‹¤ í•¨ìˆ˜
```python
# Policy Loss (Clipped Surrogate Objective)
ratio = torch.exp(new_log_prob - old_log_prob)
surr1 = ratio * advantage
surr2 = torch.clamp(ratio, 1 - clip_range, 1 + clip_range) * advantage
policy_loss = -torch.min(surr1, surr2).mean()

# Value Loss
value_loss = F.mse_loss(value_pred, returns)

# Entropy Loss (Exploration)
entropy_loss = -entropy.mean()

# Total Loss
total_loss = policy_loss + vf_coef * value_loss + ent_coef * entropy_loss
```

### 4. ì—…ë°ì´íŠ¸ ë°˜ë³µ
```python
# n_epochs ë™ì•ˆ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë°˜ë³µ í•™ìŠµ
for epoch in range(n_epochs):
    for batch in get_minibatches(buffer, batch_size):
        optimizer.zero_grad()
        loss = compute_loss(batch)
        loss.backward()
        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)
        optimizer.step()
```

---

## PPO vs ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜

| ì•Œê³ ë¦¬ì¦˜ | íƒ€ì… | ìƒ˜í”Œ íš¨ìœ¨ | ì•ˆì •ì„± | ì ìš© |
|---------|-----|----------|--------|------|
| **PPO** | On-Policy | â­â­â­ | â­â­â­â­â­ | ë¡œë´‡, ê²Œì„, ì œì–´ |
| **SAC** | Off-Policy | â­â­â­â­â­ | â­â­â­â­ | ì—°ì† ì œì–´ |
| **TD3** | Off-Policy | â­â­â­â­â­ | â­â­â­â­ | ì—°ì† ì œì–´ |
| **A2C** | On-Policy | â­â­ | â­â­â­ | ë¹ ë¥¸ í”„ë¡œí† íƒ€ì… |
| **DQN** | Off-Policy | â­â­â­ | â­â­ | ì´ì‚° í–‰ë™ |

### ğŸ¤” ì–¸ì œ PPOë¥¼ ì‚¬ìš©í• ê¹Œ?
- âœ… **ì•ˆì •ì ì¸ í•™ìŠµ**ì´ ì¤‘ìš”í•  ë•Œ
- âœ… **ë¡œë´‡ ì œì–´** ê°™ì€ ì—°ì† í–‰ë™
- âœ… **ì²˜ìŒ ì‹œë„**í•˜ëŠ” í™˜ê²½ (ê¸°ë³¸ ì„ íƒ)
- âŒ ìƒ˜í”Œ íš¨ìœ¨ì´ ë§¤ìš° ì¤‘ìš”í•  ë•Œ (SAC ì‚¬ìš©)

---

## í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒì„¸

### ğŸ›ï¸ í•µì‹¬ íŒŒë¼ë¯¸í„°

#### 1. `learning_rate` (í•™ìŠµë¥ )
```python
# ê¸°ë³¸: 3e-4
# ë²”ìœ„: 1e-5 ~ 1e-3

# ë„ˆë¬´ ë†’ìœ¼ë©´: í•™ìŠµ ë¶ˆì•ˆì •, NaN ë°œìƒ
# ë„ˆë¬´ ë‚®ìœ¼ë©´: í•™ìŠµ ëŠë¦¼, ìˆ˜ë ´ ì•ˆ í•¨

# ìŠ¤ì¼€ì¤„ë§ ì˜ˆì œ
from stable_baselines3.common.callbacks import LinearSchedule
learning_rate = LinearSchedule(3e-4, 1e-5).value
```

#### 2. `n_steps` (ë°°ì¹˜ í¬ê¸°)
```python
# ê¸°ë³¸: 2048
# ë²”ìœ„: 512 ~ 8192

# ì‘ìœ¼ë©´: í•™ìŠµ ë¹ ë¦„, ë¶ˆì•ˆì •
# í¬ë©´: í•™ìŠµ ì•ˆì •, ë©”ëª¨ë¦¬ ë§ì´ ì‚¬ìš©

# ê³„ì‚° ê³µì‹
total_samples_per_update = n_steps * num_envs
# ì˜ˆ: 2048 * 4 = 8192 samples
```

#### 3. `batch_size` (ë¯¸ë‹ˆë°°ì¹˜)
```python
# ê¸°ë³¸: 64
# ë²”ìœ„: 32 ~ 512

# ê·œì¹™: batch_size < n_steps
# ì¼ë°˜ì : batch_size = n_steps / {8, 16, 32}

# ì˜ˆ:
n_steps = 2048
batch_size = 64  # 2048 / 32 = 64
```

#### 4. `n_epochs` (ì—…ë°ì´íŠ¸ ë°˜ë³µ)
```python
# ê¸°ë³¸: 10
# ë²”ìœ„: 3 ~ 30

# ì ìœ¼ë©´: ìƒ˜í”Œ ë¹„íš¨ìœ¨
# ë§ìœ¼ë©´: ê³¼ì í•© ìœ„í—˜

# ì ì‘ ì˜ˆì œ
if value_loss < threshold:
    n_epochs = 15  # ë” ë§ì´ í•™ìŠµ
```

#### 5. `gamma` (í• ì¸ìœ¨)
```python
# ê¸°ë³¸: 0.99
# ë²”ìœ„: 0.9 ~ 0.9999

# ë‚®ìœ¼ë©´: ë‹¨ê¸° ë³´ìƒ ì¤‘ì‹œ
# ë†’ìœ¼ë©´: ì¥ê¸° ë³´ìƒ ì¤‘ì‹œ

# ì—í”¼ì†Œë“œ ê¸¸ì´ì— ë”°ë¼ ì¡°ì •
# ì§§ì€ ì—í”¼ì†Œë“œ: gamma = 0.95
# ê¸´ ì—í”¼ì†Œë“œ: gamma = 0.99
```

#### 6. `gae_lambda` (GAE ëŒë‹¤)
```python
# ê¸°ë³¸: 0.95
# ë²”ìœ„: 0.9 ~ 0.99

# ë‚®ìœ¼ë©´: í¸í–¥ ë†’ìŒ, ë¶„ì‚° ë‚®ìŒ
# ë†’ìœ¼ë©´: í¸í–¥ ë‚®ìŒ, ë¶„ì‚° ë†’ìŒ

# ì¼ë°˜ì : gammaì™€ ë¹„ìŠ·í•œ ê°’
gae_lambda = 0.95
gamma = 0.99
```

#### 7. `clip_range` (í´ë¦¬í•‘ ë²”ìœ„)
```python
# ê¸°ë³¸: 0.2
# ë²”ìœ„: 0.1 ~ 0.3

# ì‘ìœ¼ë©´: ì•ˆì •, ëŠë¦¼
# í¬ë©´: ë¹ ë¦„, ë¶ˆì•ˆì •

# ì ì‘ì  í´ë¦¬í•‘
clip_range = schedule(initial=0.2, final=0.1)
```

#### 8. `ent_coef` (ì—”íŠ¸ë¡œí”¼ ê³„ìˆ˜)
```python
# ê¸°ë³¸: 0.0
# ë²”ìœ„: 0.0 ~ 0.01

# 0ì´ë©´: Exploration ì•½í•¨
# ë†’ìœ¼ë©´: ëœë¤ í–‰ë™ ì¦ê°€

# í•™ìŠµ ì´ˆê¸°: 0.01 (íƒìƒ‰)
# í•™ìŠµ í›„ê¸°: 0.0 (Exploitation)
```

---

## ì‹¤ì „ íŒ

### ğŸ’¡ íŒ 1: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ìˆœì„œ
1. **ë¨¼ì € ë³´ìƒ í•¨ìˆ˜ ê²€ì¦** (ê°€ì¥ ì¤‘ìš”!)
2. `learning_rate` ì¡°ì • (1e-4 ~ 1e-3)
3. `n_steps` ì¡°ì • (1024 ~ 4096)
4. `clip_range` ì¡°ì • (0.1 ~ 0.3)
5. ë‚˜ë¨¸ì§€ëŠ” ê¸°ë³¸ê°’ ìœ ì§€

### ğŸ’¡ íŒ 2: í•™ìŠµ ëª¨ë‹ˆí„°ë§
```python
# Tensorboard í•„ìˆ˜ í™•ì¸ í•­ëª©
# 1. ep_rew_mean: ì¦ê°€í•˜ëŠ”ê°€?
# 2. ep_len_mean: ê°ì†Œí•˜ëŠ”ê°€? (íš¨ìœ¨ì„±)
# 3. policy_loss: ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ì§€ ì•Šì€ê°€?
# 4. value_loss: ê°ì†Œí•˜ëŠ”ê°€?
# 5. approx_kl: 0.01 ~ 0.05 ë²”ìœ„ì¸ê°€?
# 6. clip_fraction: 0.1 ~ 0.3 ë²”ìœ„ì¸ê°€?
```

### ğŸ’¡ íŒ 3: ë³‘ë ¬ í™˜ê²½
```python
# ë‹¨ì¼ í™˜ê²½: ëŠë¦¼
env = gym.make("CartPole-v1")

# ë³‘ë ¬ í™˜ê²½: ë¹ ë¦„ (ê¶Œì¥)
from stable_baselines3.common.vec_env import SubprocVecEnv

def make_env():
    return gym.make("CartPole-v1")

env = SubprocVecEnv([make_env for _ in range(4)])

# n_steps * num_envs = total samples
# 2048 * 4 = 8192 samples per update
```

### ğŸ’¡ íŒ 4: í•™ìŠµ ì‹œê°„ ë‹¨ì¶•
```python
# 1. GPU ì‚¬ìš©
model = PPO("MlpPolicy", env, device="cuda")

# 2. ë³‘ë ¬ í™˜ê²½
env = SubprocVecEnv([make_env for _ in range(8)])

# 3. n_steps ì¦ê°€
n_steps = 4096  # ë” í° ë°°ì¹˜

# 4. n_epochs ê°ì†Œ
n_epochs = 5  # ë¹ ë¥¸ ì—…ë°ì´íŠ¸
```

### ğŸ’¡ íŒ 5: ë””ë²„ê¹…
```python
# ë¬¸ì œ: í•™ìŠµ ì•ˆ ë¨
# ì²´í¬ 1: ë³´ìƒì´ ë³€í™”í•˜ëŠ”ê°€?
# ì²´í¬ 2: approx_klì´ ë„ˆë¬´ í°ê°€? (> 0.1)
# ì²´í¬ 3: value_lossê°€ ê°ì†Œí•˜ëŠ”ê°€?

# í•´ê²°:
# - learning_rate ë‚®ì¶”ê¸°
# - clip_range ë‚®ì¶”ê¸°
# - n_epochs ì¤„ì´ê¸°
```

---

## ğŸ”¬ ê³ ê¸‰ ê¸°ë²•

### 1. Learning Rate ìŠ¤ì¼€ì¤„ë§
```python
def linear_schedule(initial_value):
    def func(progress_remaining):
        return progress_remaining * initial_value
    return func

model = PPO(
    "MlpPolicy",
    env,
    learning_rate=linear_schedule(3e-4),
)
```

### 2. Curriculum Learning
```python
class CurriculumEnv(gym.Env):
    def __init__(self):
        self.difficulty = 0.1
    
    def increase_difficulty(self):
        self.difficulty = min(1.0, self.difficulty + 0.1)
    
    def reset(self):
        # ì–´ë ¤ì›€ì— ë”°ë¼ ì´ˆê¸° ìƒíƒœ ë³€ê²½
        return self._get_obs()
```

### 3. Reward Scaling
```python
from stable_baselines3.common.vec_env import VecNormalize

env = DummyVecEnv([make_env])
env = VecNormalize(env, norm_reward=True, norm_obs=True)

# í•™ìŠµ í›„
env.save("vec_normalize.pkl")
```

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
1. **PPO ì›ë³¸**: [Proximal Policy Optimization Algorithms](https://arxiv.org/abs/1707.06347)
2. **GAE**: [High-Dimensional Continuous Control Using Generalized Advantage Estimation](https://arxiv.org/abs/1506.02438)
3. **TRPO**: [Trust Region Policy Optimization](https://arxiv.org/abs/1502.05477)

### ì½”ë“œ
1. **Stable-Baselines3 PPO**: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/ppo/ppo.py
2. **OpenAI Spinning Up PPO**: https://github.com/openai/spinningup/tree/master/spinup/algos/pytorch/ppo
3. **CleanRL PPO**: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/ppo.py

### íŠœí† ë¦¬ì–¼
1. **HuggingFace Deep RL**: https://huggingface.co/deep-rl-course/unit8/introduction
2. **Stable-Baselines3 Docs**: https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html
3. **Spinning Up PPO**: https://spinningup.openai.com/en/latest/algorithms/ppo.html

---

## ğŸ¯ RoArm-M3 ì ìš©

### í˜„ì¬ ì„¤ì •
```python
PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
)
```

### ì œì•ˆ ìˆ˜ì •
```python
PPO(
    "MlpPolicy",
    env,
    learning_rate=1e-4,        # â†“ ì•ˆì •ì„±
    n_steps=4096,              # â†‘ ìƒ˜í”Œ ìˆ˜
    batch_size=128,            # â†‘ ë°°ì¹˜
    n_epochs=15,               # â†‘ ì—…ë°ì´íŠ¸
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.1,            # â†“ ì•ˆì •ì„±
    ent_coef=0.005,            # + Exploration
    max_grad_norm=0.5,
    device="cuda",
)
```

